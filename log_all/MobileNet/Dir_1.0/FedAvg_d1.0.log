nohup: ignoring input
================================================================================
Summary of training process:
Dataset                : Cifar10
Batch size             : 64
Learing rate           : 0.001
Number of total clients: 100
Split method           : distribution
Split parameter        : 1.0
Clients per round      : 10
Number of global rounds: 100
Number of local rounds : 10
Feature from layer     : 11
Feature percentage     : 0.1
Local training loss    : CE
Loss of beta           : 1.0
Algorithm              : FedAvg
Modelname              : MOBNET
Mode                   : training
Seed                   : 0
================================================================================
Files already downloaded and verified
Files already downloaded and verified
Here
Done
Class frequencies:
client,class0,class1,class2,class3,class4,class5,class6,class7,class8,class9,Amount
Client   0,0.030,0.038,0.170,0.050,0.006,0.110,0.088,0.146,0.114,0.250,501
Client   1,0.185,0.153,0.00,0.052,0.045,0.072,0.00,0.095,0.150,0.250,601
Client   2,0.116,0.061,0.133,0.177,0.110,0.041,0.026,0.057,0.279,0.00,610
Client   3,0.030,0.075,0.144,0.033,0.234,0.063,0.048,0.075,0.210,0.090,334
Client   4,0.042,0.023,0.069,0.031,0.023,0.027,0.025,0.084,0.322,0.354,478
Client   5,0.103,0.451,0.099,0.009,0.023,0.026,0.033,0.007,0.171,0.077,426
Client   6,0.187,0.136,0.033,0.041,0.012,0.022,0.138,0.196,0.236,0.00,509
Client   7,0.019,0.035,0.010,0.027,0.085,0.022,0.083,0.034,0.259,0.426,625
Client   8,0.010,0.221,0.014,0.019,0.027,0.171,0.539,0.00,0.00,0.00,592
Client   9,0.035,0.063,0.033,0.056,0.038,0.161,0.090,0.035,0.163,0.327,735
Client  10,0.226,0.027,0.039,0.027,0.065,0.044,0.126,0.277,0.168,0.00,585
Client  11,0.073,0.145,0.021,0.043,0.038,0.103,0.009,0.209,0.098,0.261,234
Client  12,0.011,0.051,0.021,0.151,0.030,0.091,0.042,0.285,0.319,0.00,571
Client  13,0.008,0.051,0.107,0.027,0.035,0.011,0.238,0.112,0.043,0.369,374
Client  14,0.046,0.109,0.228,0.352,0.133,0.012,0.081,0.040,0.00,0.00,505
Client  15,0.007,0.196,0.017,0.063,0.029,0.419,0.070,0.010,0.058,0.131,413
Client  16,0.069,0.032,0.262,0.052,0.013,0.146,0.004,0.074,0.044,0.303,679
Client  17,0.089,0.045,0.206,0.092,0.006,0.066,0.021,0.095,0.380,0.00,671
Client  18,0.062,0.310,0.049,0.204,0.015,0.056,0.084,0.220,0.00,0.00,549
Client  19,0.122,0.025,0.466,0.144,0.243,0.00,0.00,0.00,0.00,0.00,597
Client  20,0.091,0.158,0.017,0.065,0.041,0.129,0.065,0.179,0.067,0.189,418
Client  21,0.110,0.166,0.030,0.157,0.00,0.025,0.120,0.102,0.289,0.00,591
Client  22,0.027,0.123,0.216,0.109,0.189,0.016,0.130,0.014,0.176,0.00,561
Client  23,0.164,0.029,0.027,0.252,0.137,0.038,0.027,0.066,0.027,0.235,452
Client  24,0.069,0.054,0.065,0.072,0.180,0.113,0.281,0.030,0.017,0.120,540
Client  25,0.150,0.110,0.018,0.204,0.004,0.105,0.049,0.034,0.034,0.293,447
Client  26,0.068,0.098,0.088,0.015,0.009,0.051,0.034,0.030,0.045,0.562,468
Client  27,0.067,0.275,0.009,0.016,0.046,0.106,0.033,0.158,0.129,0.162,568
Client  28,0.005,0.057,0.069,0.017,0.104,0.084,0.366,0.040,0.030,0.228,404
Client  29,0.062,0.066,0.160,0.027,0.008,0.012,0.289,0.094,0.282,0.00,595
Client  30,0.019,0.062,0.190,0.093,0.062,0.050,0.228,0.081,0.211,0.005,421
Client  31,0.020,0.124,0.037,0.057,0.201,0.119,0.072,0.117,0.132,0.122,403
Client  32,0.081,0.083,0.130,0.010,0.119,0.073,0.023,0.034,0.055,0.392,385
Client  33,0.092,0.314,0.016,0.149,0.084,0.035,0.287,0.022,0.00,0.00,509
Client  34,0.043,0.126,0.266,0.185,0.239,0.053,0.028,0.059,0.00,0.00,507
Client  35,0.017,0.058,0.322,0.053,0.102,0.135,0.043,0.054,0.216,0.00,606
Client  36,0.152,0.073,0.186,0.031,0.169,0.033,0.010,0.219,0.058,0.067,479
Client  37,0.285,0.024,0.041,0.029,0.006,0.164,0.084,0.137,0.229,0.00,628
Client  38,0.218,0.036,0.088,0.055,0.097,0.027,0.012,0.042,0.121,0.303,330
Client  39,0.011,0.068,0.092,0.279,0.068,0.014,0.014,0.141,0.136,0.179,369
Client  40,0.154,0.026,0.003,0.025,0.046,0.003,0.109,0.020,0.159,0.455,649
Client  41,0.007,0.044,0.056,0.322,0.065,0.061,0.080,0.017,0.015,0.334,413
Client  42,0.060,0.023,0.271,0.006,0.050,0.175,0.194,0.027,0.194,0.00,520
Client  43,0.406,0.082,0.003,0.023,0.237,0.014,0.020,0.073,0.127,0.017,355
Client  44,0.031,0.027,0.236,0.100,0.253,0.110,0.133,0.110,0.00,0.00,518
Client  45,0.035,0.126,0.113,0.100,0.013,0.026,0.278,0.017,0.204,0.087,230
Client  46,0.155,0.137,0.040,0.258,0.149,0.002,0.058,0.201,0.00,0.00,503
Client  47,0.333,0.026,0.119,0.117,0.037,0.133,0.056,0.047,0.070,0.063,429
Client  48,0.038,0.372,0.147,0.053,0.059,0.055,0.005,0.271,0.00,0.00,546
Client  49,0.027,0.029,0.012,0.018,0.271,0.082,0.076,0.107,0.012,0.366,513
Client  50,0.032,0.014,0.346,0.017,0.046,0.101,0.121,0.092,0.130,0.101,347
Client  51,0.358,0.112,0.180,0.320,0.008,0.008,0.014,0.00,0.00,0.00,500
Client  52,0.098,0.013,0.163,0.075,0.117,0.002,0.293,0.017,0.071,0.151,478
Client  53,0.208,0.063,0.029,0.308,0.118,0.275,0.00,0.00,0.00,0.00,510
Client  54,0.035,0.074,0.030,0.248,0.058,0.049,0.100,0.081,0.056,0.268,568
Client  55,0.008,0.249,0.008,0.277,0.018,0.440,0.00,0.00,0.00,0.00,502
Client  56,0.047,0.138,0.009,0.111,0.239,0.074,0.016,0.061,0.306,0.00,687
Client  57,0.026,0.065,0.043,0.002,0.036,0.022,0.103,0.361,0.156,0.187,507
Client  58,0.079,0.217,0.061,0.178,0.097,0.368,0.00,0.00,0.00,0.00,506
Client  59,0.081,0.169,0.318,0.008,0.179,0.035,0.010,0.200,0.00,0.00,509
Client  60,0.027,0.016,0.033,0.123,0.006,0.349,0.106,0.011,0.329,0.00,705
Client  61,0.039,0.114,0.091,0.171,0.030,0.159,0.167,0.130,0.065,0.033,508
Client  62,0.003,0.100,0.006,0.173,0.128,0.116,0.145,0.034,0.031,0.264,648
Client  63,0.069,0.132,0.057,0.033,0.230,0.065,0.278,0.045,0.033,0.057,418
Client  64,0.039,0.050,0.013,0.161,0.094,0.131,0.142,0.030,0.083,0.258,542
Client  65,0.090,0.003,0.189,0.037,0.048,0.185,0.284,0.163,0.00,0.00,588
Client  66,0.196,0.044,0.063,0.035,0.083,0.093,0.094,0.391,0.00,0.00,540
Client  67,0.074,0.004,0.145,0.004,0.495,0.278,0.00,0.00,0.00,0.00,503
Client  68,0.012,0.036,0.017,0.010,0.046,0.017,0.017,0.065,0.072,0.708,414
Client  69,0.296,0.192,0.132,0.045,0.014,0.018,0.204,0.055,0.006,0.038,494
Client  70,0.167,0.061,0.406,0.131,0.130,0.106,0.00,0.00,0.00,0.00,540
Client  71,0.046,0.020,0.027,0.108,0.179,0.207,0.201,0.060,0.150,0.00,546
Client  72,0.039,0.261,0.076,0.061,0.002,0.011,0.007,0.345,0.197,0.00,537
Client  73,0.296,0.020,0.010,0.298,0.039,0.125,0.088,0.124,0.00,0.00,510
Client  74,0.054,0.139,0.246,0.037,0.070,0.048,0.126,0.039,0.128,0.113,460
Client  75,0.110,0.030,0.002,0.114,0.184,0.042,0.291,0.066,0.082,0.078,499
Client  76,0.411,0.013,0.002,0.074,0.025,0.009,0.101,0.366,0.00,0.00,557
Client  77,0.007,0.093,0.057,0.073,0.177,0.023,0.007,0.147,0.087,0.330,300
Client  78,0.054,0.049,0.003,0.005,0.132,0.130,0.280,0.067,0.010,0.269,386
Client  79,0.027,0.082,0.012,0.051,0.133,0.039,0.329,0.327,0.00,0.00,513
Client  80,0.257,0.00,0.056,0.021,0.160,0.506,0.00,0.00,0.00,0.00,536
Client  81,0.085,0.050,0.312,0.025,0.088,0.010,0.310,0.015,0.106,0.00,520
Client  82,0.005,0.037,0.136,0.198,0.285,0.115,0.030,0.047,0.014,0.133,572
Client  83,0.151,0.024,0.070,0.209,0.064,0.121,0.087,0.141,0.133,0.00,503
Client  84,0.110,0.131,0.046,0.065,0.058,0.210,0.018,0.362,0.00,0.00,566
Client  85,0.107,0.024,0.033,0.228,0.239,0.031,0.042,0.108,0.190,0.00,553
Client  86,0.324,0.044,0.056,0.073,0.321,0.060,0.023,0.100,0.00,0.00,521
Client  87,0.212,0.008,0.008,0.053,0.021,0.120,0.214,0.143,0.222,0.00,532
Client  88,0.148,0.160,0.086,0.051,0.121,0.058,0.201,0.103,0.072,0.00,513
Client  89,0.025,0.195,0.338,0.135,0.174,0.132,0.00,0.00,0.00,0.00,517
Client  90,0.016,0.395,0.165,0.054,0.171,0.200,0.00,0.00,0.00,0.00,504
Client  91,0.035,0.388,0.058,0.045,0.088,0.018,0.020,0.090,0.183,0.075,399
Client  92,0.039,0.111,0.098,0.007,0.160,0.186,0.046,0.049,0.124,0.182,307
Client  93,0.121,0.047,0.103,0.224,0.222,0.037,0.018,0.079,0.018,0.129,379
Client  94,0.161,0.166,0.199,0.030,0.033,0.035,0.035,0.202,0.106,0.033,367
Client  95,0.081,0.135,0.010,0.111,0.164,0.006,0.143,0.265,0.00,0.085,495
Client  96,0.010,0.262,0.081,0.008,0.043,0.096,0.242,0.020,0.128,0.111,397
Client  97,0.055,0.027,0.018,0.191,0.205,0.320,0.068,0.011,0.093,0.011,440
Client  98,0.307,0.043,0.083,0.002,0.026,0.201,0.021,0.120,0.196,0.00,606
Client  99,0.129,0.160,0.055,0.311,0.091,0.050,0.020,0.182,0.002,0.00,505
Num_samples of Training set per client: [501, 601, 610, 334, 478, 426, 509, 625, 592, 735, 585, 234, 571, 374, 505, 413, 679, 671, 549, 597, 418, 591, 561, 452, 540, 447, 468, 568, 404, 595, 421, 403, 385, 509, 507, 606, 479, 628, 330, 369, 649, 413, 520, 355, 518, 230, 503, 429, 546, 513, 347, 500, 478, 510, 568, 502, 687, 507, 506, 509, 705, 508, 648, 418, 542, 588, 540, 503, 414, 494, 540, 546, 537, 510, 460, 499, 557, 300, 386, 513, 536, 520, 572, 503, 566, 553, 521, 532, 513, 517, 504, 399, 307, 379, 367, 495, 397, 440, 606, 505]
Total_training_samples: 50000
Global test set: 10000
Finish Generating Samples, distribution saved
MobileNetV2(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layers): Sequential(
    (0): Block(
      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Block(
      (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (6): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (7): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (8): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (9): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (10): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (12): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (13): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (14): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (15): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (16): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv2): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear): Linear(in_features=1280, out_features=10, bias=True)
)
number of parameters: 2296922
clients initializting...
output size: 10
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<01:32,  1.07it/s]  2%|▏         | 2/100 [00:01<01:28,  1.10it/s]  3%|▎         | 3/100 [00:02<01:27,  1.11it/s]  4%|▍         | 4/100 [00:03<01:26,  1.11it/s]  5%|▌         | 5/100 [00:04<01:26,  1.10it/s]  6%|▌         | 6/100 [00:05<01:26,  1.09it/s]  7%|▋         | 7/100 [00:06<01:25,  1.09it/s]  8%|▊         | 8/100 [00:07<01:25,  1.08it/s]  9%|▉         | 9/100 [00:08<01:21,  1.12it/s] 10%|█         | 10/100 [00:09<01:20,  1.12it/s] 11%|█         | 11/100 [00:09<01:18,  1.13it/s] 12%|█▏        | 12/100 [00:10<01:18,  1.12it/s] 13%|█▎        | 13/100 [00:11<01:17,  1.13it/s] 14%|█▍        | 14/100 [00:12<01:17,  1.11it/s] 15%|█▌        | 15/100 [00:13<01:15,  1.13it/s] 16%|█▌        | 16/100 [00:14<01:14,  1.13it/s] 17%|█▋        | 17/100 [00:15<01:14,  1.11it/s] 18%|█▊        | 18/100 [00:16<01:13,  1.11it/s] 19%|█▉        | 19/100 [00:17<01:11,  1.13it/s] 20%|██        | 20/100 [00:17<01:07,  1.19it/s] 21%|██        | 21/100 [00:18<01:08,  1.15it/s] 22%|██▏       | 22/100 [00:19<01:06,  1.17it/s] 23%|██▎       | 23/100 [00:20<01:05,  1.17it/s] 24%|██▍       | 24/100 [00:21<01:05,  1.15it/s] 25%|██▌       | 25/100 [00:25<02:23,  1.91s/it] 26%|██▌       | 26/100 [00:26<01:59,  1.61s/it] 27%|██▋       | 27/100 [00:27<01:42,  1.40s/it] 28%|██▊       | 28/100 [00:28<01:30,  1.25s/it] 29%|██▉       | 29/100 [00:29<01:22,  1.16s/it] 30%|███       | 30/100 [00:30<01:15,  1.08s/it] 31%|███       | 31/100 [00:31<01:10,  1.03s/it] 32%|███▏      | 32/100 [00:32<01:08,  1.01s/it] 33%|███▎      | 33/100 [00:33<01:07,  1.01s/it] 34%|███▍      | 34/100 [00:33<01:04,  1.02it/s] 35%|███▌      | 35/100 [00:34<01:02,  1.05it/s] 36%|███▌      | 36/100 [00:35<01:01,  1.05it/s] 37%|███▋      | 37/100 [00:36<01:01,  1.03it/s] 38%|███▊      | 38/100 [00:37<00:59,  1.04it/s] 39%|███▉      | 39/100 [00:38<00:59,  1.03it/s] 40%|████      | 40/100 [00:39<00:58,  1.02it/s] 41%|████      | 41/100 [00:40<00:58,  1.01it/s] 42%|████▏     | 42/100 [00:41<00:57,  1.01it/s] 43%|████▎     | 43/100 [00:42<00:55,  1.02it/s] 44%|████▍     | 44/100 [00:43<00:54,  1.02it/s] 45%|████▌     | 45/100 [00:44<00:52,  1.04it/s] 46%|████▌     | 46/100 [00:45<00:51,  1.04it/s] 47%|████▋     | 47/100 [00:46<00:48,  1.08it/s] 48%|████▊     | 48/100 [00:47<00:48,  1.08it/s] 49%|████▉     | 49/100 [00:48<00:46,  1.10it/s] 50%|█████     | 50/100 [00:52<01:40,  2.02s/it] 51%|█████     | 51/100 [00:53<01:23,  1.70s/it] 52%|█████▏    | 52/100 [00:54<01:08,  1.43s/it] 53%|█████▎    | 53/100 [00:55<01:00,  1.28s/it] 54%|█████▍    | 54/100 [00:56<00:52,  1.13s/it] 55%|█████▌    | 55/100 [00:57<00:48,  1.07s/it] 56%|█████▌    | 56/100 [00:58<00:43,  1.02it/s] 57%|█████▋    | 57/100 [00:58<00:41,  1.05it/s] 58%|█████▊    | 58/100 [00:59<00:39,  1.05it/s] 59%|█████▉    | 59/100 [01:00<00:36,  1.12it/s] 60%|██████    | 60/100 [01:01<00:35,  1.12it/s] 61%|██████    | 61/100 [01:02<00:34,  1.12it/s] 62%|██████▏   | 62/100 [01:03<00:34,  1.11it/s] 63%|██████▎   | 63/100 [01:04<00:34,  1.09it/s] 64%|██████▍   | 64/100 [01:05<00:33,  1.07it/s] 65%|██████▌   | 65/100 [01:06<00:32,  1.07it/s] 66%|██████▌   | 66/100 [01:07<00:30,  1.10it/s] 67%|██████▋   | 67/100 [01:07<00:29,  1.12it/s] 68%|██████▊   | 68/100 [01:08<00:27,  1.17it/s] 69%|██████▉   | 69/100 [01:09<00:27,  1.13it/s] 70%|███████   | 70/100 [01:10<00:26,  1.12it/s] 71%|███████   | 71/100 [01:11<00:24,  1.17it/s] 72%|███████▏  | 72/100 [01:12<00:24,  1.16it/s] 73%|███████▎  | 73/100 [01:13<00:23,  1.16it/s] 74%|███████▍  | 74/100 [01:13<00:22,  1.17it/s] 75%|███████▌  | 75/100 [01:14<00:21,  1.14it/s] 76%|███████▌  | 76/100 [01:15<00:21,  1.11it/s] 77%|███████▋  | 77/100 [01:16<00:20,  1.13it/s] 78%|███████▊  | 78/100 [01:20<00:42,  1.94s/it] 79%|███████▉  | 79/100 [01:21<00:34,  1.64s/it] 80%|████████  | 80/100 [01:22<00:28,  1.41s/it] 81%|████████  | 81/100 [01:23<00:22,  1.21s/it] 82%|████████▏ | 82/100 [01:24<00:20,  1.11s/it] 83%|████████▎ | 83/100 [01:25<00:18,  1.06s/it] 84%|████████▍ | 84/100 [01:26<00:16,  1.02s/it] 85%|████████▌ | 85/100 [01:27<00:14,  1.03it/s] 86%|████████▌ | 86/100 [01:28<00:13,  1.05it/s] 87%|████████▋ | 87/100 [01:28<00:12,  1.08it/s] 88%|████████▊ | 88/100 [01:29<00:11,  1.09it/s] 89%|████████▉ | 89/100 [01:30<00:10,  1.10it/s] 90%|█████████ | 90/100 [01:31<00:08,  1.15it/s] 91%|█████████ | 91/100 [01:32<00:07,  1.20it/s] 92%|█████████▏| 92/100 [01:33<00:06,  1.17it/s] 93%|█████████▎| 93/100 [01:34<00:06,  1.14it/s] 94%|█████████▍| 94/100 [01:35<00:05,  1.11it/s] 95%|█████████▌| 95/100 [01:35<00:04,  1.11it/s] 96%|█████████▌| 96/100 [01:36<00:03,  1.11it/s] 97%|█████████▋| 97/100 [01:37<00:02,  1.10it/s] 98%|█████████▊| 98/100 [01:38<00:01,  1.10it/s] 99%|█████████▉| 99/100 [01:39<00:00,  1.11it/s]100%|██████████| 100/100 [01:40<00:00,  1.12it/s]100%|██████████| 100/100 [01:40<00:00,  1.00s/it]
Number of users per round / total users: 10  /  100
Finished creating FL server.
=== Training starts: algorithm FedAvg ===
-------------Round number:  0  -------------
loss: CE learning rate: 0.001
training loss: tensor(0.9501)
loss: CE learning rate: 0.001
training loss: tensor(1.0312)
loss: CE learning rate: 0.001
training loss: tensor(1.1169)
loss: CE learning rate: 0.001
training loss: tensor(0.6235)
loss: CE learning rate: 0.001
training loss: tensor(1.1076)
loss: CE learning rate: 0.001
training loss: tensor(1.0990)
loss: CE learning rate: 0.001
training loss: tensor(1.0978)
loss: CE learning rate: 0.001
training loss: tensor(0.9413)
loss: CE learning rate: 0.001
training loss: tensor(1.1254)
loss: CE learning rate: 0.001
training loss: tensor(1.1147)
Global Model Acc on global data: 0.1003 length of data: 10000
save a model
-------------Round number:  1  -------------
loss: CE learning rate: 0.000982
training loss: tensor(0.5381)
loss: CE learning rate: 0.000982
training loss: tensor(0.7833)
loss: CE learning rate: 0.000982
training loss: tensor(0.7684)
loss: CE learning rate: 0.000982
training loss: tensor(0.9506)
loss: CE learning rate: 0.000982
training loss: tensor(0.7879)
loss: CE learning rate: 0.000982
training loss: tensor(1.0265)
loss: CE learning rate: 0.000982
training loss: tensor(0.9474)
loss: CE learning rate: 0.000982
training loss: tensor(0.9002)
loss: CE learning rate: 0.000982
training loss: tensor(0.8146)
loss: CE learning rate: 0.000982
training loss: tensor(0.9938)
Global Model Acc on global data: 0.1169 length of data: 10000
save a model
-------------Round number:  2  -------------
loss: CE learning rate: 0.000964
training loss: tensor(0.6914)
loss: CE learning rate: 0.000964
training loss: tensor(0.7202)
loss: CE learning rate: 0.000964
training loss: tensor(0.7017)
loss: CE learning rate: 0.000964
training loss: tensor(0.8481)
loss: CE learning rate: 0.000964
training loss: tensor(0.8466)
loss: CE learning rate: 0.000964
training loss: tensor(1.0078)
loss: CE learning rate: 0.000964
training loss: tensor(0.8041)
loss: CE learning rate: 0.000964
training loss: tensor(0.9046)
loss: CE learning rate: 0.000964
training loss: tensor(0.8282)
loss: CE learning rate: 0.000964
training loss: tensor(0.8470)
Global Model Acc on global data: 0.1692 length of data: 10000
save a model
-------------Round number:  3  -------------
loss: CE learning rate: 0.000946
training loss: tensor(0.5364)
loss: CE learning rate: 0.000946
training loss: tensor(0.5023)
loss: CE learning rate: 0.000946
training loss: tensor(0.5629)
loss: CE learning rate: 0.000946
training loss: tensor(0.6772)
loss: CE learning rate: 0.000946
training loss: tensor(0.6463)
loss: CE learning rate: 0.000946
training loss: tensor(0.5670)
loss: CE learning rate: 0.000946
training loss: tensor(0.6088)
loss: CE learning rate: 0.000946
training loss: tensor(0.6655)
loss: CE learning rate: 0.000946
training loss: tensor(0.6125)
loss: CE learning rate: 0.000946
training loss: tensor(0.4864)
Global Model Acc on global data: 0.2429 length of data: 10000
save a model
-------------Round number:  4  -------------
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5866)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5643)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5151)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5478)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.6165)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.6082)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.4994)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5121)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.6871)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5438)
Global Model Acc on global data: 0.2677 length of data: 10000
save a model
-------------Round number:  5  -------------
loss: CE learning rate: 0.00091
training loss: tensor(0.4612)
loss: CE learning rate: 0.00091
training loss: tensor(0.9155)
loss: CE learning rate: 0.00091
training loss: tensor(0.4811)
loss: CE learning rate: 0.00091
training loss: tensor(0.5299)
loss: CE learning rate: 0.00091
training loss: tensor(0.5745)
loss: CE learning rate: 0.00091
training loss: tensor(0.6805)
loss: CE learning rate: 0.00091
training loss: tensor(0.5002)
loss: CE learning rate: 0.00091
training loss: tensor(0.4451)
loss: CE learning rate: 0.00091
training loss: tensor(0.5034)
loss: CE learning rate: 0.00091
training loss: tensor(0.5125)
Global Model Acc on global data: 0.2985 length of data: 10000
save a model
-------------Round number:  6  -------------
loss: CE learning rate: 0.000892
training loss: tensor(0.3778)
loss: CE learning rate: 0.000892
training loss: tensor(0.4789)
loss: CE learning rate: 0.000892
training loss: tensor(0.5070)
loss: CE learning rate: 0.000892
training loss: tensor(0.4303)
loss: CE learning rate: 0.000892
training loss: tensor(0.5989)
loss: CE learning rate: 0.000892
training loss: tensor(0.4377)
loss: CE learning rate: 0.000892
training loss: tensor(0.3925)
loss: CE learning rate: 0.000892
training loss: tensor(0.4257)
loss: CE learning rate: 0.000892
training loss: tensor(0.5772)
loss: CE learning rate: 0.000892
training loss: tensor(0.5007)
Global Model Acc on global data: 0.3354 length of data: 10000
save a model
-------------Round number:  7  -------------
loss: CE learning rate: 0.000874
training loss: tensor(0.3864)
loss: CE learning rate: 0.000874
training loss: tensor(0.8482)
loss: CE learning rate: 0.000874
training loss: tensor(0.8088)
loss: CE learning rate: 0.000874
training loss: tensor(0.5816)
loss: CE learning rate: 0.000874
training loss: tensor(0.8268)
loss: CE learning rate: 0.000874
training loss: tensor(0.4048)
loss: CE learning rate: 0.000874
training loss: tensor(0.5079)
loss: CE learning rate: 0.000874
training loss: tensor(0.4443)
loss: CE learning rate: 0.000874
training loss: tensor(0.4678)
loss: CE learning rate: 0.000874
training loss: tensor(0.4649)
Global Model Acc on global data: 0.3225 length of data: 10000
-------------Round number:  8  -------------
loss: CE learning rate: 0.000856
training loss: tensor(0.2978)
loss: CE learning rate: 0.000856
training loss: tensor(0.5909)
loss: CE learning rate: 0.000856
training loss: tensor(0.5151)
loss: CE learning rate: 0.000856
training loss: tensor(0.5282)
loss: CE learning rate: 0.000856
training loss: tensor(0.2801)
loss: CE learning rate: 0.000856
training loss: tensor(0.4187)
loss: CE learning rate: 0.000856
training loss: tensor(0.3087)
loss: CE learning rate: 0.000856
training loss: tensor(0.3484)
loss: CE learning rate: 0.000856
training loss: tensor(0.5102)
loss: CE learning rate: 0.000856
training loss: tensor(0.3918)
Global Model Acc on global data: 0.3388 length of data: 10000
save a model
-------------Round number:  9  -------------
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.4380)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.4785)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.2989)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.8731)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.5432)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.4727)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.5665)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.3238)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.4660)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.3273)
Global Model Acc on global data: 0.3871 length of data: 10000
save a model
-------------Round number:  10  -------------
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.4016)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3936)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3334)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3144)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3036)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3827)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.6753)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3584)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.5543)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.5628)
Global Model Acc on global data: 0.3706 length of data: 10000
-------------Round number:  11  -------------
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.2307)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.7639)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3724)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3660)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3102)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3547)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.4029)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3123)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3898)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.2800)
Global Model Acc on global data: 0.3882 length of data: 10000
save a model
-------------Round number:  12  -------------
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.2737)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.2853)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.4198)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.2851)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.6505)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.4145)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.4586)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.3202)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.3863)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.5357)
Global Model Acc on global data: 0.4144 length of data: 10000
save a model
-------------Round number:  13  -------------
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.2375)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3140)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3842)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.2518)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3887)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.4276)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.4655)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3375)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3433)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3640)
Global Model Acc on global data: 0.4504 length of data: 10000
save a model
-------------Round number:  14  -------------
loss: CE learning rate: 0.000748
training loss: tensor(0.3634)
loss: CE learning rate: 0.000748
training loss: tensor(0.3090)
loss: CE learning rate: 0.000748
training loss: tensor(0.3621)
loss: CE learning rate: 0.000748
training loss: tensor(0.3155)
loss: CE learning rate: 0.000748
training loss: tensor(0.2565)
loss: CE learning rate: 0.000748
training loss: tensor(0.3708)
loss: CE learning rate: 0.000748
training loss: tensor(0.4169)
loss: CE learning rate: 0.000748
training loss: tensor(0.5205)
loss: CE learning rate: 0.000748
training loss: tensor(0.3710)
loss: CE learning rate: 0.000748
training loss: tensor(0.3902)
Global Model Acc on global data: 0.4138 length of data: 10000
-------------Round number:  15  -------------
loss: CE learning rate: 0.00073
training loss: tensor(0.3117)
loss: CE learning rate: 0.00073
training loss: tensor(0.3256)
loss: CE learning rate: 0.00073
training loss: tensor(0.3104)
loss: CE learning rate: 0.00073
training loss: tensor(0.2600)
loss: CE learning rate: 0.00073
training loss: tensor(0.2142)
loss: CE learning rate: 0.00073
training loss: tensor(0.5472)
loss: CE learning rate: 0.00073
training loss: tensor(0.3195)
loss: CE learning rate: 0.00073
training loss: tensor(0.2059)
loss: CE learning rate: 0.00073
training loss: tensor(0.4950)
loss: CE learning rate: 0.00073
training loss: tensor(0.3247)
Global Model Acc on global data: 0.4147 length of data: 10000
-------------Round number:  16  -------------
loss: CE learning rate: 0.000712
training loss: tensor(0.5110)
loss: CE learning rate: 0.000712
training loss: tensor(0.4288)
loss: CE learning rate: 0.000712
training loss: tensor(0.4193)
loss: CE learning rate: 0.000712
training loss: tensor(0.4715)
loss: CE learning rate: 0.000712
training loss: tensor(0.3958)
loss: CE learning rate: 0.000712
training loss: tensor(0.3012)
loss: CE learning rate: 0.000712
training loss: tensor(0.4177)
loss: CE learning rate: 0.000712
training loss: tensor(0.6964)
loss: CE learning rate: 0.000712
training loss: tensor(0.4330)
loss: CE learning rate: 0.000712
training loss: tensor(0.3490)
Global Model Acc on global data: 0.4428 length of data: 10000
-------------Round number:  17  -------------
loss: CE learning rate: 0.000694
training loss: tensor(0.2436)
loss: CE learning rate: 0.000694
training loss: tensor(0.3317)
loss: CE learning rate: 0.000694
training loss: tensor(0.1891)
loss: CE learning rate: 0.000694
training loss: tensor(0.4646)
loss: CE learning rate: 0.000694
training loss: tensor(0.3249)
loss: CE learning rate: 0.000694
training loss: tensor(0.3815)
loss: CE learning rate: 0.000694
training loss: tensor(0.4036)
loss: CE learning rate: 0.000694
training loss: tensor(0.3231)
loss: CE learning rate: 0.000694
training loss: tensor(0.2890)
loss: CE learning rate: 0.000694
training loss: tensor(0.2027)
Global Model Acc on global data: 0.4865 length of data: 10000
save a model
-------------Round number:  18  -------------
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3648)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.2078)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.2829)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3449)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3258)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3102)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3116)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.2979)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3318)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3135)
Global Model Acc on global data: 0.4774 length of data: 10000
-------------Round number:  19  -------------
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3416)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.2594)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3306)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.2786)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3789)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.4808)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.2980)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.2991)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.2842)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3621)
Global Model Acc on global data: 0.4651 length of data: 10000
-------------Round number:  20  -------------
loss: CE learning rate: 0.00064
training loss: tensor(0.2751)
loss: CE learning rate: 0.00064
training loss: tensor(0.3867)
loss: CE learning rate: 0.00064
training loss: tensor(0.2308)
loss: CE learning rate: 0.00064
training loss: tensor(0.2606)
loss: CE learning rate: 0.00064
training loss: tensor(0.2998)
loss: CE learning rate: 0.00064
training loss: tensor(0.1933)
loss: CE learning rate: 0.00064
training loss: tensor(0.1791)
loss: CE learning rate: 0.00064
training loss: tensor(0.2927)
loss: CE learning rate: 0.00064
training loss: tensor(0.2552)
loss: CE learning rate: 0.00064
training loss: tensor(1.2481)
Global Model Acc on global data: 0.4534 length of data: 10000
-------------Round number:  21  -------------
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.4203)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.4110)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.2353)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.2339)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.3306)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.1770)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.2574)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.3172)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.3269)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.4581)
Global Model Acc on global data: 0.4672 length of data: 10000
-------------Round number:  22  -------------
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2095)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.9346)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2776)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2283)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.1820)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.1440)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.3801)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2673)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2913)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.1566)
Global Model Acc on global data: 0.4502 length of data: 10000
-------------Round number:  23  -------------
loss: CE learning rate: 0.000586
training loss: tensor(0.6814)
loss: CE learning rate: 0.000586
training loss: tensor(0.3169)
loss: CE learning rate: 0.000586
training loss: tensor(0.2334)
loss: CE learning rate: 0.000586
training loss: tensor(0.8729)
loss: CE learning rate: 0.000586
training loss: tensor(0.4832)
loss: CE learning rate: 0.000586
training loss: tensor(0.1286)
loss: CE learning rate: 0.000586
training loss: tensor(0.3818)
loss: CE learning rate: 0.000586
training loss: tensor(0.2699)
loss: CE learning rate: 0.000586
training loss: tensor(0.3375)
loss: CE learning rate: 0.000586
training loss: tensor(0.2528)
Global Model Acc on global data: 0.4782 length of data: 10000
-------------Round number:  24  -------------
loss: CE learning rate: 0.000568
training loss: tensor(0.2922)
loss: CE learning rate: 0.000568
training loss: tensor(0.1831)
loss: CE learning rate: 0.000568
training loss: tensor(0.0692)
loss: CE learning rate: 0.000568
training loss: tensor(0.2880)
loss: CE learning rate: 0.000568
training loss: tensor(0.3345)
loss: CE learning rate: 0.000568
training loss: tensor(0.2693)
loss: CE learning rate: 0.000568
training loss: tensor(0.9167)
loss: CE learning rate: 0.000568
training loss: tensor(0.1653)
loss: CE learning rate: 0.000568
training loss: tensor(0.2480)
loss: CE learning rate: 0.000568
training loss: tensor(0.3966)
Global Model Acc on global data: 0.4763 length of data: 10000
-------------Round number:  25  -------------
loss: CE learning rate: 0.00055
training loss: tensor(0.4031)
loss: CE learning rate: 0.00055
training loss: tensor(0.2308)
loss: CE learning rate: 0.00055
training loss: tensor(0.3285)
loss: CE learning rate: 0.00055
training loss: tensor(0.3154)
loss: CE learning rate: 0.00055
training loss: tensor(0.3330)
loss: CE learning rate: 0.00055
training loss: tensor(0.3261)
loss: CE learning rate: 0.00055
training loss: tensor(0.2400)
loss: CE learning rate: 0.00055
training loss: tensor(0.4319)
loss: CE learning rate: 0.00055
training loss: tensor(0.2714)
loss: CE learning rate: 0.00055
training loss: tensor(0.2741)
Global Model Acc on global data: 0.4909 length of data: 10000
save a model
-------------Round number:  26  -------------
loss: CE learning rate: 0.000532
training loss: tensor(0.3114)
loss: CE learning rate: 0.000532
training loss: tensor(0.2707)
loss: CE learning rate: 0.000532
training loss: tensor(0.2337)
loss: CE learning rate: 0.000532
training loss: tensor(0.3516)
loss: CE learning rate: 0.000532
training loss: tensor(0.4313)
loss: CE learning rate: 0.000532
training loss: tensor(0.4384)
loss: CE learning rate: 0.000532
training loss: tensor(0.3202)
loss: CE learning rate: 0.000532
training loss: tensor(0.3264)
loss: CE learning rate: 0.000532
training loss: tensor(0.1942)
loss: CE learning rate: 0.000532
training loss: tensor(0.2640)
Global Model Acc on global data: 0.4845 length of data: 10000
-------------Round number:  27  -------------
loss: CE learning rate: 0.000514
training loss: tensor(0.3235)
loss: CE learning rate: 0.000514
training loss: tensor(0.6184)
loss: CE learning rate: 0.000514
training loss: tensor(0.3087)
loss: CE learning rate: 0.000514
training loss: tensor(0.3912)
loss: CE learning rate: 0.000514
training loss: tensor(0.4332)
loss: CE learning rate: 0.000514
training loss: tensor(0.2108)
loss: CE learning rate: 0.000514
training loss: tensor(0.3619)
loss: CE learning rate: 0.000514
training loss: tensor(0.1980)
loss: CE learning rate: 0.000514
training loss: tensor(0.3410)
loss: CE learning rate: 0.000514
training loss: tensor(0.2858)
Global Model Acc on global data: 0.5144 length of data: 10000
save a model
-------------Round number:  28  -------------
loss: CE learning rate: 0.000496
training loss: tensor(0.2904)
loss: CE learning rate: 0.000496
training loss: tensor(0.1815)
loss: CE learning rate: 0.000496
training loss: tensor(0.2230)
loss: CE learning rate: 0.000496
training loss: tensor(0.2153)
loss: CE learning rate: 0.000496
training loss: tensor(0.3583)
loss: CE learning rate: 0.000496
training loss: tensor(0.2351)
loss: CE learning rate: 0.000496
training loss: tensor(0.2700)
loss: CE learning rate: 0.000496
training loss: tensor(0.2349)
loss: CE learning rate: 0.000496
training loss: tensor(0.3443)
loss: CE learning rate: 0.000496
training loss: tensor(0.1270)
Global Model Acc on global data: 0.516 length of data: 10000
save a model
-------------Round number:  29  -------------
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2555)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2374)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.3575)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.3654)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.3039)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.1806)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.3047)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2665)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.3624)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2026)
Global Model Acc on global data: 0.527 length of data: 10000
save a model
-------------Round number:  30  -------------
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2410)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2830)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2601)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2401)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.1313)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2047)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.3034)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.3939)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2544)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.3263)
Global Model Acc on global data: 0.5119 length of data: 10000
-------------Round number:  31  -------------
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.1917)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.1407)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2487)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.3502)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2435)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.1853)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.0709)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.0795)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.6551)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2365)
Global Model Acc on global data: 0.4925 length of data: 10000
-------------Round number:  32  -------------
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.2537)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.4133)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.1191)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.3745)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.2185)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.1045)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.1472)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.4463)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.2790)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.3816)
Global Model Acc on global data: 0.5124 length of data: 10000
-------------Round number:  33  -------------
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.1263)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2539)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.1304)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2057)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.4407)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.0984)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2755)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.4335)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2012)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2578)
Global Model Acc on global data: 0.4989 length of data: 10000
-------------Round number:  34  -------------
loss: CE learning rate: 0.000388
training loss: tensor(0.1516)
loss: CE learning rate: 0.000388
training loss: tensor(0.2035)
loss: CE learning rate: 0.000388
training loss: tensor(0.3785)
loss: CE learning rate: 0.000388
training loss: tensor(0.5561)
loss: CE learning rate: 0.000388
training loss: tensor(1.8997)
loss: CE learning rate: 0.000388
training loss: tensor(0.3504)
loss: CE learning rate: 0.000388
training loss: tensor(0.2205)
loss: CE learning rate: 0.000388
training loss: tensor(0.2533)
loss: CE learning rate: 0.000388
training loss: tensor(0.3842)
loss: CE learning rate: 0.000388
training loss: tensor(0.2783)
Global Model Acc on global data: 0.5261 length of data: 10000
-------------Round number:  35  -------------
loss: CE learning rate: 0.00037
training loss: tensor(0.3774)
loss: CE learning rate: 0.00037
training loss: tensor(0.0676)
loss: CE learning rate: 0.00037
training loss: tensor(0.2420)
loss: CE learning rate: 0.00037
training loss: tensor(0.1492)
loss: CE learning rate: 0.00037
training loss: tensor(0.1480)
loss: CE learning rate: 0.00037
training loss: tensor(0.1460)
loss: CE learning rate: 0.00037
training loss: tensor(0.2045)
loss: CE learning rate: 0.00037
training loss: tensor(0.3273)
loss: CE learning rate: 0.00037
training loss: tensor(0.2541)
loss: CE learning rate: 0.00037
training loss: tensor(0.2895)
Global Model Acc on global data: 0.5433 length of data: 10000
save a model
-------------Round number:  36  -------------
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.2099)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.2109)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.1195)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.1001)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.3278)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.3102)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.3649)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.6629)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.2170)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.1158)
Global Model Acc on global data: 0.5477 length of data: 10000
save a model
-------------Round number:  37  -------------
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.3576)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(1.3062)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.1858)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.9254)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.3206)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.1338)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2429)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.1446)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2818)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2391)
Global Model Acc on global data: 0.5401 length of data: 10000
-------------Round number:  38  -------------
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.2773)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.2651)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.7041)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.4307)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.4067)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.3072)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.2743)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.1661)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.2709)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.3274)
Global Model Acc on global data: 0.5595 length of data: 10000
save a model
-------------Round number:  39  -------------
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2037)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2854)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2368)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.1977)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2358)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.3097)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.1684)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2133)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.1122)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.5965)
Global Model Acc on global data: 0.529 length of data: 10000
-------------Round number:  40  -------------
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.6262)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.2772)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.3627)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.3295)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.5615)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.5624)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.3559)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.3488)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.3439)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.3028)
Global Model Acc on global data: 0.5448 length of data: 10000
-------------Round number:  41  -------------
loss: CE learning rate: 0.000262
training loss: tensor(0.2874)
loss: CE learning rate: 0.000262
training loss: tensor(0.5458)
loss: CE learning rate: 0.000262
training loss: tensor(0.0848)
loss: CE learning rate: 0.000262
training loss: tensor(0.2488)
loss: CE learning rate: 0.000262
training loss: tensor(0.3070)
loss: CE learning rate: 0.000262
training loss: tensor(1.0944)
loss: CE learning rate: 0.000262
training loss: tensor(0.2428)
loss: CE learning rate: 0.000262
training loss: tensor(0.1299)
loss: CE learning rate: 0.000262
training loss: tensor(0.1125)
loss: CE learning rate: 0.000262
training loss: tensor(0.3518)
Global Model Acc on global data: 0.5488 length of data: 10000
-------------Round number:  42  -------------
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.3829)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.2376)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.2262)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.2485)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.2506)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.2742)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.1535)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.2470)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.1140)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.0836)
Global Model Acc on global data: 0.5345 length of data: 10000
-------------Round number:  43  -------------
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.3087)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.0818)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2294)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.0761)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2501)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.1813)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2191)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.3067)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.5179)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.3708)
Global Model Acc on global data: 0.545 length of data: 10000
-------------Round number:  44  -------------
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.2047)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.3962)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.0800)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.2857)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.1791)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.3330)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.3411)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.5278)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.1756)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.6050)
Global Model Acc on global data: 0.5476 length of data: 10000
-------------Round number:  45  -------------
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.0353)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.4516)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.1495)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.3825)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.0280)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.4796)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.9042)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.4111)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.0590)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.2186)
Global Model Acc on global data: 0.5494 length of data: 10000
-------------Round number:  46  -------------
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.0662)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.1153)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.0051)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.0780)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.2403)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.3685)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.1451)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.1816)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.1713)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.1360)
Global Model Acc on global data: 0.5319 length of data: 10000
-------------Round number:  47  -------------
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.1502)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.4099)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.4623)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.2307)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.2995)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.5561)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.4311)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.2256)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.2240)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.3383)
Global Model Acc on global data: 0.5434 length of data: 10000
-------------Round number:  48  -------------
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2375)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2707)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.0533)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2555)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.4594)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2393)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.1504)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2921)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2772)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2083)
Global Model Acc on global data: 0.5429 length of data: 10000
-------------Round number:  49  -------------
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.3215)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.3544)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.3675)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.2047)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.2952)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.2912)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.4507)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.0657)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.1207)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.3990)
Global Model Acc on global data: 0.5461 length of data: 10000
-------------Round number:  50  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3211)
loss: CE learning rate: 0.0001
training loss: tensor(0.4476)
loss: CE learning rate: 0.0001
training loss: tensor(0.2304)
loss: CE learning rate: 0.0001
training loss: tensor(0.2728)
loss: CE learning rate: 0.0001
training loss: tensor(0.1892)
loss: CE learning rate: 0.0001
training loss: tensor(0.1402)
loss: CE learning rate: 0.0001
training loss: tensor(0.2649)
loss: CE learning rate: 0.0001
training loss: tensor(0.1054)
loss: CE learning rate: 0.0001
training loss: tensor(0.1554)
loss: CE learning rate: 0.0001
training loss: tensor(0.4923)
Global Model Acc on global data: 0.5475 length of data: 10000
-------------Round number:  51  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0196)
loss: CE learning rate: 0.0001
training loss: tensor(0.5149)
loss: CE learning rate: 0.0001
training loss: tensor(0.3650)
loss: CE learning rate: 0.0001
training loss: tensor(0.2818)
loss: CE learning rate: 0.0001
training loss: tensor(1.2501)
loss: CE learning rate: 0.0001
training loss: tensor(0.7662)
loss: CE learning rate: 0.0001
training loss: tensor(0.1782)
loss: CE learning rate: 0.0001
training loss: tensor(0.3371)
loss: CE learning rate: 0.0001
training loss: tensor(0.2586)
loss: CE learning rate: 0.0001
training loss: tensor(0.1263)
Global Model Acc on global data: 0.54 length of data: 10000
-------------Round number:  52  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1474)
loss: CE learning rate: 0.0001
training loss: tensor(0.1774)
loss: CE learning rate: 0.0001
training loss: tensor(0.1921)
loss: CE learning rate: 0.0001
training loss: tensor(0.4688)
loss: CE learning rate: 0.0001
training loss: tensor(0.0356)
loss: CE learning rate: 0.0001
training loss: tensor(0.1613)
loss: CE learning rate: 0.0001
training loss: tensor(0.0074)
loss: CE learning rate: 0.0001
training loss: tensor(0.3488)
loss: CE learning rate: 0.0001
training loss: tensor(0.4992)
loss: CE learning rate: 0.0001
training loss: tensor(0.2535)
Global Model Acc on global data: 0.5235 length of data: 10000
-------------Round number:  53  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2379)
loss: CE learning rate: 0.0001
training loss: tensor(0.8458)
loss: CE learning rate: 0.0001
training loss: tensor(0.4752)
loss: CE learning rate: 0.0001
training loss: tensor(0.6148)
loss: CE learning rate: 0.0001
training loss: tensor(0.0058)
loss: CE learning rate: 0.0001
training loss: tensor(1.6618)
loss: CE learning rate: 0.0001
training loss: tensor(1.1022)
loss: CE learning rate: 0.0001
training loss: tensor(0.1451)
loss: CE learning rate: 0.0001
training loss: tensor(0.3008)
loss: CE learning rate: 0.0001
training loss: tensor(0.4977)
Global Model Acc on global data: 0.5306 length of data: 10000
-------------Round number:  54  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3002)
loss: CE learning rate: 0.0001
training loss: tensor(0.1522)
loss: CE learning rate: 0.0001
training loss: tensor(0.3554)
loss: CE learning rate: 0.0001
training loss: tensor(0.4307)
loss: CE learning rate: 0.0001
training loss: tensor(0.3499)
loss: CE learning rate: 0.0001
training loss: tensor(0.2994)
loss: CE learning rate: 0.0001
training loss: tensor(0.3917)
loss: CE learning rate: 0.0001
training loss: tensor(0.1030)
loss: CE learning rate: 0.0001
training loss: tensor(0.1910)
loss: CE learning rate: 0.0001
training loss: tensor(0.0387)
Global Model Acc on global data: 0.521 length of data: 10000
-------------Round number:  55  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4130)
loss: CE learning rate: 0.0001
training loss: tensor(0.4013)
loss: CE learning rate: 0.0001
training loss: tensor(0.1881)
loss: CE learning rate: 0.0001
training loss: tensor(0.1358)
loss: CE learning rate: 0.0001
training loss: tensor(0.2155)
loss: CE learning rate: 0.0001
training loss: tensor(0.1182)
loss: CE learning rate: 0.0001
training loss: tensor(0.3190)
loss: CE learning rate: 0.0001
training loss: tensor(0.2705)
loss: CE learning rate: 0.0001
training loss: tensor(0.0483)
loss: CE learning rate: 0.0001
training loss: tensor(0.8475)
Global Model Acc on global data: 0.5316 length of data: 10000
-------------Round number:  56  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2958)
loss: CE learning rate: 0.0001
training loss: tensor(0.3110)
loss: CE learning rate: 0.0001
training loss: tensor(0.9223)
loss: CE learning rate: 0.0001
training loss: tensor(0.7904)
loss: CE learning rate: 0.0001
training loss: tensor(0.2685)
loss: CE learning rate: 0.0001
training loss: tensor(0.2249)
loss: CE learning rate: 0.0001
training loss: tensor(0.1251)
loss: CE learning rate: 0.0001
training loss: tensor(0.1829)
loss: CE learning rate: 0.0001
training loss: tensor(0.1644)
loss: CE learning rate: 0.0001
training loss: tensor(0.0858)
Global Model Acc on global data: 0.5447 length of data: 10000
-------------Round number:  57  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1480)
loss: CE learning rate: 0.0001
training loss: tensor(0.0507)
loss: CE learning rate: 0.0001
training loss: tensor(0.2749)
loss: CE learning rate: 0.0001
training loss: tensor(0.3540)
loss: CE learning rate: 0.0001
training loss: tensor(0.1636)
loss: CE learning rate: 0.0001
training loss: tensor(0.1048)
loss: CE learning rate: 0.0001
training loss: tensor(0.1486)
loss: CE learning rate: 0.0001
training loss: tensor(0.1743)
loss: CE learning rate: 0.0001
training loss: tensor(0.4668)
loss: CE learning rate: 0.0001
training loss: tensor(0.1872)
Global Model Acc on global data: 0.5393 length of data: 10000
-------------Round number:  58  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3631)
loss: CE learning rate: 0.0001
training loss: tensor(0.3902)
loss: CE learning rate: 0.0001
training loss: tensor(0.1002)
loss: CE learning rate: 0.0001
training loss: tensor(0.1427)
loss: CE learning rate: 0.0001
training loss: tensor(0.1368)
loss: CE learning rate: 0.0001
training loss: tensor(0.2809)
loss: CE learning rate: 0.0001
training loss: tensor(0.1486)
loss: CE learning rate: 0.0001
training loss: tensor(0.0703)
loss: CE learning rate: 0.0001
training loss: tensor(0.3402)
loss: CE learning rate: 0.0001
training loss: tensor(0.2500)
Global Model Acc on global data: 0.5367 length of data: 10000
-------------Round number:  59  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1471)
loss: CE learning rate: 0.0001
training loss: tensor(0.2127)
loss: CE learning rate: 0.0001
training loss: tensor(0.0815)
loss: CE learning rate: 0.0001
training loss: tensor(0.2445)
loss: CE learning rate: 0.0001
training loss: tensor(0.4280)
loss: CE learning rate: 0.0001
training loss: tensor(0.1012)
loss: CE learning rate: 0.0001
training loss: tensor(0.3040)
loss: CE learning rate: 0.0001
training loss: tensor(0.3782)
loss: CE learning rate: 0.0001
training loss: tensor(0.1553)
loss: CE learning rate: 0.0001
training loss: tensor(0.4315)
Global Model Acc on global data: 0.5387 length of data: 10000
-------------Round number:  60  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.6197)
loss: CE learning rate: 0.0001
training loss: tensor(0.2242)
loss: CE learning rate: 0.0001
training loss: tensor(0.2004)
loss: CE learning rate: 0.0001
training loss: tensor(0.1079)
loss: CE learning rate: 0.0001
training loss: tensor(0.2376)
loss: CE learning rate: 0.0001
training loss: tensor(0.3740)
loss: CE learning rate: 0.0001
training loss: tensor(0.2461)
loss: CE learning rate: 0.0001
training loss: tensor(0.2566)
loss: CE learning rate: 0.0001
training loss: tensor(0.2931)
loss: CE learning rate: 0.0001
training loss: tensor(0.1575)
Global Model Acc on global data: 0.5333 length of data: 10000
-------------Round number:  61  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2178)
loss: CE learning rate: 0.0001
training loss: tensor(0.2868)
loss: CE learning rate: 0.0001
training loss: tensor(0.1574)
loss: CE learning rate: 0.0001
training loss: tensor(0.0878)
loss: CE learning rate: 0.0001
training loss: tensor(0.1239)
loss: CE learning rate: 0.0001
training loss: tensor(0.1485)
loss: CE learning rate: 0.0001
training loss: tensor(0.3180)
loss: CE learning rate: 0.0001
training loss: tensor(0.0658)
loss: CE learning rate: 0.0001
training loss: tensor(0.2278)
loss: CE learning rate: 0.0001
training loss: tensor(0.1531)
Global Model Acc on global data: 0.5408 length of data: 10000
-------------Round number:  62  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2132)
loss: CE learning rate: 0.0001
training loss: tensor(0.1982)
loss: CE learning rate: 0.0001
training loss: tensor(0.1340)
loss: CE learning rate: 0.0001
training loss: tensor(0.1458)
loss: CE learning rate: 0.0001
training loss: tensor(0.0415)
loss: CE learning rate: 0.0001
training loss: tensor(0.3034)
loss: CE learning rate: 0.0001
training loss: tensor(0.7965)
loss: CE learning rate: 0.0001
training loss: tensor(0.2476)
loss: CE learning rate: 0.0001
training loss: tensor(0.1465)
loss: CE learning rate: 0.0001
training loss: tensor(0.0972)
Global Model Acc on global data: 0.5313 length of data: 10000
-------------Round number:  63  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3278)
loss: CE learning rate: 0.0001
training loss: tensor(0.1284)
loss: CE learning rate: 0.0001
training loss: tensor(0.1416)
loss: CE learning rate: 0.0001
training loss: tensor(0.0301)
loss: CE learning rate: 0.0001
training loss: tensor(0.0850)
loss: CE learning rate: 0.0001
training loss: tensor(0.1484)
loss: CE learning rate: 0.0001
training loss: tensor(0.0851)
loss: CE learning rate: 0.0001
training loss: tensor(0.1368)
loss: CE learning rate: 0.0001
training loss: tensor(0.2911)
loss: CE learning rate: 0.0001
training loss: tensor(0.4139)
Global Model Acc on global data: 0.5236 length of data: 10000
-------------Round number:  64  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0262)
loss: CE learning rate: 0.0001
training loss: tensor(0.0387)
loss: CE learning rate: 0.0001
training loss: tensor(0.6999)
loss: CE learning rate: 0.0001
training loss: tensor(0.2847)
loss: CE learning rate: 0.0001
training loss: tensor(0.7324)
loss: CE learning rate: 0.0001
training loss: tensor(0.2190)
loss: CE learning rate: 0.0001
training loss: tensor(0.0132)
loss: CE learning rate: 0.0001
training loss: tensor(0.2067)
loss: CE learning rate: 0.0001
training loss: tensor(0.1370)
loss: CE learning rate: 0.0001
training loss: tensor(0.4196)
Global Model Acc on global data: 0.5314 length of data: 10000
-------------Round number:  65  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0063)
loss: CE learning rate: 0.0001
training loss: tensor(0.2599)
loss: CE learning rate: 0.0001
training loss: tensor(0.0978)
loss: CE learning rate: 0.0001
training loss: tensor(0.4187)
loss: CE learning rate: 0.0001
training loss: tensor(0.1903)
loss: CE learning rate: 0.0001
training loss: tensor(0.2955)
loss: CE learning rate: 0.0001
training loss: tensor(0.0666)
loss: CE learning rate: 0.0001
training loss: tensor(0.1435)
loss: CE learning rate: 0.0001
training loss: tensor(0.1865)
loss: CE learning rate: 0.0001
training loss: tensor(0.0888)
Global Model Acc on global data: 0.5361 length of data: 10000
-------------Round number:  66  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1083)
loss: CE learning rate: 0.0001
training loss: tensor(0.2497)
loss: CE learning rate: 0.0001
training loss: tensor(0.1900)
loss: CE learning rate: 0.0001
training loss: tensor(0.2943)
loss: CE learning rate: 0.0001
training loss: tensor(0.1593)
loss: CE learning rate: 0.0001
training loss: tensor(0.3294)
loss: CE learning rate: 0.0001
training loss: tensor(0.0663)
loss: CE learning rate: 0.0001
training loss: tensor(0.1046)
loss: CE learning rate: 0.0001
training loss: tensor(0.4666)
loss: CE learning rate: 0.0001
training loss: tensor(0.6494)
Global Model Acc on global data: 0.5439 length of data: 10000
-------------Round number:  67  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0221)
loss: CE learning rate: 0.0001
training loss: tensor(0.0630)
loss: CE learning rate: 0.0001
training loss: tensor(0.1861)
loss: CE learning rate: 0.0001
training loss: tensor(0.1657)
loss: CE learning rate: 0.0001
training loss: tensor(0.5048)
loss: CE learning rate: 0.0001
training loss: tensor(0.0475)
loss: CE learning rate: 0.0001
training loss: tensor(0.3094)
loss: CE learning rate: 0.0001
training loss: tensor(0.1099)
loss: CE learning rate: 0.0001
training loss: tensor(0.1299)
loss: CE learning rate: 0.0001
training loss: tensor(0.3207)
Global Model Acc on global data: 0.5334 length of data: 10000
-------------Round number:  68  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0156)
loss: CE learning rate: 0.0001
training loss: tensor(0.3094)
loss: CE learning rate: 0.0001
training loss: tensor(0.1930)
loss: CE learning rate: 0.0001
training loss: tensor(0.2248)
loss: CE learning rate: 0.0001
training loss: tensor(0.1847)
loss: CE learning rate: 0.0001
training loss: tensor(0.0033)
loss: CE learning rate: 0.0001
training loss: tensor(0.1569)
loss: CE learning rate: 0.0001
training loss: tensor(0.2374)
loss: CE learning rate: 0.0001
training loss: tensor(0.2463)
loss: CE learning rate: 0.0001
training loss: tensor(0.0686)
Global Model Acc on global data: 0.536 length of data: 10000
-------------Round number:  69  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2114)
loss: CE learning rate: 0.0001
training loss: tensor(0.1122)
loss: CE learning rate: 0.0001
training loss: tensor(0.4128)
loss: CE learning rate: 0.0001
training loss: tensor(0.1568)
loss: CE learning rate: 0.0001
training loss: tensor(0.0816)
loss: CE learning rate: 0.0001
training loss: tensor(0.1063)
loss: CE learning rate: 0.0001
training loss: tensor(0.4877)
loss: CE learning rate: 0.0001
training loss: tensor(0.2260)
loss: CE learning rate: 0.0001
training loss: tensor(0.1491)
loss: CE learning rate: 0.0001
training loss: tensor(0.3580)
Global Model Acc on global data: 0.534 length of data: 10000
-------------Round number:  70  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2435)
loss: CE learning rate: 0.0001
training loss: tensor(0.4317)
loss: CE learning rate: 0.0001
training loss: tensor(0.4280)
loss: CE learning rate: 0.0001
training loss: tensor(1.0965)
loss: CE learning rate: 0.0001
training loss: tensor(0.3318)
loss: CE learning rate: 0.0001
training loss: tensor(0.0050)
loss: CE learning rate: 0.0001
training loss: tensor(0.0945)
loss: CE learning rate: 0.0001
training loss: tensor(0.1404)
loss: CE learning rate: 0.0001
training loss: tensor(0.1635)
loss: CE learning rate: 0.0001
training loss: tensor(0.0181)
Global Model Acc on global data: 0.5228 length of data: 10000
-------------Round number:  71  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2928)
loss: CE learning rate: 0.0001
training loss: tensor(0.1846)
loss: CE learning rate: 0.0001
training loss: tensor(0.0756)
loss: CE learning rate: 0.0001
training loss: tensor(0.0512)
loss: CE learning rate: 0.0001
training loss: tensor(0.1350)
loss: CE learning rate: 0.0001
training loss: tensor(0.1909)
loss: CE learning rate: 0.0001
training loss: tensor(0.4947)
loss: CE learning rate: 0.0001
training loss: tensor(0.1716)
loss: CE learning rate: 0.0001
training loss: tensor(0.1399)
loss: CE learning rate: 0.0001
training loss: tensor(0.0436)
Global Model Acc on global data: 0.5228 length of data: 10000
-------------Round number:  72  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1049)
loss: CE learning rate: 0.0001
training loss: tensor(0.1067)
loss: CE learning rate: 0.0001
training loss: tensor(0.2686)
loss: CE learning rate: 0.0001
training loss: tensor(0.1693)
loss: CE learning rate: 0.0001
training loss: tensor(0.1185)
loss: CE learning rate: 0.0001
training loss: tensor(0.0533)
loss: CE learning rate: 0.0001
training loss: tensor(0.4909)
loss: CE learning rate: 0.0001
training loss: tensor(0.2028)
loss: CE learning rate: 0.0001
training loss: tensor(0.2409)
loss: CE learning rate: 0.0001
training loss: tensor(0.0214)
Global Model Acc on global data: 0.5362 length of data: 10000
-------------Round number:  73  -------------
loss: CE learning rate: 0.0001
training loss: tensor(1.1694)
loss: CE learning rate: 0.0001
training loss: tensor(0.1221)
loss: CE learning rate: 0.0001
training loss: tensor(0.1388)
loss: CE learning rate: 0.0001
training loss: tensor(0.0824)
loss: CE learning rate: 0.0001
training loss: tensor(0.2466)
loss: CE learning rate: 0.0001
training loss: tensor(0.4280)
loss: CE learning rate: 0.0001
training loss: tensor(0.5098)
loss: CE learning rate: 0.0001
training loss: tensor(0.0384)
loss: CE learning rate: 0.0001
training loss: tensor(0.1171)
loss: CE learning rate: 0.0001
training loss: tensor(0.0655)
Global Model Acc on global data: 0.5313 length of data: 10000
-------------Round number:  74  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0729)
loss: CE learning rate: 0.0001
training loss: tensor(0.0987)
loss: CE learning rate: 0.0001
training loss: tensor(0.1176)
loss: CE learning rate: 0.0001
training loss: tensor(0.0517)
loss: CE learning rate: 0.0001
training loss: tensor(0.2439)
loss: CE learning rate: 0.0001
training loss: tensor(0.2742)
loss: CE learning rate: 0.0001
training loss: tensor(0.1980)
loss: CE learning rate: 0.0001
training loss: tensor(0.0260)
loss: CE learning rate: 0.0001
training loss: tensor(0.2451)
loss: CE learning rate: 0.0001
training loss: tensor(0.0976)
Global Model Acc on global data: 0.5325 length of data: 10000
-------------Round number:  75  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2692)
loss: CE learning rate: 0.0001
training loss: tensor(0.1277)
loss: CE learning rate: 0.0001
training loss: tensor(0.1281)
loss: CE learning rate: 0.0001
training loss: tensor(0.3112)
loss: CE learning rate: 0.0001
training loss: tensor(0.1583)
loss: CE learning rate: 0.0001
training loss: tensor(0.3785)
loss: CE learning rate: 0.0001
training loss: tensor(0.0186)
loss: CE learning rate: 0.0001
training loss: tensor(0.0029)
loss: CE learning rate: 0.0001
training loss: tensor(0.1938)
loss: CE learning rate: 0.0001
training loss: tensor(0.4056)
Global Model Acc on global data: 0.5346 length of data: 10000
-------------Round number:  76  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2500)
loss: CE learning rate: 0.0001
training loss: tensor(0.0015)
loss: CE learning rate: 0.0001
training loss: tensor(1.3503)
loss: CE learning rate: 0.0001
training loss: tensor(0.3270)
loss: CE learning rate: 0.0001
training loss: tensor(0.1116)
loss: CE learning rate: 0.0001
training loss: tensor(0.2201)
loss: CE learning rate: 0.0001
training loss: tensor(0.1940)
loss: CE learning rate: 0.0001
training loss: tensor(0.2652)
loss: CE learning rate: 0.0001
training loss: tensor(0.1236)
loss: CE learning rate: 0.0001
training loss: tensor(0.5518)
Global Model Acc on global data: 0.531 length of data: 10000
-------------Round number:  77  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2274)
loss: CE learning rate: 0.0001
training loss: tensor(0.1388)
loss: CE learning rate: 0.0001
training loss: tensor(0.2249)
loss: CE learning rate: 0.0001
training loss: tensor(0.3654)
loss: CE learning rate: 0.0001
training loss: tensor(0.0974)
loss: CE learning rate: 0.0001
training loss: tensor(0.1078)
loss: CE learning rate: 0.0001
training loss: tensor(0.1820)
loss: CE learning rate: 0.0001
training loss: tensor(0.1336)
loss: CE learning rate: 0.0001
training loss: tensor(0.2331)
loss: CE learning rate: 0.0001
training loss: tensor(0.3648)
Global Model Acc on global data: 0.5295 length of data: 10000
-------------Round number:  78  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0291)
loss: CE learning rate: 0.0001
training loss: tensor(0.2131)
loss: CE learning rate: 0.0001
training loss: tensor(0.0825)
loss: CE learning rate: 0.0001
training loss: tensor(0.1075)
loss: CE learning rate: 0.0001
training loss: tensor(0.2121)
loss: CE learning rate: 0.0001
training loss: tensor(0.0089)
loss: CE learning rate: 0.0001
training loss: tensor(0.1037)
loss: CE learning rate: 0.0001
training loss: tensor(0.1003)
loss: CE learning rate: 0.0001
training loss: tensor(0.1039)
loss: CE learning rate: 0.0001
training loss: tensor(0.1590)
Global Model Acc on global data: 0.5355 length of data: 10000
-------------Round number:  79  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2293)
loss: CE learning rate: 0.0001
training loss: tensor(0.1140)
loss: CE learning rate: 0.0001
training loss: tensor(0.1045)
loss: CE learning rate: 0.0001
training loss: tensor(0.4890)
loss: CE learning rate: 0.0001
training loss: tensor(0.1182)
loss: CE learning rate: 0.0001
training loss: tensor(1.0376)
loss: CE learning rate: 0.0001
training loss: tensor(0.2713)
loss: CE learning rate: 0.0001
training loss: tensor(0.0796)
loss: CE learning rate: 0.0001
training loss: tensor(0.2681)
loss: CE learning rate: 0.0001
training loss: tensor(0.1854)
Global Model Acc on global data: 0.5346 length of data: 10000
-------------Round number:  80  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0022)
loss: CE learning rate: 0.0001
training loss: tensor(0.4458)
loss: CE learning rate: 0.0001
training loss: tensor(0.0271)
loss: CE learning rate: 0.0001
training loss: tensor(0.1365)
loss: CE learning rate: 0.0001
training loss: tensor(0.5393)
loss: CE learning rate: 0.0001
training loss: tensor(0.1729)
loss: CE learning rate: 0.0001
training loss: tensor(0.0886)
loss: CE learning rate: 0.0001
training loss: tensor(0.0408)
loss: CE learning rate: 0.0001
training loss: tensor(0.2233)
loss: CE learning rate: 0.0001
training loss: tensor(0.1507)
Global Model Acc on global data: 0.5232 length of data: 10000
-------------Round number:  81  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0304)
loss: CE learning rate: 0.0001
training loss: tensor(0.4214)
loss: CE learning rate: 0.0001
training loss: tensor(0.1822)
loss: CE learning rate: 0.0001
training loss: tensor(0.9353)
loss: CE learning rate: 0.0001
training loss: tensor(0.3553)
loss: CE learning rate: 0.0001
training loss: tensor(0.3195)
loss: CE learning rate: 0.0001
training loss: tensor(0.1063)
loss: CE learning rate: 0.0001
training loss: tensor(0.0876)
loss: CE learning rate: 0.0001
training loss: tensor(0.0953)
loss: CE learning rate: 0.0001
training loss: tensor(0.1713)
Global Model Acc on global data: 0.5245 length of data: 10000
-------------Round number:  82  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2325)
loss: CE learning rate: 0.0001
training loss: tensor(0.0329)
loss: CE learning rate: 0.0001
training loss: tensor(0.2242)
loss: CE learning rate: 0.0001
training loss: tensor(0.0083)
loss: CE learning rate: 0.0001
training loss: tensor(0.1251)
loss: CE learning rate: 0.0001
training loss: tensor(0.1045)
loss: CE learning rate: 0.0001
training loss: tensor(0.3018)
loss: CE learning rate: 0.0001
training loss: tensor(0.1032)
loss: CE learning rate: 0.0001
training loss: tensor(0.2109)
loss: CE learning rate: 0.0001
training loss: tensor(0.1596)
Global Model Acc on global data: 0.5327 length of data: 10000
-------------Round number:  83  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0940)
loss: CE learning rate: 0.0001
training loss: tensor(0.2734)
loss: CE learning rate: 0.0001
training loss: tensor(0.0130)
loss: CE learning rate: 0.0001
training loss: tensor(0.0525)
loss: CE learning rate: 0.0001
training loss: tensor(0.3010)
loss: CE learning rate: 0.0001
training loss: tensor(0.1698)
loss: CE learning rate: 0.0001
training loss: tensor(0.0773)
loss: CE learning rate: 0.0001
training loss: tensor(0.0441)
loss: CE learning rate: 0.0001
training loss: tensor(0.3696)
loss: CE learning rate: 0.0001
training loss: tensor(0.2174)
Global Model Acc on global data: 0.5285 length of data: 10000
-------------Round number:  84  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1307)
loss: CE learning rate: 0.0001
training loss: tensor(0.1049)
loss: CE learning rate: 0.0001
training loss: tensor(0.0953)
loss: CE learning rate: 0.0001
training loss: tensor(0.1534)
loss: CE learning rate: 0.0001
training loss: tensor(0.2025)
loss: CE learning rate: 0.0001
training loss: tensor(0.3191)
loss: CE learning rate: 0.0001
training loss: tensor(1.1389)
loss: CE learning rate: 0.0001
training loss: tensor(0.0474)
loss: CE learning rate: 0.0001
training loss: tensor(0.0445)
loss: CE learning rate: 0.0001
training loss: tensor(0.1035)
Global Model Acc on global data: 0.5317 length of data: 10000
-------------Round number:  85  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2779)
loss: CE learning rate: 0.0001
training loss: tensor(0.1396)
loss: CE learning rate: 0.0001
training loss: tensor(0.1051)
loss: CE learning rate: 0.0001
training loss: tensor(0.2225)
loss: CE learning rate: 0.0001
training loss: tensor(0.1410)
loss: CE learning rate: 0.0001
training loss: tensor(0.5865)
loss: CE learning rate: 0.0001
training loss: tensor(0.0895)
loss: CE learning rate: 0.0001
training loss: tensor(0.1509)
loss: CE learning rate: 0.0001
training loss: tensor(0.2245)
loss: CE learning rate: 0.0001
training loss: tensor(0.2261)
Global Model Acc on global data: 0.5408 length of data: 10000
-------------Round number:  86  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0312)
loss: CE learning rate: 0.0001
training loss: tensor(0.1257)
loss: CE learning rate: 0.0001
training loss: tensor(0.1471)
loss: CE learning rate: 0.0001
training loss: tensor(0.2345)
loss: CE learning rate: 0.0001
training loss: tensor(0.0698)
loss: CE learning rate: 0.0001
training loss: tensor(0.0428)
loss: CE learning rate: 0.0001
training loss: tensor(0.9905)
loss: CE learning rate: 0.0001
training loss: tensor(0.4109)
loss: CE learning rate: 0.0001
training loss: tensor(0.0480)
loss: CE learning rate: 0.0001
training loss: tensor(0.3168)
Global Model Acc on global data: 0.5371 length of data: 10000
-------------Round number:  87  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0487)
loss: CE learning rate: 0.0001
training loss: tensor(0.1342)
loss: CE learning rate: 0.0001
training loss: tensor(0.0596)
loss: CE learning rate: 0.0001
training loss: tensor(1.1792)
loss: CE learning rate: 0.0001
training loss: tensor(0.1457)
loss: CE learning rate: 0.0001
training loss: tensor(0.1055)
loss: CE learning rate: 0.0001
training loss: tensor(0.3030)
loss: CE learning rate: 0.0001
training loss: tensor(0.0147)
loss: CE learning rate: 0.0001
training loss: tensor(0.2085)
loss: CE learning rate: 0.0001
training loss: tensor(0.0801)
Global Model Acc on global data: 0.5358 length of data: 10000
-------------Round number:  88  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0232)
loss: CE learning rate: 0.0001
training loss: tensor(0.1679)
loss: CE learning rate: 0.0001
training loss: tensor(0.3264)
loss: CE learning rate: 0.0001
training loss: tensor(0.0594)
loss: CE learning rate: 0.0001
training loss: tensor(0.0071)
loss: CE learning rate: 0.0001
training loss: tensor(0.0707)
loss: CE learning rate: 0.0001
training loss: tensor(0.1434)
loss: CE learning rate: 0.0001
training loss: tensor(0.0959)
loss: CE learning rate: 0.0001
training loss: tensor(0.1274)
loss: CE learning rate: 0.0001
training loss: tensor(0.3853)
Global Model Acc on global data: 0.5301 length of data: 10000
-------------Round number:  89  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1914)
loss: CE learning rate: 0.0001
training loss: tensor(0.2147)
loss: CE learning rate: 0.0001
training loss: tensor(0.1675)
loss: CE learning rate: 0.0001
training loss: tensor(0.0584)
loss: CE learning rate: 0.0001
training loss: tensor(0.1212)
loss: CE learning rate: 0.0001
training loss: tensor(0.0824)
loss: CE learning rate: 0.0001
training loss: tensor(0.1872)
loss: CE learning rate: 0.0001
training loss: tensor(0.3557)
loss: CE learning rate: 0.0001
training loss: tensor(0.1432)
loss: CE learning rate: 0.0001
training loss: tensor(0.1221)
Global Model Acc on global data: 0.5225 length of data: 10000
-------------Round number:  90  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1087)
loss: CE learning rate: 0.0001
training loss: tensor(0.0226)
loss: CE learning rate: 0.0001
training loss: tensor(0.2731)
loss: CE learning rate: 0.0001
training loss: tensor(0.2378)
loss: CE learning rate: 0.0001
training loss: tensor(0.0880)
loss: CE learning rate: 0.0001
training loss: tensor(0.1288)
loss: CE learning rate: 0.0001
training loss: tensor(0.0709)
loss: CE learning rate: 0.0001
training loss: tensor(0.2181)
loss: CE learning rate: 0.0001
training loss: tensor(0.1654)
loss: CE learning rate: 0.0001
training loss: tensor(0.2867)
Global Model Acc on global data: 0.5211 length of data: 10000
-------------Round number:  91  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1289)
loss: CE learning rate: 0.0001
training loss: tensor(0.0657)
loss: CE learning rate: 0.0001
training loss: tensor(0.2308)
loss: CE learning rate: 0.0001
training loss: tensor(0.1608)
loss: CE learning rate: 0.0001
training loss: tensor(1.1272)
loss: CE learning rate: 0.0001
training loss: tensor(0.1372)
loss: CE learning rate: 0.0001
training loss: tensor(0.0867)
loss: CE learning rate: 0.0001
training loss: tensor(0.4862)
loss: CE learning rate: 0.0001
training loss: tensor(0.1730)
loss: CE learning rate: 0.0001
training loss: tensor(0.2529)
Global Model Acc on global data: 0.5401 length of data: 10000
-------------Round number:  92  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0223)
loss: CE learning rate: 0.0001
training loss: tensor(0.2189)
loss: CE learning rate: 0.0001
training loss: tensor(0.0559)
loss: CE learning rate: 0.0001
training loss: tensor(0.2845)
loss: CE learning rate: 0.0001
training loss: tensor(0.3901)
loss: CE learning rate: 0.0001
training loss: tensor(0.0601)
loss: CE learning rate: 0.0001
training loss: tensor(0.4155)
loss: CE learning rate: 0.0001
training loss: tensor(0.1889)
loss: CE learning rate: 0.0001
training loss: tensor(0.5508)
loss: CE learning rate: 0.0001
training loss: tensor(0.0563)
Global Model Acc on global data: 0.5348 length of data: 10000
-------------Round number:  93  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0812)
loss: CE learning rate: 0.0001
training loss: tensor(0.0599)
loss: CE learning rate: 0.0001
training loss: tensor(0.1305)
loss: CE learning rate: 0.0001
training loss: tensor(0.0566)
loss: CE learning rate: 0.0001
training loss: tensor(0.1275)
loss: CE learning rate: 0.0001
training loss: tensor(0.2133)
loss: CE learning rate: 0.0001
training loss: tensor(0.1015)
loss: CE learning rate: 0.0001
training loss: tensor(0.1681)
loss: CE learning rate: 0.0001
training loss: tensor(0.1631)
loss: CE learning rate: 0.0001
training loss: tensor(0.1825)
Global Model Acc on global data: 0.5342 length of data: 10000
-------------Round number:  94  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4975)
loss: CE learning rate: 0.0001
training loss: tensor(0.0393)
loss: CE learning rate: 0.0001
training loss: tensor(0.0692)
loss: CE learning rate: 0.0001
training loss: tensor(0.0760)
loss: CE learning rate: 0.0001
training loss: tensor(0.1273)
loss: CE learning rate: 0.0001
training loss: tensor(0.1612)
loss: CE learning rate: 0.0001
training loss: tensor(0.3105)
loss: CE learning rate: 0.0001
training loss: tensor(0.1804)
loss: CE learning rate: 0.0001
training loss: tensor(0.0907)
loss: CE learning rate: 0.0001
training loss: tensor(0.0136)
Global Model Acc on global data: 0.5333 length of data: 10000
-------------Round number:  95  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4269)
loss: CE learning rate: 0.0001
training loss: tensor(0.0455)
loss: CE learning rate: 0.0001
training loss: tensor(0.0648)
loss: CE learning rate: 0.0001
training loss: tensor(0.3400)
loss: CE learning rate: 0.0001
training loss: tensor(0.0156)
loss: CE learning rate: 0.0001
training loss: tensor(0.3116)
loss: CE learning rate: 0.0001
training loss: tensor(0.2097)
loss: CE learning rate: 0.0001
training loss: tensor(0.0140)
loss: CE learning rate: 0.0001
training loss: tensor(0.2196)
loss: CE learning rate: 0.0001
training loss: tensor(0.1098)
Global Model Acc on global data: 0.5414 length of data: 10000
-------------Round number:  96  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1175)
loss: CE learning rate: 0.0001
training loss: tensor(0.1897)
loss: CE learning rate: 0.0001
training loss: tensor(0.1236)
loss: CE learning rate: 0.0001
training loss: tensor(0.1809)
loss: CE learning rate: 0.0001
training loss: tensor(0.1102)
loss: CE learning rate: 0.0001
training loss: tensor(0.0055)
loss: CE learning rate: 0.0001
training loss: tensor(0.0495)
loss: CE learning rate: 0.0001
training loss: tensor(0.0513)
loss: CE learning rate: 0.0001
training loss: tensor(0.1550)
loss: CE learning rate: 0.0001
training loss: tensor(0.6051)
Global Model Acc on global data: 0.5318 length of data: 10000
-------------Round number:  97  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0676)
loss: CE learning rate: 0.0001
training loss: tensor(0.2600)
loss: CE learning rate: 0.0001
training loss: tensor(0.0925)
loss: CE learning rate: 0.0001
training loss: tensor(0.0586)
loss: CE learning rate: 0.0001
training loss: tensor(0.0225)
loss: CE learning rate: 0.0001
training loss: tensor(0.0050)
loss: CE learning rate: 0.0001
training loss: tensor(0.1352)
loss: CE learning rate: 0.0001
training loss: tensor(0.1133)
loss: CE learning rate: 0.0001
training loss: tensor(0.1527)
loss: CE learning rate: 0.0001
training loss: tensor(0.1957)
Global Model Acc on global data: 0.5251 length of data: 10000
-------------Round number:  98  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4807)
loss: CE learning rate: 0.0001
training loss: tensor(0.1384)
loss: CE learning rate: 0.0001
training loss: tensor(0.1462)
loss: CE learning rate: 0.0001
training loss: tensor(0.0846)
loss: CE learning rate: 0.0001
training loss: tensor(0.0163)
loss: CE learning rate: 0.0001
training loss: tensor(0.0719)
loss: CE learning rate: 0.0001
training loss: tensor(0.1044)
loss: CE learning rate: 0.0001
training loss: tensor(0.3165)
loss: CE learning rate: 0.0001
training loss: tensor(0.0824)
loss: CE learning rate: 0.0001
training loss: tensor(0.1559)
Global Model Acc on global data: 0.5172 length of data: 10000
-------------Round number:  99  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0393)
loss: CE learning rate: 0.0001
training loss: tensor(0.0628)
loss: CE learning rate: 0.0001
training loss: tensor(0.0918)
loss: CE learning rate: 0.0001
training loss: tensor(0.9286)
loss: CE learning rate: 0.0001
training loss: tensor(0.1230)
loss: CE learning rate: 0.0001
training loss: tensor(0.2020)
loss: CE learning rate: 0.0001
training loss: tensor(0.1196)
loss: CE learning rate: 0.0001
training loss: tensor(0.2523)
loss: CE learning rate: 0.0001
training loss: tensor(0.0608)
loss: CE learning rate: 0.0001
training loss: tensor(0.0082)
Global Model Acc on global data: 0.5289 length of data: 10000
================================================================================
Summary of training process:
Dataset                : Cifar10
Batch size             : 64
Learing rate           : 0.001
Number of total clients: 100
Split method           : distribution
Split parameter        : 1.0
Clients per round      : 10
Number of global rounds: 100
Number of local rounds : 10
Feature from layer     : 11
Feature percentage     : 0.1
Local training loss    : NT_CE
Loss of beta           : 1.0
Algorithm              : FedNTD
Modelname              : MOBNET
Mode                   : training
Seed                   : 0
================================================================================
Files already downloaded and verified
Files already downloaded and verified
Here
Done
Class frequencies:
client,class0,class1,class2,class3,class4,class5,class6,class7,class8,class9,Amount
Client   0,0.030,0.038,0.170,0.050,0.006,0.110,0.088,0.146,0.114,0.250,501
Client   1,0.185,0.153,0.00,0.052,0.045,0.072,0.00,0.095,0.150,0.250,601
Client   2,0.116,0.061,0.133,0.177,0.110,0.041,0.026,0.057,0.279,0.00,610
Client   3,0.030,0.075,0.144,0.033,0.234,0.063,0.048,0.075,0.210,0.090,334
Client   4,0.042,0.023,0.069,0.031,0.023,0.027,0.025,0.084,0.322,0.354,478
Client   5,0.103,0.451,0.099,0.009,0.023,0.026,0.033,0.007,0.171,0.077,426
Client   6,0.187,0.136,0.033,0.041,0.012,0.022,0.138,0.196,0.236,0.00,509
Client   7,0.019,0.035,0.010,0.027,0.085,0.022,0.083,0.034,0.259,0.426,625
Client   8,0.010,0.221,0.014,0.019,0.027,0.171,0.539,0.00,0.00,0.00,592
Client   9,0.035,0.063,0.033,0.056,0.038,0.161,0.090,0.035,0.163,0.327,735
Client  10,0.226,0.027,0.039,0.027,0.065,0.044,0.126,0.277,0.168,0.00,585
Client  11,0.073,0.145,0.021,0.043,0.038,0.103,0.009,0.209,0.098,0.261,234
Client  12,0.011,0.051,0.021,0.151,0.030,0.091,0.042,0.285,0.319,0.00,571
Client  13,0.008,0.051,0.107,0.027,0.035,0.011,0.238,0.112,0.043,0.369,374
Client  14,0.046,0.109,0.228,0.352,0.133,0.012,0.081,0.040,0.00,0.00,505
Client  15,0.007,0.196,0.017,0.063,0.029,0.419,0.070,0.010,0.058,0.131,413
Client  16,0.069,0.032,0.262,0.052,0.013,0.146,0.004,0.074,0.044,0.303,679
Client  17,0.089,0.045,0.206,0.092,0.006,0.066,0.021,0.095,0.380,0.00,671
Client  18,0.062,0.310,0.049,0.204,0.015,0.056,0.084,0.220,0.00,0.00,549
Client  19,0.122,0.025,0.466,0.144,0.243,0.00,0.00,0.00,0.00,0.00,597
Client  20,0.091,0.158,0.017,0.065,0.041,0.129,0.065,0.179,0.067,0.189,418
Client  21,0.110,0.166,0.030,0.157,0.00,0.025,0.120,0.102,0.289,0.00,591
Client  22,0.027,0.123,0.216,0.109,0.189,0.016,0.130,0.014,0.176,0.00,561
Client  23,0.164,0.029,0.027,0.252,0.137,0.038,0.027,0.066,0.027,0.235,452
Client  24,0.069,0.054,0.065,0.072,0.180,0.113,0.281,0.030,0.017,0.120,540
Client  25,0.150,0.110,0.018,0.204,0.004,0.105,0.049,0.034,0.034,0.293,447
Client  26,0.068,0.098,0.088,0.015,0.009,0.051,0.034,0.030,0.045,0.562,468
Client  27,0.067,0.275,0.009,0.016,0.046,0.106,0.033,0.158,0.129,0.162,568
Client  28,0.005,0.057,0.069,0.017,0.104,0.084,0.366,0.040,0.030,0.228,404
Client  29,0.062,0.066,0.160,0.027,0.008,0.012,0.289,0.094,0.282,0.00,595
Client  30,0.019,0.062,0.190,0.093,0.062,0.050,0.228,0.081,0.211,0.005,421
Client  31,0.020,0.124,0.037,0.057,0.201,0.119,0.072,0.117,0.132,0.122,403
Client  32,0.081,0.083,0.130,0.010,0.119,0.073,0.023,0.034,0.055,0.392,385
Client  33,0.092,0.314,0.016,0.149,0.084,0.035,0.287,0.022,0.00,0.00,509
Client  34,0.043,0.126,0.266,0.185,0.239,0.053,0.028,0.059,0.00,0.00,507
Client  35,0.017,0.058,0.322,0.053,0.102,0.135,0.043,0.054,0.216,0.00,606
Client  36,0.152,0.073,0.186,0.031,0.169,0.033,0.010,0.219,0.058,0.067,479
Client  37,0.285,0.024,0.041,0.029,0.006,0.164,0.084,0.137,0.229,0.00,628
Client  38,0.218,0.036,0.088,0.055,0.097,0.027,0.012,0.042,0.121,0.303,330
Client  39,0.011,0.068,0.092,0.279,0.068,0.014,0.014,0.141,0.136,0.179,369
Client  40,0.154,0.026,0.003,0.025,0.046,0.003,0.109,0.020,0.159,0.455,649
Client  41,0.007,0.044,0.056,0.322,0.065,0.061,0.080,0.017,0.015,0.334,413
Client  42,0.060,0.023,0.271,0.006,0.050,0.175,0.194,0.027,0.194,0.00,520
Client  43,0.406,0.082,0.003,0.023,0.237,0.014,0.020,0.073,0.127,0.017,355
Client  44,0.031,0.027,0.236,0.100,0.253,0.110,0.133,0.110,0.00,0.00,518
Client  45,0.035,0.126,0.113,0.100,0.013,0.026,0.278,0.017,0.204,0.087,230
Client  46,0.155,0.137,0.040,0.258,0.149,0.002,0.058,0.201,0.00,0.00,503
Client  47,0.333,0.026,0.119,0.117,0.037,0.133,0.056,0.047,0.070,0.063,429
Client  48,0.038,0.372,0.147,0.053,0.059,0.055,0.005,0.271,0.00,0.00,546
Client  49,0.027,0.029,0.012,0.018,0.271,0.082,0.076,0.107,0.012,0.366,513
Client  50,0.032,0.014,0.346,0.017,0.046,0.101,0.121,0.092,0.130,0.101,347
Client  51,0.358,0.112,0.180,0.320,0.008,0.008,0.014,0.00,0.00,0.00,500
Client  52,0.098,0.013,0.163,0.075,0.117,0.002,0.293,0.017,0.071,0.151,478
Client  53,0.208,0.063,0.029,0.308,0.118,0.275,0.00,0.00,0.00,0.00,510
Client  54,0.035,0.074,0.030,0.248,0.058,0.049,0.100,0.081,0.056,0.268,568
Client  55,0.008,0.249,0.008,0.277,0.018,0.440,0.00,0.00,0.00,0.00,502
Client  56,0.047,0.138,0.009,0.111,0.239,0.074,0.016,0.061,0.306,0.00,687
Client  57,0.026,0.065,0.043,0.002,0.036,0.022,0.103,0.361,0.156,0.187,507
Client  58,0.079,0.217,0.061,0.178,0.097,0.368,0.00,0.00,0.00,0.00,506
Client  59,0.081,0.169,0.318,0.008,0.179,0.035,0.010,0.200,0.00,0.00,509
Client  60,0.027,0.016,0.033,0.123,0.006,0.349,0.106,0.011,0.329,0.00,705
Client  61,0.039,0.114,0.091,0.171,0.030,0.159,0.167,0.130,0.065,0.033,508
Client  62,0.003,0.100,0.006,0.173,0.128,0.116,0.145,0.034,0.031,0.264,648
Client  63,0.069,0.132,0.057,0.033,0.230,0.065,0.278,0.045,0.033,0.057,418
Client  64,0.039,0.050,0.013,0.161,0.094,0.131,0.142,0.030,0.083,0.258,542
Client  65,0.090,0.003,0.189,0.037,0.048,0.185,0.284,0.163,0.00,0.00,588
Client  66,0.196,0.044,0.063,0.035,0.083,0.093,0.094,0.391,0.00,0.00,540
Client  67,0.074,0.004,0.145,0.004,0.495,0.278,0.00,0.00,0.00,0.00,503
Client  68,0.012,0.036,0.017,0.010,0.046,0.017,0.017,0.065,0.072,0.708,414
Client  69,0.296,0.192,0.132,0.045,0.014,0.018,0.204,0.055,0.006,0.038,494
Client  70,0.167,0.061,0.406,0.131,0.130,0.106,0.00,0.00,0.00,0.00,540
Client  71,0.046,0.020,0.027,0.108,0.179,0.207,0.201,0.060,0.150,0.00,546
Client  72,0.039,0.261,0.076,0.061,0.002,0.011,0.007,0.345,0.197,0.00,537
Client  73,0.296,0.020,0.010,0.298,0.039,0.125,0.088,0.124,0.00,0.00,510
Client  74,0.054,0.139,0.246,0.037,0.070,0.048,0.126,0.039,0.128,0.113,460
Client  75,0.110,0.030,0.002,0.114,0.184,0.042,0.291,0.066,0.082,0.078,499
Client  76,0.411,0.013,0.002,0.074,0.025,0.009,0.101,0.366,0.00,0.00,557
Client  77,0.007,0.093,0.057,0.073,0.177,0.023,0.007,0.147,0.087,0.330,300
Client  78,0.054,0.049,0.003,0.005,0.132,0.130,0.280,0.067,0.010,0.269,386
Client  79,0.027,0.082,0.012,0.051,0.133,0.039,0.329,0.327,0.00,0.00,513
Client  80,0.257,0.00,0.056,0.021,0.160,0.506,0.00,0.00,0.00,0.00,536
Client  81,0.085,0.050,0.312,0.025,0.088,0.010,0.310,0.015,0.106,0.00,520
Client  82,0.005,0.037,0.136,0.198,0.285,0.115,0.030,0.047,0.014,0.133,572
Client  83,0.151,0.024,0.070,0.209,0.064,0.121,0.087,0.141,0.133,0.00,503
Client  84,0.110,0.131,0.046,0.065,0.058,0.210,0.018,0.362,0.00,0.00,566
Client  85,0.107,0.024,0.033,0.228,0.239,0.031,0.042,0.108,0.190,0.00,553
Client  86,0.324,0.044,0.056,0.073,0.321,0.060,0.023,0.100,0.00,0.00,521
Client  87,0.212,0.008,0.008,0.053,0.021,0.120,0.214,0.143,0.222,0.00,532
Client  88,0.148,0.160,0.086,0.051,0.121,0.058,0.201,0.103,0.072,0.00,513
Client  89,0.025,0.195,0.338,0.135,0.174,0.132,0.00,0.00,0.00,0.00,517
Client  90,0.016,0.395,0.165,0.054,0.171,0.200,0.00,0.00,0.00,0.00,504
Client  91,0.035,0.388,0.058,0.045,0.088,0.018,0.020,0.090,0.183,0.075,399
Client  92,0.039,0.111,0.098,0.007,0.160,0.186,0.046,0.049,0.124,0.182,307
Client  93,0.121,0.047,0.103,0.224,0.222,0.037,0.018,0.079,0.018,0.129,379
Client  94,0.161,0.166,0.199,0.030,0.033,0.035,0.035,0.202,0.106,0.033,367
Client  95,0.081,0.135,0.010,0.111,0.164,0.006,0.143,0.265,0.00,0.085,495
Client  96,0.010,0.262,0.081,0.008,0.043,0.096,0.242,0.020,0.128,0.111,397
Client  97,0.055,0.027,0.018,0.191,0.205,0.320,0.068,0.011,0.093,0.011,440
Client  98,0.307,0.043,0.083,0.002,0.026,0.201,0.021,0.120,0.196,0.00,606
Client  99,0.129,0.160,0.055,0.311,0.091,0.050,0.020,0.182,0.002,0.00,505
Num_samples of Training set per client: [501, 601, 610, 334, 478, 426, 509, 625, 592, 735, 585, 234, 571, 374, 505, 413, 679, 671, 549, 597, 418, 591, 561, 452, 540, 447, 468, 568, 404, 595, 421, 403, 385, 509, 507, 606, 479, 628, 330, 369, 649, 413, 520, 355, 518, 230, 503, 429, 546, 513, 347, 500, 478, 510, 568, 502, 687, 507, 506, 509, 705, 508, 648, 418, 542, 588, 540, 503, 414, 494, 540, 546, 537, 510, 460, 499, 557, 300, 386, 513, 536, 520, 572, 503, 566, 553, 521, 532, 513, 517, 504, 399, 307, 379, 367, 495, 397, 440, 606, 505]
Total_training_samples: 50000
Global test set: 10000
Finish Generating Samples, distribution saved
MobileNetV2(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layers): Sequential(
    (0): Block(
      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Block(
      (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (6): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (7): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (8): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (9): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (10): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (12): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (13): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (14): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (15): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (16): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv2): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear): Linear(in_features=1280, out_features=10, bias=True)
)
number of parameters: 2296922
clients initializting...
output size: 10
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<01:30,  1.09it/s]  2%|▏         | 2/100 [00:01<01:28,  1.11it/s]  3%|▎         | 3/100 [00:02<01:26,  1.12it/s]  4%|▍         | 4/100 [00:03<01:26,  1.11it/s]  5%|▌         | 5/100 [00:04<01:26,  1.09it/s]  6%|▌         | 6/100 [00:05<01:27,  1.08it/s]  7%|▋         | 7/100 [00:06<01:25,  1.08it/s]  8%|▊         | 8/100 [00:07<01:25,  1.08it/s]  9%|▉         | 9/100 [00:08<01:21,  1.12it/s] 10%|█         | 10/100 [00:09<01:21,  1.10it/s] 11%|█         | 11/100 [00:10<01:20,  1.10it/s] 12%|█▏        | 12/100 [00:10<01:20,  1.10it/s] 13%|█▎        | 13/100 [00:11<01:18,  1.10it/s] 14%|█▍        | 14/100 [00:12<01:17,  1.11it/s] 15%|█▌        | 15/100 [00:13<01:14,  1.14it/s] 16%|█▌        | 16/100 [00:14<01:14,  1.12it/s] 17%|█▋        | 17/100 [00:15<01:14,  1.11it/s] 18%|█▊        | 18/100 [00:16<01:13,  1.11it/s] 19%|█▉        | 19/100 [00:17<01:12,  1.12it/s] 20%|██        | 20/100 [00:17<01:07,  1.18it/s] 21%|██        | 21/100 [00:18<01:08,  1.15it/s] 22%|██▏       | 22/100 [00:19<01:06,  1.17it/s] 23%|██▎       | 23/100 [00:20<01:05,  1.17it/s] 24%|██▍       | 24/100 [00:21<01:06,  1.14it/s] 25%|██▌       | 25/100 [00:25<02:19,  1.86s/it] 26%|██▌       | 26/100 [00:26<01:56,  1.57s/it] 27%|██▋       | 27/100 [00:27<01:39,  1.36s/it] 28%|██▊       | 28/100 [00:28<01:30,  1.25s/it] 29%|██▉       | 29/100 [00:29<01:21,  1.15s/it] 30%|███       | 30/100 [00:30<01:15,  1.07s/it] 31%|███       | 31/100 [00:31<01:10,  1.03s/it] 32%|███▏      | 32/100 [00:31<01:07,  1.01it/s] 33%|███▎      | 33/100 [00:32<01:04,  1.04it/s] 34%|███▍      | 34/100 [00:33<01:01,  1.08it/s] 35%|███▌      | 35/100 [00:34<00:58,  1.12it/s] 36%|███▌      | 36/100 [00:35<00:56,  1.13it/s] 37%|███▋      | 37/100 [00:36<00:55,  1.13it/s] 38%|███▊      | 38/100 [00:37<00:53,  1.15it/s] 39%|███▉      | 39/100 [00:37<00:52,  1.15it/s] 40%|████      | 40/100 [00:38<00:52,  1.15it/s] 41%|████      | 41/100 [00:39<00:52,  1.13it/s] 42%|████▏     | 42/100 [00:40<00:51,  1.13it/s] 43%|████▎     | 43/100 [00:41<00:49,  1.15it/s] 44%|████▍     | 44/100 [00:42<00:49,  1.13it/s] 45%|████▌     | 45/100 [00:43<00:48,  1.14it/s] 46%|████▌     | 46/100 [00:44<00:47,  1.14it/s] 47%|████▋     | 47/100 [00:44<00:45,  1.17it/s] 48%|████▊     | 48/100 [00:45<00:45,  1.15it/s] 49%|████▉     | 49/100 [00:46<00:43,  1.16it/s] 50%|█████     | 50/100 [00:50<01:32,  1.86s/it] 51%|█████     | 51/100 [00:51<01:16,  1.57s/it] 52%|█████▏    | 52/100 [00:52<01:04,  1.33s/it] 53%|█████▎    | 53/100 [00:53<00:56,  1.21s/it] 54%|█████▍    | 54/100 [00:54<00:49,  1.07s/it] 55%|█████▌    | 55/100 [00:55<00:45,  1.02s/it] 56%|█████▌    | 56/100 [00:55<00:41,  1.07it/s] 57%|█████▋    | 57/100 [00:56<00:39,  1.08it/s] 58%|█████▊    | 58/100 [00:57<00:39,  1.05it/s] 59%|█████▉    | 59/100 [00:58<00:37,  1.11it/s] 60%|██████    | 60/100 [00:59<00:37,  1.08it/s] 61%|██████    | 61/100 [01:00<00:36,  1.08it/s] 62%|██████▏   | 62/100 [01:01<00:36,  1.05it/s] 63%|██████▎   | 63/100 [01:02<00:35,  1.04it/s] 64%|██████▍   | 64/100 [01:03<00:34,  1.04it/s] 65%|██████▌   | 65/100 [01:04<00:33,  1.04it/s] 66%|██████▌   | 66/100 [01:05<00:32,  1.06it/s] 67%|██████▋   | 67/100 [01:06<00:30,  1.08it/s] 68%|██████▊   | 68/100 [01:06<00:28,  1.13it/s] 69%|██████▉   | 69/100 [01:07<00:27,  1.11it/s] 70%|███████   | 70/100 [01:08<00:27,  1.09it/s] 71%|███████   | 71/100 [01:09<00:25,  1.14it/s] 72%|███████▏  | 72/100 [01:10<00:24,  1.13it/s] 73%|███████▎  | 73/100 [01:11<00:23,  1.13it/s] 74%|███████▍  | 74/100 [01:12<00:22,  1.14it/s] 75%|███████▌  | 75/100 [01:13<00:22,  1.11it/s] 76%|███████▌  | 76/100 [01:14<00:21,  1.10it/s] 77%|███████▋  | 77/100 [01:15<00:20,  1.12it/s] 78%|███████▊  | 78/100 [01:19<00:41,  1.90s/it] 79%|███████▉  | 79/100 [01:20<00:33,  1.60s/it] 80%|████████  | 80/100 [01:21<00:27,  1.37s/it] 81%|████████  | 81/100 [01:21<00:22,  1.17s/it] 82%|████████▏ | 82/100 [01:22<00:19,  1.07s/it] 83%|████████▎ | 83/100 [01:23<00:17,  1.02s/it] 84%|████████▍ | 84/100 [01:24<00:15,  1.03it/s] 85%|████████▌ | 85/100 [01:25<00:14,  1.05it/s] 86%|████████▌ | 86/100 [01:26<00:13,  1.07it/s] 87%|████████▋ | 87/100 [01:26<00:11,  1.10it/s] 88%|████████▊ | 88/100 [01:27<00:10,  1.11it/s] 89%|████████▉ | 89/100 [01:28<00:09,  1.12it/s] 90%|█████████ | 90/100 [01:29<00:08,  1.17it/s] 91%|█████████ | 91/100 [01:30<00:07,  1.21it/s] 92%|█████████▏| 92/100 [01:31<00:06,  1.17it/s] 93%|█████████▎| 93/100 [01:32<00:06,  1.15it/s] 94%|█████████▍| 94/100 [01:32<00:05,  1.13it/s] 95%|█████████▌| 95/100 [01:33<00:04,  1.12it/s] 96%|█████████▌| 96/100 [01:34<00:03,  1.13it/s] 97%|█████████▋| 97/100 [01:35<00:02,  1.14it/s] 98%|█████████▊| 98/100 [01:36<00:01,  1.14it/s] 99%|█████████▉| 99/100 [01:37<00:00,  1.15it/s]100%|██████████| 100/100 [01:38<00:00,  1.16it/s]100%|██████████| 100/100 [01:38<00:00,  1.02it/s]
Number of users per round / total users: 10  /  100
Finished creating FL server.
=== Training starts: algorithm FedNTD ===
-------------Round number:  0  -------------
loss: NT_CE learning rate: 0.001
training loss: tensor(1.2556) KL loss: tensor(0.1906)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.4137) KL loss: tensor(0.2181)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.5162) KL loss: tensor(0.2345)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.0471) KL loss: tensor(0.2182)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.4514) KL loss: tensor(0.2375)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.4946) KL loss: tensor(0.2413)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.4372) KL loss: tensor(0.2150)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.2837) KL loss: tensor(0.2183)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.5395) KL loss: tensor(0.2144)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.5105) KL loss: tensor(0.2484)
Global Model Acc on global data: 0.1123 length of data: 10000
save a model
-------------Round number:  1  -------------
loss: NT_CE learning rate: 0.000982
training loss: tensor(0.8184) KL loss: tensor(0.1694)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.1343) KL loss: tensor(0.2146)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.1117) KL loss: tensor(0.2154)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.2373) KL loss: tensor(0.2396)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.1275) KL loss: tensor(0.2298)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.1680) KL loss: tensor(0.2109)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.2876) KL loss: tensor(0.2436)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.1972) KL loss: tensor(0.2115)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.1374) KL loss: tensor(0.2187)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.2144) KL loss: tensor(0.2176)
Global Model Acc on global data: 0.1165 length of data: 10000
save a model
-------------Round number:  2  -------------
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.9830) KL loss: tensor(0.2330)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.8334) KL loss: tensor(0.2054)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.8860) KL loss: tensor(0.2161)
loss: NT_CE learning rate: 0.000964
training loss: tensor(1.0245) KL loss: tensor(0.2255)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.9469) KL loss: tensor(0.2171)
loss: NT_CE learning rate: 0.000964
training loss: tensor(1.0487) KL loss: tensor(0.2332)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.9697) KL loss: tensor(0.2256)
loss: NT_CE learning rate: 0.000964
training loss: tensor(1.0126) KL loss: tensor(0.2541)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.8789) KL loss: tensor(0.2181)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.7980) KL loss: tensor(0.2056)
Global Model Acc on global data: 0.125 length of data: 10000
save a model
-------------Round number:  3  -------------
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.7515) KL loss: tensor(0.2255)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.6065) KL loss: tensor(0.1654)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.8985) KL loss: tensor(0.2691)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.9420) KL loss: tensor(0.2660)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.8795) KL loss: tensor(0.2686)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.8980) KL loss: tensor(0.2558)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.9580) KL loss: tensor(0.2743)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.9770) KL loss: tensor(0.2564)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.8521) KL loss: tensor(0.2688)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.8000) KL loss: tensor(0.2598)
Global Model Acc on global data: 0.1555 length of data: 10000
save a model
-------------Round number:  4  -------------
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.8461) KL loss: tensor(0.2554)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.9758) KL loss: tensor(0.2801)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.7247) KL loss: tensor(0.2408)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.8026) KL loss: tensor(0.2501)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.7971) KL loss: tensor(0.2292)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.8585) KL loss: tensor(0.2524)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.7543) KL loss: tensor(0.2473)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.7906) KL loss: tensor(0.2439)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.8536) KL loss: tensor(0.2578)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.8544) KL loss: tensor(0.2536)
Global Model Acc on global data: 0.1869 length of data: 10000
save a model
-------------Round number:  5  -------------
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.8330) KL loss: tensor(0.2701)
loss: NT_CE learning rate: 0.00091
training loss: tensor(1.2742) KL loss: tensor(0.3387)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.8451) KL loss: tensor(0.2916)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.7339) KL loss: tensor(0.2553)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.9317) KL loss: tensor(0.2968)
loss: NT_CE learning rate: 0.00091
training loss: tensor(1.1533) KL loss: tensor(0.4085)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.7757) KL loss: tensor(0.2663)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.7844) KL loss: tensor(0.2616)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.8442) KL loss: tensor(0.2891)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.7742) KL loss: tensor(0.2657)
Global Model Acc on global data: 0.2063 length of data: 10000
save a model
-------------Round number:  6  -------------
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.6975) KL loss: tensor(0.2633)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.7933) KL loss: tensor(0.2655)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.8195) KL loss: tensor(0.2817)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.7072) KL loss: tensor(0.2588)
loss: NT_CE learning rate: 0.000892
training loss: tensor(1.0424) KL loss: tensor(0.3396)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.7996) KL loss: tensor(0.2810)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.6852) KL loss: tensor(0.2566)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.7825) KL loss: tensor(0.2707)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.8980) KL loss: tensor(0.2693)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.7756) KL loss: tensor(0.2680)
Global Model Acc on global data: 0.2394 length of data: 10000
save a model
-------------Round number:  7  -------------
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.7409) KL loss: tensor(0.2618)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.8522) KL loss: tensor(0.2733)
loss: NT_CE learning rate: 0.000874
training loss: tensor(1.1897) KL loss: tensor(0.3406)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.9621) KL loss: tensor(0.3117)
loss: NT_CE learning rate: 0.000874
training loss: tensor(1.1215) KL loss: tensor(0.3291)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.6949) KL loss: tensor(0.2400)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.7793) KL loss: tensor(0.2726)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.7755) KL loss: tensor(0.2695)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.7154) KL loss: tensor(0.3085)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.7677) KL loss: tensor(0.2715)
Global Model Acc on global data: 0.274 length of data: 10000
save a model
-------------Round number:  8  -------------
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.5628) KL loss: tensor(0.2447)
loss: NT_CE learning rate: 0.000856
training loss: tensor(1.0174) KL loss: tensor(0.4063)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.8676) KL loss: tensor(0.3253)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.8382) KL loss: tensor(0.2746)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.6999) KL loss: tensor(0.2400)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.8454) KL loss: tensor(0.2842)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.6298) KL loss: tensor(0.2792)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.7009) KL loss: tensor(0.2617)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.9166) KL loss: tensor(0.3592)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.7768) KL loss: tensor(0.2827)
Global Model Acc on global data: 0.2745 length of data: 10000
save a model
-------------Round number:  9  -------------
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.6815) KL loss: tensor(0.2575)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.7277) KL loss: tensor(0.2897)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.6426) KL loss: tensor(0.2563)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.7303) KL loss: tensor(0.2644)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.7692) KL loss: tensor(0.3003)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.7774) KL loss: tensor(0.2856)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.8397) KL loss: tensor(0.3250)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.6018) KL loss: tensor(0.2454)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.7176) KL loss: tensor(0.2626)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.7072) KL loss: tensor(0.2527)
Global Model Acc on global data: 0.3111 length of data: 10000
save a model
-------------Round number:  10  -------------
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.5628) KL loss: tensor(0.2255)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.7021) KL loss: tensor(0.2677)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.6257) KL loss: tensor(0.2431)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.5519) KL loss: tensor(0.2366)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.5647) KL loss: tensor(0.2132)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.6603) KL loss: tensor(0.2950)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(1.0261) KL loss: tensor(0.3060)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.6463) KL loss: tensor(0.2613)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.8336) KL loss: tensor(0.3275)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.6060) KL loss: tensor(0.2351)
Global Model Acc on global data: 0.3262 length of data: 10000
save a model
-------------Round number:  11  -------------
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.4865) KL loss: tensor(0.2237)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.7918) KL loss: tensor(0.2763)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.5350) KL loss: tensor(0.2243)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.6198) KL loss: tensor(0.2271)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.5097) KL loss: tensor(0.2280)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.6117) KL loss: tensor(0.2469)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.6738) KL loss: tensor(0.3081)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.4868) KL loss: tensor(0.2226)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.5858) KL loss: tensor(0.2306)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.5005) KL loss: tensor(0.2070)
Global Model Acc on global data: 0.353 length of data: 10000
save a model
-------------Round number:  12  -------------
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.4530) KL loss: tensor(0.2172)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.4314) KL loss: tensor(0.2114)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.5647) KL loss: tensor(0.2269)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.5203) KL loss: tensor(0.2314)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.5281) KL loss: tensor(0.2236)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.6866) KL loss: tensor(0.2914)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.7516) KL loss: tensor(0.2731)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.5511) KL loss: tensor(0.2359)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.6421) KL loss: tensor(0.2707)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.8368) KL loss: tensor(0.3127)
Global Model Acc on global data: 0.3591 length of data: 10000
save a model
-------------Round number:  13  -------------
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.4181) KL loss: tensor(0.1922)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.5098) KL loss: tensor(0.2443)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.6509) KL loss: tensor(0.2441)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.4687) KL loss: tensor(0.2164)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.5761) KL loss: tensor(0.2446)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.6055) KL loss: tensor(0.2535)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.6166) KL loss: tensor(0.2361)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.5882) KL loss: tensor(0.2716)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.5722) KL loss: tensor(0.2439)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.5856) KL loss: tensor(0.2366)
Global Model Acc on global data: 0.3763 length of data: 10000
save a model
-------------Round number:  14  -------------
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.6140) KL loss: tensor(0.2413)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.5439) KL loss: tensor(0.2291)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.5517) KL loss: tensor(0.2244)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.5494) KL loss: tensor(0.2226)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.5263) KL loss: tensor(0.2020)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.5531) KL loss: tensor(0.2329)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.6301) KL loss: tensor(0.2464)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.5897) KL loss: tensor(0.2632)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.6200) KL loss: tensor(0.2602)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.6615) KL loss: tensor(0.2324)
Global Model Acc on global data: 0.3714 length of data: 10000
-------------Round number:  15  -------------
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.5850) KL loss: tensor(0.2100)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.5810) KL loss: tensor(0.2236)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.4629) KL loss: tensor(0.1976)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.4687) KL loss: tensor(0.1883)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.4299) KL loss: tensor(0.1912)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.8367) KL loss: tensor(0.3260)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.5207) KL loss: tensor(0.2094)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.3928) KL loss: tensor(0.1735)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.6885) KL loss: tensor(0.2551)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.5074) KL loss: tensor(0.1900)
Global Model Acc on global data: 0.4032 length of data: 10000
save a model
-------------Round number:  16  -------------
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.5710) KL loss: tensor(0.2597)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.6163) KL loss: tensor(0.2513)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.5748) KL loss: tensor(0.2410)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.5572) KL loss: tensor(0.2447)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.6077) KL loss: tensor(0.2327)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.4458) KL loss: tensor(0.2073)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.5795) KL loss: tensor(0.2369)
loss: NT_CE learning rate: 0.000712
training loss: tensor(1.0315) KL loss: tensor(0.3111)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.5948) KL loss: tensor(0.2444)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.4960) KL loss: tensor(0.2488)
Global Model Acc on global data: 0.4466 length of data: 10000
save a model
-------------Round number:  17  -------------
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.4241) KL loss: tensor(0.2046)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.5145) KL loss: tensor(0.2111)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.3351) KL loss: tensor(0.1951)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.5472) KL loss: tensor(0.2370)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.5378) KL loss: tensor(0.2225)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.5840) KL loss: tensor(0.2311)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.5907) KL loss: tensor(0.2220)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.4786) KL loss: tensor(0.2158)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.4742) KL loss: tensor(0.2046)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.3410) KL loss: tensor(0.1726)
Global Model Acc on global data: 0.4562 length of data: 10000
save a model
-------------Round number:  18  -------------
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.5512) KL loss: tensor(0.2131)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.3807) KL loss: tensor(0.1877)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.4522) KL loss: tensor(0.2093)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.5925) KL loss: tensor(0.2405)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.5157) KL loss: tensor(0.2177)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.4820) KL loss: tensor(0.2017)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.5304) KL loss: tensor(0.2223)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.5217) KL loss: tensor(0.2156)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.5327) KL loss: tensor(0.2241)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.5405) KL loss: tensor(0.2469)
Global Model Acc on global data: 0.4705 length of data: 10000
save a model
-------------Round number:  19  -------------
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.5112) KL loss: tensor(0.2217)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.4298) KL loss: tensor(0.1935)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.5203) KL loss: tensor(0.2309)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.5006) KL loss: tensor(0.2060)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.5340) KL loss: tensor(0.2195)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.5892) KL loss: tensor(0.2554)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.4797) KL loss: tensor(0.1997)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.4923) KL loss: tensor(0.2094)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.5000) KL loss: tensor(0.1992)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.5936) KL loss: tensor(0.2187)
Global Model Acc on global data: 0.472 length of data: 10000
save a model
-------------Round number:  20  -------------
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.4406) KL loss: tensor(0.2032)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.5717) KL loss: tensor(0.2535)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.3901) KL loss: tensor(0.1892)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.4355) KL loss: tensor(0.2252)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.4248) KL loss: tensor(0.1981)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.3427) KL loss: tensor(0.1689)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.3306) KL loss: tensor(0.1665)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.4568) KL loss: tensor(0.2061)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.3864) KL loss: tensor(0.1910)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.8748) KL loss: tensor(0.3997)
Global Model Acc on global data: 0.4426 length of data: 10000
-------------Round number:  21  -------------
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.5882) KL loss: tensor(0.2542)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.4900) KL loss: tensor(0.2030)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.4107) KL loss: tensor(0.1830)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.4045) KL loss: tensor(0.1732)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.5087) KL loss: tensor(0.2500)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.3457) KL loss: tensor(0.1704)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.3961) KL loss: tensor(0.1817)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.4808) KL loss: tensor(0.2071)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.4583) KL loss: tensor(0.1972)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.5263) KL loss: tensor(0.2052)
Global Model Acc on global data: 0.4622 length of data: 10000
-------------Round number:  22  -------------
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.3317) KL loss: tensor(0.1693)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.8598) KL loss: tensor(0.4231)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.4581) KL loss: tensor(0.1934)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.3985) KL loss: tensor(0.2042)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.2821) KL loss: tensor(0.1448)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.2870) KL loss: tensor(0.1530)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.4753) KL loss: tensor(0.1939)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.3924) KL loss: tensor(0.2042)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.4936) KL loss: tensor(0.1792)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.2932) KL loss: tensor(0.1596)
Global Model Acc on global data: 0.4626 length of data: 10000
-------------Round number:  23  -------------
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.5541) KL loss: tensor(0.2099)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.4549) KL loss: tensor(0.1802)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.3696) KL loss: tensor(0.1813)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.5060) KL loss: tensor(0.1885)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.4955) KL loss: tensor(0.1911)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.2114) KL loss: tensor(0.1296)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.4738) KL loss: tensor(0.2104)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.4394) KL loss: tensor(0.1671)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.4519) KL loss: tensor(0.1756)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.3742) KL loss: tensor(0.1705)
Global Model Acc on global data: 0.4567 length of data: 10000
-------------Round number:  24  -------------
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.3523) KL loss: tensor(0.1627)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.3494) KL loss: tensor(0.1817)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.1662) KL loss: tensor(0.1121)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.4653) KL loss: tensor(0.2006)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.4062) KL loss: tensor(0.1479)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.3954) KL loss: tensor(0.1694)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.9367) KL loss: tensor(0.3178)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.2906) KL loss: tensor(0.1408)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.3318) KL loss: tensor(0.1523)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.4233) KL loss: tensor(0.1769)
Global Model Acc on global data: 0.473 length of data: 10000
save a model
-------------Round number:  25  -------------
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.4353) KL loss: tensor(0.1790)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.3375) KL loss: tensor(0.1526)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.3983) KL loss: tensor(0.1514)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.4705) KL loss: tensor(0.1732)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.4378) KL loss: tensor(0.1745)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.4059) KL loss: tensor(0.1564)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.3349) KL loss: tensor(0.1475)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.5975) KL loss: tensor(0.2510)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.3421) KL loss: tensor(0.1425)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.3880) KL loss: tensor(0.1610)
Global Model Acc on global data: 0.4888 length of data: 10000
save a model
-------------Round number:  26  -------------
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.3700) KL loss: tensor(0.1521)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.3820) KL loss: tensor(0.1647)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.4023) KL loss: tensor(0.1535)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.3760) KL loss: tensor(0.1702)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.4335) KL loss: tensor(0.1692)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.4398) KL loss: tensor(0.1746)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.4006) KL loss: tensor(0.1661)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.4343) KL loss: tensor(0.1707)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.2769) KL loss: tensor(0.1311)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.3875) KL loss: tensor(0.1663)
Global Model Acc on global data: 0.4822 length of data: 10000
-------------Round number:  27  -------------
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.5148) KL loss: tensor(0.2433)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.4013) KL loss: tensor(0.1553)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.3752) KL loss: tensor(0.1669)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.4499) KL loss: tensor(0.1758)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.3846) KL loss: tensor(0.1774)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.3258) KL loss: tensor(0.1481)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.4339) KL loss: tensor(0.1736)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.2888) KL loss: tensor(0.1432)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.3949) KL loss: tensor(0.1682)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.4067) KL loss: tensor(0.1972)
Global Model Acc on global data: 0.486 length of data: 10000
-------------Round number:  28  -------------
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.3551) KL loss: tensor(0.1548)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.2760) KL loss: tensor(0.1234)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.3150) KL loss: tensor(0.1417)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.3408) KL loss: tensor(0.1597)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.3820) KL loss: tensor(0.1435)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.3438) KL loss: tensor(0.1365)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.3586) KL loss: tensor(0.1405)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.3369) KL loss: tensor(0.1438)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.4165) KL loss: tensor(0.1624)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.2342) KL loss: tensor(0.1187)
Global Model Acc on global data: 0.5029 length of data: 10000
save a model
-------------Round number:  29  -------------
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.3798) KL loss: tensor(0.1423)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.2948) KL loss: tensor(0.1235)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.4900) KL loss: tensor(0.1928)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.3485) KL loss: tensor(0.1587)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.3866) KL loss: tensor(0.1522)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.2774) KL loss: tensor(0.1323)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.3650) KL loss: tensor(0.1515)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.3456) KL loss: tensor(0.1517)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.5129) KL loss: tensor(0.2195)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.2674) KL loss: tensor(0.1253)
Global Model Acc on global data: 0.5214 length of data: 10000
save a model
-------------Round number:  30  -------------
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3220) KL loss: tensor(0.1382)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3705) KL loss: tensor(0.1679)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3486) KL loss: tensor(0.1481)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3369) KL loss: tensor(0.1525)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.2292) KL loss: tensor(0.1186)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3765) KL loss: tensor(0.1771)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.4098) KL loss: tensor(0.1764)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.6380) KL loss: tensor(0.2084)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3303) KL loss: tensor(0.1438)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.4926) KL loss: tensor(0.2180)
Global Model Acc on global data: 0.5088 length of data: 10000
-------------Round number:  31  -------------
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.2661) KL loss: tensor(0.1235)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.2019) KL loss: tensor(0.1007)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.3111) KL loss: tensor(0.1200)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.3791) KL loss: tensor(0.1637)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.3185) KL loss: tensor(0.1450)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.2460) KL loss: tensor(0.1222)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.1736) KL loss: tensor(0.1072)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.1650) KL loss: tensor(0.0966)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.6444) KL loss: tensor(0.2930)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.2727) KL loss: tensor(0.1253)
Global Model Acc on global data: 0.5103 length of data: 10000
-------------Round number:  32  -------------
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.3136) KL loss: tensor(0.1408)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.3799) KL loss: tensor(0.1500)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.2129) KL loss: tensor(0.1152)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.4439) KL loss: tensor(0.1515)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.3147) KL loss: tensor(0.1150)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.1792) KL loss: tensor(0.0937)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.2358) KL loss: tensor(0.1153)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.3634) KL loss: tensor(0.1379)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.3322) KL loss: tensor(0.1418)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.3933) KL loss: tensor(0.1549)
Global Model Acc on global data: 0.5118 length of data: 10000
-------------Round number:  33  -------------
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.2372) KL loss: tensor(0.1260)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.3053) KL loss: tensor(0.1229)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.2199) KL loss: tensor(0.1080)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.2928) KL loss: tensor(0.1287)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.4335) KL loss: tensor(0.1769)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.1949) KL loss: tensor(0.1064)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.3014) KL loss: tensor(0.1245)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.4407) KL loss: tensor(0.1884)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.2731) KL loss: tensor(0.1298)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.3061) KL loss: tensor(0.1246)
Global Model Acc on global data: 0.5026 length of data: 10000
-------------Round number:  34  -------------
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.2454) KL loss: tensor(0.1123)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.3031) KL loss: tensor(0.1464)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.4269) KL loss: tensor(0.1576)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.4020) KL loss: tensor(0.1542)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.7978) KL loss: tensor(0.2951)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.3279) KL loss: tensor(0.1167)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.3020) KL loss: tensor(0.1440)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.3157) KL loss: tensor(0.1348)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.3774) KL loss: tensor(0.1467)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.3489) KL loss: tensor(0.1289)
Global Model Acc on global data: 0.5301 length of data: 10000
save a model
-------------Round number:  35  -------------
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.3217) KL loss: tensor(0.1556)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.1658) KL loss: tensor(0.0999)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.2855) KL loss: tensor(0.1206)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.2273) KL loss: tensor(0.1123)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.2206) KL loss: tensor(0.1055)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.2810) KL loss: tensor(0.1291)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.2612) KL loss: tensor(0.1330)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.3827) KL loss: tensor(0.1417)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.3294) KL loss: tensor(0.1245)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.3442) KL loss: tensor(0.1358)
Global Model Acc on global data: 0.54 length of data: 10000
save a model
-------------Round number:  36  -------------
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.2538) KL loss: tensor(0.1087)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.2856) KL loss: tensor(0.1427)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.2009) KL loss: tensor(0.1066)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.1939) KL loss: tensor(0.1022)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.3746) KL loss: tensor(0.1391)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.3534) KL loss: tensor(0.1275)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.4031) KL loss: tensor(0.1376)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.3893) KL loss: tensor(0.1329)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.2685) KL loss: tensor(0.1142)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.2241) KL loss: tensor(0.1250)
Global Model Acc on global data: 0.5487 length of data: 10000
save a model
-------------Round number:  37  -------------
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.3838) KL loss: tensor(0.1207)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.7742) KL loss: tensor(0.3532)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.2958) KL loss: tensor(0.1406)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.6777) KL loss: tensor(0.2763)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.3109) KL loss: tensor(0.1241)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.2201) KL loss: tensor(0.1127)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.3213) KL loss: tensor(0.1435)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.2258) KL loss: tensor(0.1130)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.3590) KL loss: tensor(0.1612)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.2763) KL loss: tensor(0.1080)
Global Model Acc on global data: 0.5411 length of data: 10000
-------------Round number:  38  -------------
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.2582) KL loss: tensor(0.1401)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.2966) KL loss: tensor(0.1120)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.5221) KL loss: tensor(0.2332)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.3687) KL loss: tensor(0.1253)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.3788) KL loss: tensor(0.1362)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.3551) KL loss: tensor(0.1392)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.2917) KL loss: tensor(0.1134)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.2155) KL loss: tensor(0.1089)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.2896) KL loss: tensor(0.1114)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.3764) KL loss: tensor(0.1235)
Global Model Acc on global data: 0.5632 length of data: 10000
save a model
-------------Round number:  39  -------------
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2540) KL loss: tensor(0.1208)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2993) KL loss: tensor(0.1168)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2827) KL loss: tensor(0.1057)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2106) KL loss: tensor(0.0994)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2682) KL loss: tensor(0.1131)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.3303) KL loss: tensor(0.1275)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2472) KL loss: tensor(0.1135)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2682) KL loss: tensor(0.1276)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.1981) KL loss: tensor(0.1069)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.7389) KL loss: tensor(0.3039)
Global Model Acc on global data: 0.5602 length of data: 10000
-------------Round number:  40  -------------
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.6235) KL loss: tensor(0.2392)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.2723) KL loss: tensor(0.1114)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.3234) KL loss: tensor(0.1195)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.3442) KL loss: tensor(0.1302)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.4471) KL loss: tensor(0.1903)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.4332) KL loss: tensor(0.1390)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.2613) KL loss: tensor(0.1407)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.3076) KL loss: tensor(0.1066)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.2789) KL loss: tensor(0.1179)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.3226) KL loss: tensor(0.1427)
Global Model Acc on global data: 0.5607 length of data: 10000
-------------Round number:  41  -------------
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.3180) KL loss: tensor(0.1554)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.4261) KL loss: tensor(0.1412)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.1528) KL loss: tensor(0.0890)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.2468) KL loss: tensor(0.1067)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.5396) KL loss: tensor(0.2270)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.7115) KL loss: tensor(0.2764)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.2858) KL loss: tensor(0.1179)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.1901) KL loss: tensor(0.0905)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.1639) KL loss: tensor(0.0827)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.3473) KL loss: tensor(0.1241)
Global Model Acc on global data: 0.5763 length of data: 10000
save a model
-------------Round number:  42  -------------
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.3620) KL loss: tensor(0.1174)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.2601) KL loss: tensor(0.1090)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.2429) KL loss: tensor(0.1041)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.2639) KL loss: tensor(0.0951)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.3044) KL loss: tensor(0.1476)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.2494) KL loss: tensor(0.1145)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.1803) KL loss: tensor(0.0967)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.2685) KL loss: tensor(0.1084)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.1931) KL loss: tensor(0.1006)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.1399) KL loss: tensor(0.0842)
Global Model Acc on global data: 0.5638 length of data: 10000
-------------Round number:  43  -------------
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.2949) KL loss: tensor(0.1155)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.1535) KL loss: tensor(0.0858)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.2705) KL loss: tensor(0.1091)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.1352) KL loss: tensor(0.0816)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.2238) KL loss: tensor(0.0952)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.2763) KL loss: tensor(0.1310)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.2345) KL loss: tensor(0.0982)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.2749) KL loss: tensor(0.0949)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.4273) KL loss: tensor(0.1400)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.5399) KL loss: tensor(0.2171)
Global Model Acc on global data: 0.5763 length of data: 10000
-------------Round number:  44  -------------
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.2169) KL loss: tensor(0.1130)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.3502) KL loss: tensor(0.1183)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.1451) KL loss: tensor(0.0819)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.2857) KL loss: tensor(0.1056)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.1875) KL loss: tensor(0.0909)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.3701) KL loss: tensor(0.1343)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.2580) KL loss: tensor(0.1087)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.3691) KL loss: tensor(0.1319)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.2219) KL loss: tensor(0.0989)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.4729) KL loss: tensor(0.1994)
Global Model Acc on global data: 0.5775 length of data: 10000
save a model
-------------Round number:  45  -------------
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.1041) KL loss: tensor(0.0648)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.3595) KL loss: tensor(0.1255)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.2152) KL loss: tensor(0.1156)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.2806) KL loss: tensor(0.0870)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.1107) KL loss: tensor(0.0714)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.3908) KL loss: tensor(0.1169)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.5507) KL loss: tensor(0.2087)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.2871) KL loss: tensor(0.1286)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.1320) KL loss: tensor(0.0786)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.2274) KL loss: tensor(0.0825)
Global Model Acc on global data: 0.5806 length of data: 10000
save a model
-------------Round number:  46  -------------
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.1431) KL loss: tensor(0.0840)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.1576) KL loss: tensor(0.0685)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.0781) KL loss: tensor(0.0557)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.1626) KL loss: tensor(0.0924)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.2850) KL loss: tensor(0.1353)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.3221) KL loss: tensor(0.1165)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.2009) KL loss: tensor(0.0972)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.2002) KL loss: tensor(0.0877)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.2098) KL loss: tensor(0.0866)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.1821) KL loss: tensor(0.0872)
Global Model Acc on global data: 0.5812 length of data: 10000
save a model
-------------Round number:  47  -------------
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.1619) KL loss: tensor(0.0826)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.3542) KL loss: tensor(0.1135)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.3742) KL loss: tensor(0.1079)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.2593) KL loss: tensor(0.1187)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.2765) KL loss: tensor(0.1058)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.3882) KL loss: tensor(0.1398)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.3246) KL loss: tensor(0.1484)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.2126) KL loss: tensor(0.0943)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.2026) KL loss: tensor(0.1150)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.3286) KL loss: tensor(0.1054)
Global Model Acc on global data: 0.5747 length of data: 10000
-------------Round number:  48  -------------
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.2535) KL loss: tensor(0.0989)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.2324) KL loss: tensor(0.0875)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.1278) KL loss: tensor(0.0766)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.2532) KL loss: tensor(0.1020)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.3452) KL loss: tensor(0.1040)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.5121) KL loss: tensor(0.1771)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.1805) KL loss: tensor(0.0737)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.2908) KL loss: tensor(0.1015)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.2393) KL loss: tensor(0.0872)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.2441) KL loss: tensor(0.0975)
Global Model Acc on global data: 0.581 length of data: 10000
-------------Round number:  49  -------------
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2592) KL loss: tensor(0.0798)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.3000) KL loss: tensor(0.1051)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2970) KL loss: tensor(0.1026)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2097) KL loss: tensor(0.0837)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2606) KL loss: tensor(0.1029)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.3163) KL loss: tensor(0.1054)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.3286) KL loss: tensor(0.1078)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.1118) KL loss: tensor(0.0688)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.1828) KL loss: tensor(0.0819)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.3009) KL loss: tensor(0.1099)
Global Model Acc on global data: 0.581 length of data: 10000
-------------Round number:  50  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3217) KL loss: tensor(0.1116)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5567) KL loss: tensor(0.2305)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2326) KL loss: tensor(0.0880)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3143) KL loss: tensor(0.0999)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1859) KL loss: tensor(0.0840)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1494) KL loss: tensor(0.0883)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2668) KL loss: tensor(0.0951)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1483) KL loss: tensor(0.0774)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1742) KL loss: tensor(0.0722)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4149) KL loss: tensor(0.1213)
Global Model Acc on global data: 0.5756 length of data: 10000
-------------Round number:  51  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0988) KL loss: tensor(0.0680)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3766) KL loss: tensor(0.1126)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3194) KL loss: tensor(0.1001)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2851) KL loss: tensor(0.0895)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6354) KL loss: tensor(0.2341)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5046) KL loss: tensor(0.1511)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2245) KL loss: tensor(0.0946)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3115) KL loss: tensor(0.1082)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5381) KL loss: tensor(0.2324)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2000) KL loss: tensor(0.0881)
Global Model Acc on global data: 0.573 length of data: 10000
-------------Round number:  52  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1919) KL loss: tensor(0.0933)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2264) KL loss: tensor(0.0825)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2098) KL loss: tensor(0.1141)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3765) KL loss: tensor(0.1248)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1177) KL loss: tensor(0.0718)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1874) KL loss: tensor(0.0673)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0832) KL loss: tensor(0.0587)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2861) KL loss: tensor(0.1115)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3409) KL loss: tensor(0.1207)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2551) KL loss: tensor(0.0866)
Global Model Acc on global data: 0.5657 length of data: 10000
-------------Round number:  53  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2504) KL loss: tensor(0.0952)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6414) KL loss: tensor(0.2714)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3728) KL loss: tensor(0.1247)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4196) KL loss: tensor(0.1507)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0826) KL loss: tensor(0.0589)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8086) KL loss: tensor(0.2708)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6287) KL loss: tensor(0.1823)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2133) KL loss: tensor(0.0981)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2820) KL loss: tensor(0.0905)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4197) KL loss: tensor(0.1126)
Global Model Acc on global data: 0.5674 length of data: 10000
-------------Round number:  54  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3177) KL loss: tensor(0.1108)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1965) KL loss: tensor(0.0876)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3001) KL loss: tensor(0.0957)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3638) KL loss: tensor(0.1112)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3136) KL loss: tensor(0.1243)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2794) KL loss: tensor(0.0930)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3016) KL loss: tensor(0.1391)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1770) KL loss: tensor(0.0815)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2262) KL loss: tensor(0.0891)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1097) KL loss: tensor(0.0715)
Global Model Acc on global data: 0.5617 length of data: 10000
-------------Round number:  55  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3692) KL loss: tensor(0.1163)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3490) KL loss: tensor(0.1120)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2114) KL loss: tensor(0.0967)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1906) KL loss: tensor(0.0882)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2266) KL loss: tensor(0.0862)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1671) KL loss: tensor(0.0779)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3011) KL loss: tensor(0.1486)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2614) KL loss: tensor(0.0864)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1237) KL loss: tensor(0.0750)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6745) KL loss: tensor(0.2040)
Global Model Acc on global data: 0.5732 length of data: 10000
-------------Round number:  56  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2936) KL loss: tensor(0.1144)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4091) KL loss: tensor(0.1851)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5868) KL loss: tensor(0.2076)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4840) KL loss: tensor(0.1548)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2786) KL loss: tensor(0.1139)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2211) KL loss: tensor(0.1299)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1831) KL loss: tensor(0.0771)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2182) KL loss: tensor(0.1031)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1848) KL loss: tensor(0.0794)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1451) KL loss: tensor(0.0843)
Global Model Acc on global data: 0.5796 length of data: 10000
-------------Round number:  57  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2173) KL loss: tensor(0.1005)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1296) KL loss: tensor(0.0787)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2938) KL loss: tensor(0.1120)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3337) KL loss: tensor(0.1247)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2423) KL loss: tensor(0.0972)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1797) KL loss: tensor(0.0956)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1948) KL loss: tensor(0.0881)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2078) KL loss: tensor(0.0930)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3726) KL loss: tensor(0.1424)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1955) KL loss: tensor(0.0907)
Global Model Acc on global data: 0.5732 length of data: 10000
-------------Round number:  58  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3105) KL loss: tensor(0.1044)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3101) KL loss: tensor(0.1030)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1932) KL loss: tensor(0.0793)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2444) KL loss: tensor(0.1099)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1926) KL loss: tensor(0.0915)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2963) KL loss: tensor(0.0962)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1849) KL loss: tensor(0.0859)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1525) KL loss: tensor(0.0758)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3196) KL loss: tensor(0.1109)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2606) KL loss: tensor(0.1074)
Global Model Acc on global data: 0.5726 length of data: 10000
-------------Round number:  59  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2603) KL loss: tensor(0.1454)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2342) KL loss: tensor(0.0942)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1577) KL loss: tensor(0.0881)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2404) KL loss: tensor(0.0990)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3312) KL loss: tensor(0.1073)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1655) KL loss: tensor(0.0827)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2871) KL loss: tensor(0.1061)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3427) KL loss: tensor(0.1264)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1977) KL loss: tensor(0.0823)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3676) KL loss: tensor(0.1077)
Global Model Acc on global data: 0.5796 length of data: 10000
-------------Round number:  60  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6590) KL loss: tensor(0.2661)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2478) KL loss: tensor(0.1070)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4018) KL loss: tensor(0.1586)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1714) KL loss: tensor(0.0873)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2650) KL loss: tensor(0.0946)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3451) KL loss: tensor(0.1324)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2847) KL loss: tensor(0.1160)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2956) KL loss: tensor(0.1158)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2699) KL loss: tensor(0.0915)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2119) KL loss: tensor(0.0986)
Global Model Acc on global data: 0.5739 length of data: 10000
-------------Round number:  61  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2438) KL loss: tensor(0.0959)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2849) KL loss: tensor(0.1239)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2166) KL loss: tensor(0.0817)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1700) KL loss: tensor(0.0935)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2113) KL loss: tensor(0.0988)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2377) KL loss: tensor(0.1077)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3353) KL loss: tensor(0.1295)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1457) KL loss: tensor(0.0798)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2759) KL loss: tensor(0.1334)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2121) KL loss: tensor(0.0879)
Global Model Acc on global data: 0.5825 length of data: 10000
save a model
-------------Round number:  62  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2151) KL loss: tensor(0.0769)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2311) KL loss: tensor(0.0868)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1981) KL loss: tensor(0.1148)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2160) KL loss: tensor(0.1014)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1208) KL loss: tensor(0.0700)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3024) KL loss: tensor(0.1035)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4649) KL loss: tensor(0.2033)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3017) KL loss: tensor(0.1326)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1711) KL loss: tensor(0.0847)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1522) KL loss: tensor(0.0790)
Global Model Acc on global data: 0.578 length of data: 10000
-------------Round number:  63  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2770) KL loss: tensor(0.1078)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1732) KL loss: tensor(0.0795)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1996) KL loss: tensor(0.0855)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1140) KL loss: tensor(0.0787)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1719) KL loss: tensor(0.0883)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1902) KL loss: tensor(0.0907)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1492) KL loss: tensor(0.0705)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1865) KL loss: tensor(0.0812)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2520) KL loss: tensor(0.0971)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3043) KL loss: tensor(0.1041)
Global Model Acc on global data: 0.5732 length of data: 10000
-------------Round number:  64  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1133) KL loss: tensor(0.0733)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1071) KL loss: tensor(0.0717)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4076) KL loss: tensor(0.1949)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2593) KL loss: tensor(0.1028)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4666) KL loss: tensor(0.1864)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2376) KL loss: tensor(0.1071)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1059) KL loss: tensor(0.0748)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2217) KL loss: tensor(0.0891)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1689) KL loss: tensor(0.0954)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4866) KL loss: tensor(0.1858)
Global Model Acc on global data: 0.5776 length of data: 10000
-------------Round number:  65  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0832) KL loss: tensor(0.0595)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2611) KL loss: tensor(0.1113)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1483) KL loss: tensor(0.0789)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3431) KL loss: tensor(0.1061)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1936) KL loss: tensor(0.0669)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2648) KL loss: tensor(0.0896)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1419) KL loss: tensor(0.0847)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2047) KL loss: tensor(0.1023)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2430) KL loss: tensor(0.1046)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1524) KL loss: tensor(0.0757)
Global Model Acc on global data: 0.5819 length of data: 10000
-------------Round number:  66  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1821) KL loss: tensor(0.0838)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2446) KL loss: tensor(0.0993)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2309) KL loss: tensor(0.1022)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2507) KL loss: tensor(0.1075)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1957) KL loss: tensor(0.0981)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2979) KL loss: tensor(0.1099)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1171) KL loss: tensor(0.0689)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1623) KL loss: tensor(0.0729)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5811) KL loss: tensor(0.2338)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4147) KL loss: tensor(0.1255)
Global Model Acc on global data: 0.5861 length of data: 10000
save a model
-------------Round number:  67  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1097) KL loss: tensor(0.0717)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1507) KL loss: tensor(0.0872)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2221) KL loss: tensor(0.0962)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2131) KL loss: tensor(0.0949)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4319) KL loss: tensor(0.1360)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1272) KL loss: tensor(0.0773)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3213) KL loss: tensor(0.1285)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1987) KL loss: tensor(0.0982)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1926) KL loss: tensor(0.0987)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3088) KL loss: tensor(0.1042)
Global Model Acc on global data: 0.5776 length of data: 10000
-------------Round number:  68  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1068) KL loss: tensor(0.0700)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2800) KL loss: tensor(0.0989)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2297) KL loss: tensor(0.0999)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2566) KL loss: tensor(0.1189)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2389) KL loss: tensor(0.1201)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0849) KL loss: tensor(0.0642)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1954) KL loss: tensor(0.0721)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2275) KL loss: tensor(0.0916)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2514) KL loss: tensor(0.0917)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1484) KL loss: tensor(0.0798)
Global Model Acc on global data: 0.583 length of data: 10000
-------------Round number:  69  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2152) KL loss: tensor(0.0883)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2059) KL loss: tensor(0.0985)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3136) KL loss: tensor(0.1083)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2122) KL loss: tensor(0.0866)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1327) KL loss: tensor(0.0662)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1888) KL loss: tensor(0.0873)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3324) KL loss: tensor(0.1279)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2865) KL loss: tensor(0.1592)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1991) KL loss: tensor(0.0958)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3439) KL loss: tensor(0.1136)
Global Model Acc on global data: 0.5822 length of data: 10000
-------------Round number:  70  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2288) KL loss: tensor(0.1172)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3112) KL loss: tensor(0.1177)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4781) KL loss: tensor(0.1951)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6099) KL loss: tensor(0.2439)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3112) KL loss: tensor(0.1124)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0847) KL loss: tensor(0.0657)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1585) KL loss: tensor(0.0794)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2038) KL loss: tensor(0.0794)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2027) KL loss: tensor(0.1125)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1108) KL loss: tensor(0.0792)
Global Model Acc on global data: 0.5747 length of data: 10000
-------------Round number:  71  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2611) KL loss: tensor(0.1056)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1954) KL loss: tensor(0.0777)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1352) KL loss: tensor(0.0723)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1259) KL loss: tensor(0.0807)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1825) KL loss: tensor(0.0841)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2077) KL loss: tensor(0.0923)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3744) KL loss: tensor(0.1853)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2155) KL loss: tensor(0.0994)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1633) KL loss: tensor(0.0840)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1060) KL loss: tensor(0.0688)
Global Model Acc on global data: 0.5774 length of data: 10000
-------------Round number:  72  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1651) KL loss: tensor(0.0650)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1574) KL loss: tensor(0.0874)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2620) KL loss: tensor(0.1014)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1901) KL loss: tensor(0.0908)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1607) KL loss: tensor(0.0709)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1395) KL loss: tensor(0.0767)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4716) KL loss: tensor(0.2055)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2536) KL loss: tensor(0.1236)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2333) KL loss: tensor(0.0903)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1019) KL loss: tensor(0.0752)
Global Model Acc on global data: 0.5811 length of data: 10000
-------------Round number:  73  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5253) KL loss: tensor(0.2571)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1629) KL loss: tensor(0.0828)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1838) KL loss: tensor(0.0892)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1521) KL loss: tensor(0.0845)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2240) KL loss: tensor(0.0863)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3173) KL loss: tensor(0.1080)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3672) KL loss: tensor(0.1292)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1097) KL loss: tensor(0.0541)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1899) KL loss: tensor(0.0896)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1475) KL loss: tensor(0.0863)
Global Model Acc on global data: 0.5682 length of data: 10000
-------------Round number:  74  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1365) KL loss: tensor(0.0716)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1807) KL loss: tensor(0.0884)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1950) KL loss: tensor(0.0823)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1403) KL loss: tensor(0.0769)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2270) KL loss: tensor(0.0876)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2097) KL loss: tensor(0.0802)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2264) KL loss: tensor(0.0982)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1075) KL loss: tensor(0.0719)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2134) KL loss: tensor(0.0978)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1761) KL loss: tensor(0.0730)
Global Model Acc on global data: 0.5793 length of data: 10000
-------------Round number:  75  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2603) KL loss: tensor(0.0984)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1559) KL loss: tensor(0.0817)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1620) KL loss: tensor(0.0756)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2826) KL loss: tensor(0.0982)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1684) KL loss: tensor(0.0766)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3277) KL loss: tensor(0.1119)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0889) KL loss: tensor(0.0632)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0676) KL loss: tensor(0.0527)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2095) KL loss: tensor(0.0906)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3166) KL loss: tensor(0.1118)
Global Model Acc on global data: 0.5813 length of data: 10000
-------------Round number:  76  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1925) KL loss: tensor(0.1141)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0608) KL loss: tensor(0.0482)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6496) KL loss: tensor(0.3033)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2874) KL loss: tensor(0.1082)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1521) KL loss: tensor(0.0913)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2093) KL loss: tensor(0.0811)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2634) KL loss: tensor(0.1156)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2852) KL loss: tensor(0.1553)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1944) KL loss: tensor(0.0993)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3905) KL loss: tensor(0.1604)
Global Model Acc on global data: 0.5766 length of data: 10000
-------------Round number:  77  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2430) KL loss: tensor(0.1084)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1887) KL loss: tensor(0.0867)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2608) KL loss: tensor(0.1131)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3406) KL loss: tensor(0.1266)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1704) KL loss: tensor(0.0881)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1687) KL loss: tensor(0.0920)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2141) KL loss: tensor(0.1029)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2027) KL loss: tensor(0.1069)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2391) KL loss: tensor(0.0948)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2835) KL loss: tensor(0.0979)
Global Model Acc on global data: 0.5707 length of data: 10000
-------------Round number:  78  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1212) KL loss: tensor(0.0777)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2286) KL loss: tensor(0.0903)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1505) KL loss: tensor(0.0863)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1676) KL loss: tensor(0.0757)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2408) KL loss: tensor(0.0905)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0947) KL loss: tensor(0.0687)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2166) KL loss: tensor(0.1100)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1918) KL loss: tensor(0.0891)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1754) KL loss: tensor(0.0945)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2184) KL loss: tensor(0.0955)
Global Model Acc on global data: 0.577 length of data: 10000
-------------Round number:  79  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2306) KL loss: tensor(0.0934)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1695) KL loss: tensor(0.0758)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1747) KL loss: tensor(0.0949)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3040) KL loss: tensor(0.1581)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1535) KL loss: tensor(0.0768)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5303) KL loss: tensor(0.1728)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2582) KL loss: tensor(0.1084)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1654) KL loss: tensor(0.0935)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2859) KL loss: tensor(0.1062)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2221) KL loss: tensor(0.0943)
Global Model Acc on global data: 0.5767 length of data: 10000
-------------Round number:  80  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0683) KL loss: tensor(0.0527)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3504) KL loss: tensor(0.1269)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1230) KL loss: tensor(0.0722)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1846) KL loss: tensor(0.0843)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3634) KL loss: tensor(0.1072)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2020) KL loss: tensor(0.0872)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1532) KL loss: tensor(0.0704)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1215) KL loss: tensor(0.0716)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2434) KL loss: tensor(0.0892)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1942) KL loss: tensor(0.0908)
Global Model Acc on global data: 0.58 length of data: 10000
-------------Round number:  81  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0926) KL loss: tensor(0.0570)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3643) KL loss: tensor(0.1796)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2291) KL loss: tensor(0.0998)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4620) KL loss: tensor(0.1653)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2713) KL loss: tensor(0.0988)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2350) KL loss: tensor(0.1130)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1602) KL loss: tensor(0.0753)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1393) KL loss: tensor(0.0749)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1804) KL loss: tensor(0.0799)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2244) KL loss: tensor(0.1175)
Global Model Acc on global data: 0.5721 length of data: 10000
-------------Round number:  82  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2222) KL loss: tensor(0.0956)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0939) KL loss: tensor(0.0571)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2188) KL loss: tensor(0.0873)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0901) KL loss: tensor(0.0677)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1951) KL loss: tensor(0.0786)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1600) KL loss: tensor(0.0734)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2504) KL loss: tensor(0.0977)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1650) KL loss: tensor(0.0830)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2511) KL loss: tensor(0.1116)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1553) KL loss: tensor(0.0901)
Global Model Acc on global data: 0.5782 length of data: 10000
-------------Round number:  83  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1484) KL loss: tensor(0.0759)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2702) KL loss: tensor(0.1156)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0995) KL loss: tensor(0.0664)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1218) KL loss: tensor(0.0650)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2608) KL loss: tensor(0.0866)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1652) KL loss: tensor(0.0724)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1575) KL loss: tensor(0.0791)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1248) KL loss: tensor(0.0720)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2991) KL loss: tensor(0.1088)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2242) KL loss: tensor(0.1054)
Global Model Acc on global data: 0.5819 length of data: 10000
-------------Round number:  84  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1803) KL loss: tensor(0.0848)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1995) KL loss: tensor(0.0827)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1819) KL loss: tensor(0.0922)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1898) KL loss: tensor(0.1037)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2280) KL loss: tensor(0.0897)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2765) KL loss: tensor(0.1065)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5874) KL loss: tensor(0.2437)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1412) KL loss: tensor(0.0737)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1166) KL loss: tensor(0.0687)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1697) KL loss: tensor(0.0954)
Global Model Acc on global data: 0.5783 length of data: 10000
-------------Round number:  85  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2364) KL loss: tensor(0.1195)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1679) KL loss: tensor(0.0803)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1475) KL loss: tensor(0.0892)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2121) KL loss: tensor(0.0912)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2004) KL loss: tensor(0.0869)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3804) KL loss: tensor(0.1683)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1560) KL loss: tensor(0.0918)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2246) KL loss: tensor(0.1259)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2523) KL loss: tensor(0.1051)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2124) KL loss: tensor(0.0908)
Global Model Acc on global data: 0.5833 length of data: 10000
-------------Round number:  86  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1060) KL loss: tensor(0.0662)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2003) KL loss: tensor(0.1085)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1995) KL loss: tensor(0.0957)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2318) KL loss: tensor(0.0982)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1392) KL loss: tensor(0.0785)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1087) KL loss: tensor(0.0666)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4420) KL loss: tensor(0.2248)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4872) KL loss: tensor(0.2409)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1359) KL loss: tensor(0.0727)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2583) KL loss: tensor(0.1095)
Global Model Acc on global data: 0.5835 length of data: 10000
-------------Round number:  87  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1468) KL loss: tensor(0.0807)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1742) KL loss: tensor(0.0998)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1371) KL loss: tensor(0.0755)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5259) KL loss: tensor(0.2365)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1772) KL loss: tensor(0.0893)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1676) KL loss: tensor(0.0860)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2387) KL loss: tensor(0.1035)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0930) KL loss: tensor(0.0642)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2509) KL loss: tensor(0.1218)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1341) KL loss: tensor(0.0763)
Global Model Acc on global data: 0.5813 length of data: 10000
-------------Round number:  88  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1201) KL loss: tensor(0.0742)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2134) KL loss: tensor(0.0962)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2611) KL loss: tensor(0.0938)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1360) KL loss: tensor(0.0648)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0943) KL loss: tensor(0.0689)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1682) KL loss: tensor(0.0898)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2169) KL loss: tensor(0.0805)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1700) KL loss: tensor(0.0841)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1745) KL loss: tensor(0.0844)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3545) KL loss: tensor(0.1180)
Global Model Acc on global data: 0.5788 length of data: 10000
-------------Round number:  89  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2044) KL loss: tensor(0.1053)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1988) KL loss: tensor(0.0986)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1928) KL loss: tensor(0.0899)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1298) KL loss: tensor(0.0735)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1563) KL loss: tensor(0.0712)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1867) KL loss: tensor(0.1003)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1942) KL loss: tensor(0.1137)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3672) KL loss: tensor(0.1326)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1793) KL loss: tensor(0.0845)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1972) KL loss: tensor(0.1030)
Global Model Acc on global data: 0.5842 length of data: 10000
-------------Round number:  90  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1531) KL loss: tensor(0.0815)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1026) KL loss: tensor(0.0662)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3003) KL loss: tensor(0.1226)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2207) KL loss: tensor(0.1178)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1707) KL loss: tensor(0.0951)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1731) KL loss: tensor(0.0810)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1517) KL loss: tensor(0.0877)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2198) KL loss: tensor(0.0936)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1967) KL loss: tensor(0.0965)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2341) KL loss: tensor(0.1038)
Global Model Acc on global data: 0.5807 length of data: 10000
-------------Round number:  91  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1736) KL loss: tensor(0.0833)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1360) KL loss: tensor(0.0754)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2413) KL loss: tensor(0.1138)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1852) KL loss: tensor(0.0893)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4966) KL loss: tensor(0.1858)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1627) KL loss: tensor(0.0832)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1464) KL loss: tensor(0.0749)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2764) KL loss: tensor(0.1159)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2085) KL loss: tensor(0.1112)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2759) KL loss: tensor(0.0987)
Global Model Acc on global data: 0.5793 length of data: 10000
-------------Round number:  92  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1089) KL loss: tensor(0.0717)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2467) KL loss: tensor(0.0886)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1273) KL loss: tensor(0.0661)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2395) KL loss: tensor(0.0874)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3021) KL loss: tensor(0.0992)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1547) KL loss: tensor(0.0807)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3588) KL loss: tensor(0.1665)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2118) KL loss: tensor(0.0867)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3410) KL loss: tensor(0.1010)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1354) KL loss: tensor(0.0712)
Global Model Acc on global data: 0.5871 length of data: 10000
save a model
-------------Round number:  93  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1551) KL loss: tensor(0.0847)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1441) KL loss: tensor(0.0895)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1596) KL loss: tensor(0.0867)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1396) KL loss: tensor(0.0882)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1678) KL loss: tensor(0.0938)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2143) KL loss: tensor(0.0939)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1803) KL loss: tensor(0.1054)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2408) KL loss: tensor(0.1020)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2124) KL loss: tensor(0.0861)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2006) KL loss: tensor(0.0864)
Global Model Acc on global data: 0.5869 length of data: 10000
-------------Round number:  94  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3034) KL loss: tensor(0.1030)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1077) KL loss: tensor(0.0596)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1676) KL loss: tensor(0.0961)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1482) KL loss: tensor(0.0723)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1976) KL loss: tensor(0.0843)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1892) KL loss: tensor(0.0846)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3721) KL loss: tensor(0.1650)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2202) KL loss: tensor(0.1028)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1480) KL loss: tensor(0.0787)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0923) KL loss: tensor(0.0658)
Global Model Acc on global data: 0.5864 length of data: 10000
-------------Round number:  95  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5187) KL loss: tensor(0.2191)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1247) KL loss: tensor(0.0795)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1451) KL loss: tensor(0.0865)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2882) KL loss: tensor(0.1684)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1052) KL loss: tensor(0.0716)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2323) KL loss: tensor(0.1065)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2420) KL loss: tensor(0.0880)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1004) KL loss: tensor(0.0719)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2461) KL loss: tensor(0.1450)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1553) KL loss: tensor(0.0865)
Global Model Acc on global data: 0.5874 length of data: 10000
save a model
-------------Round number:  96  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1885) KL loss: tensor(0.0917)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2166) KL loss: tensor(0.0990)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1682) KL loss: tensor(0.0873)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2133) KL loss: tensor(0.0861)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1577) KL loss: tensor(0.0792)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0808) KL loss: tensor(0.0577)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1251) KL loss: tensor(0.0728)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1445) KL loss: tensor(0.0919)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2026) KL loss: tensor(0.0862)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3676) KL loss: tensor(0.1683)
Global Model Acc on global data: 0.5869 length of data: 10000
-------------Round number:  97  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1690) KL loss: tensor(0.0890)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2370) KL loss: tensor(0.0974)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1669) KL loss: tensor(0.0886)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1250) KL loss: tensor(0.0718)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1037) KL loss: tensor(0.0651)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0834) KL loss: tensor(0.0637)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1879) KL loss: tensor(0.0880)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1484) KL loss: tensor(0.0838)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1906) KL loss: tensor(0.0881)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2080) KL loss: tensor(0.0897)
Global Model Acc on global data: 0.5812 length of data: 10000
-------------Round number:  98  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3648) KL loss: tensor(0.1246)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1754) KL loss: tensor(0.0779)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2175) KL loss: tensor(0.0879)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1575) KL loss: tensor(0.0817)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0798) KL loss: tensor(0.0585)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1329) KL loss: tensor(0.0729)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1724) KL loss: tensor(0.0773)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2818) KL loss: tensor(0.0943)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1368) KL loss: tensor(0.0690)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1913) KL loss: tensor(0.0964)
Global Model Acc on global data: 0.57 length of data: 10000
-------------Round number:  99  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1118) KL loss: tensor(0.0688)
loss: NT_CE learning rate: 0.0001
