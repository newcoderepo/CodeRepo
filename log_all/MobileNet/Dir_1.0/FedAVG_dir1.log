nohup: ignoring input
================================================================================
Summary of training process:
Dataset:                 Cifar10
Batch size:              64
Learing rate :           0.001
personal learning rate : 0.001
Number of total clients: 100
Split parameter        : 1.0
Clients per round      : 10
Number of global rounds: 100
Number of local rounds : 10
Feature from layer     : 11
Feature reduction      : 64
Local training loss    : CE
Loss of beta           : 1.0
Algorithm              : FedAvg
Modelname              : MOBNET
Mode                   : training
================================================================================
Files already downloaded and verified
Files already downloaded and verified
Here
Done
Class frequencies:
client,class0,class1,class2,class3,class4,class5,class6,class7,class8,class9,Amount
Client   0,0.030,0.038,0.170,0.050,0.006,0.110,0.088,0.146,0.114,0.250,501
Client   1,0.185,0.153,0.00,0.052,0.045,0.072,0.00,0.095,0.150,0.250,601
Client   2,0.116,0.061,0.133,0.177,0.110,0.041,0.026,0.057,0.279,0.00,610
Client   3,0.030,0.075,0.144,0.033,0.234,0.063,0.048,0.075,0.210,0.090,334
Client   4,0.042,0.023,0.069,0.031,0.023,0.027,0.025,0.084,0.322,0.354,478
Client   5,0.103,0.451,0.099,0.009,0.023,0.026,0.033,0.007,0.171,0.077,426
Client   6,0.187,0.136,0.033,0.041,0.012,0.022,0.138,0.196,0.236,0.00,509
Client   7,0.019,0.035,0.010,0.027,0.085,0.022,0.083,0.034,0.259,0.426,625
Client   8,0.010,0.221,0.014,0.019,0.027,0.171,0.539,0.00,0.00,0.00,592
Client   9,0.035,0.063,0.033,0.056,0.038,0.161,0.090,0.035,0.163,0.327,735
Client  10,0.226,0.027,0.039,0.027,0.065,0.044,0.126,0.277,0.168,0.00,585
Client  11,0.073,0.145,0.021,0.043,0.038,0.103,0.009,0.209,0.098,0.261,234
Client  12,0.011,0.051,0.021,0.151,0.030,0.091,0.042,0.285,0.319,0.00,571
Client  13,0.008,0.051,0.107,0.027,0.035,0.011,0.238,0.112,0.043,0.369,374
Client  14,0.046,0.109,0.228,0.352,0.133,0.012,0.081,0.040,0.00,0.00,505
Client  15,0.007,0.196,0.017,0.063,0.029,0.419,0.070,0.010,0.058,0.131,413
Client  16,0.069,0.032,0.262,0.052,0.013,0.146,0.004,0.074,0.044,0.303,679
Client  17,0.089,0.045,0.206,0.092,0.006,0.066,0.021,0.095,0.380,0.00,671
Client  18,0.062,0.310,0.049,0.204,0.015,0.056,0.084,0.220,0.00,0.00,549
Client  19,0.122,0.025,0.466,0.144,0.243,0.00,0.00,0.00,0.00,0.00,597
Client  20,0.091,0.158,0.017,0.065,0.041,0.129,0.065,0.179,0.067,0.189,418
Client  21,0.110,0.166,0.030,0.157,0.00,0.025,0.120,0.102,0.289,0.00,591
Client  22,0.027,0.123,0.216,0.109,0.189,0.016,0.130,0.014,0.176,0.00,561
Client  23,0.164,0.029,0.027,0.252,0.137,0.038,0.027,0.066,0.027,0.235,452
Client  24,0.069,0.054,0.065,0.072,0.180,0.113,0.281,0.030,0.017,0.120,540
Client  25,0.150,0.110,0.018,0.204,0.004,0.105,0.049,0.034,0.034,0.293,447
Client  26,0.068,0.098,0.088,0.015,0.009,0.051,0.034,0.030,0.045,0.562,468
Client  27,0.067,0.275,0.009,0.016,0.046,0.106,0.033,0.158,0.129,0.162,568
Client  28,0.005,0.057,0.069,0.017,0.104,0.084,0.366,0.040,0.030,0.228,404
Client  29,0.062,0.066,0.160,0.027,0.008,0.012,0.289,0.094,0.282,0.00,595
Client  30,0.019,0.062,0.190,0.093,0.062,0.050,0.228,0.081,0.211,0.005,421
Client  31,0.020,0.124,0.037,0.057,0.201,0.119,0.072,0.117,0.132,0.122,403
Client  32,0.081,0.083,0.130,0.010,0.119,0.073,0.023,0.034,0.055,0.392,385
Client  33,0.092,0.314,0.016,0.149,0.084,0.035,0.287,0.022,0.00,0.00,509
Client  34,0.043,0.126,0.266,0.185,0.239,0.053,0.028,0.059,0.00,0.00,507
Client  35,0.017,0.058,0.322,0.053,0.102,0.135,0.043,0.054,0.216,0.00,606
Client  36,0.152,0.073,0.186,0.031,0.169,0.033,0.010,0.219,0.058,0.067,479
Client  37,0.285,0.024,0.041,0.029,0.006,0.164,0.084,0.137,0.229,0.00,628
Client  38,0.218,0.036,0.088,0.055,0.097,0.027,0.012,0.042,0.121,0.303,330
Client  39,0.011,0.068,0.092,0.279,0.068,0.014,0.014,0.141,0.136,0.179,369
Client  40,0.154,0.026,0.003,0.025,0.046,0.003,0.109,0.020,0.159,0.455,649
Client  41,0.007,0.044,0.056,0.322,0.065,0.061,0.080,0.017,0.015,0.334,413
Client  42,0.060,0.023,0.271,0.006,0.050,0.175,0.194,0.027,0.194,0.00,520
Client  43,0.406,0.082,0.003,0.023,0.237,0.014,0.020,0.073,0.127,0.017,355
Client  44,0.031,0.027,0.236,0.100,0.253,0.110,0.133,0.110,0.00,0.00,518
Client  45,0.035,0.126,0.113,0.100,0.013,0.026,0.278,0.017,0.204,0.087,230
Client  46,0.155,0.137,0.040,0.258,0.149,0.002,0.058,0.201,0.00,0.00,503
Client  47,0.333,0.026,0.119,0.117,0.037,0.133,0.056,0.047,0.070,0.063,429
Client  48,0.038,0.372,0.147,0.053,0.059,0.055,0.005,0.271,0.00,0.00,546
Client  49,0.027,0.029,0.012,0.018,0.271,0.082,0.076,0.107,0.012,0.366,513
Client  50,0.032,0.014,0.346,0.017,0.046,0.101,0.121,0.092,0.130,0.101,347
Client  51,0.358,0.112,0.180,0.320,0.008,0.008,0.014,0.00,0.00,0.00,500
Client  52,0.098,0.013,0.163,0.075,0.117,0.002,0.293,0.017,0.071,0.151,478
Client  53,0.208,0.063,0.029,0.308,0.118,0.275,0.00,0.00,0.00,0.00,510
Client  54,0.035,0.074,0.030,0.248,0.058,0.049,0.100,0.081,0.056,0.268,568
Client  55,0.008,0.249,0.008,0.277,0.018,0.440,0.00,0.00,0.00,0.00,502
Client  56,0.047,0.138,0.009,0.111,0.239,0.074,0.016,0.061,0.306,0.00,687
Client  57,0.026,0.065,0.043,0.002,0.036,0.022,0.103,0.361,0.156,0.187,507
Client  58,0.079,0.217,0.061,0.178,0.097,0.368,0.00,0.00,0.00,0.00,506
Client  59,0.081,0.169,0.318,0.008,0.179,0.035,0.010,0.200,0.00,0.00,509
Client  60,0.027,0.016,0.033,0.123,0.006,0.349,0.106,0.011,0.329,0.00,705
Client  61,0.039,0.114,0.091,0.171,0.030,0.159,0.167,0.130,0.065,0.033,508
Client  62,0.003,0.100,0.006,0.173,0.128,0.116,0.145,0.034,0.031,0.264,648
Client  63,0.069,0.132,0.057,0.033,0.230,0.065,0.278,0.045,0.033,0.057,418
Client  64,0.039,0.050,0.013,0.161,0.094,0.131,0.142,0.030,0.083,0.258,542
Client  65,0.090,0.003,0.189,0.037,0.048,0.185,0.284,0.163,0.00,0.00,588
Client  66,0.196,0.044,0.063,0.035,0.083,0.093,0.094,0.391,0.00,0.00,540
Client  67,0.074,0.004,0.145,0.004,0.495,0.278,0.00,0.00,0.00,0.00,503
Client  68,0.012,0.036,0.017,0.010,0.046,0.017,0.017,0.065,0.072,0.708,414
Client  69,0.296,0.192,0.132,0.045,0.014,0.018,0.204,0.055,0.006,0.038,494
Client  70,0.167,0.061,0.406,0.131,0.130,0.106,0.00,0.00,0.00,0.00,540
Client  71,0.046,0.020,0.027,0.108,0.179,0.207,0.201,0.060,0.150,0.00,546
Client  72,0.039,0.261,0.076,0.061,0.002,0.011,0.007,0.345,0.197,0.00,537
Client  73,0.296,0.020,0.010,0.298,0.039,0.125,0.088,0.124,0.00,0.00,510
Client  74,0.054,0.139,0.246,0.037,0.070,0.048,0.126,0.039,0.128,0.113,460
Client  75,0.110,0.030,0.002,0.114,0.184,0.042,0.291,0.066,0.082,0.078,499
Client  76,0.411,0.013,0.002,0.074,0.025,0.009,0.101,0.366,0.00,0.00,557
Client  77,0.007,0.093,0.057,0.073,0.177,0.023,0.007,0.147,0.087,0.330,300
Client  78,0.054,0.049,0.003,0.005,0.132,0.130,0.280,0.067,0.010,0.269,386
Client  79,0.027,0.082,0.012,0.051,0.133,0.039,0.329,0.327,0.00,0.00,513
Client  80,0.257,0.00,0.056,0.021,0.160,0.506,0.00,0.00,0.00,0.00,536
Client  81,0.085,0.050,0.312,0.025,0.088,0.010,0.310,0.015,0.106,0.00,520
Client  82,0.005,0.037,0.136,0.198,0.285,0.115,0.030,0.047,0.014,0.133,572
Client  83,0.151,0.024,0.070,0.209,0.064,0.121,0.087,0.141,0.133,0.00,503
Client  84,0.110,0.131,0.046,0.065,0.058,0.210,0.018,0.362,0.00,0.00,566
Client  85,0.107,0.024,0.033,0.228,0.239,0.031,0.042,0.108,0.190,0.00,553
Client  86,0.324,0.044,0.056,0.073,0.321,0.060,0.023,0.100,0.00,0.00,521
Client  87,0.212,0.008,0.008,0.053,0.021,0.120,0.214,0.143,0.222,0.00,532
Client  88,0.148,0.160,0.086,0.051,0.121,0.058,0.201,0.103,0.072,0.00,513
Client  89,0.025,0.195,0.338,0.135,0.174,0.132,0.00,0.00,0.00,0.00,517
Client  90,0.016,0.395,0.165,0.054,0.171,0.200,0.00,0.00,0.00,0.00,504
Client  91,0.035,0.388,0.058,0.045,0.088,0.018,0.020,0.090,0.183,0.075,399
Client  92,0.039,0.111,0.098,0.007,0.160,0.186,0.046,0.049,0.124,0.182,307
Client  93,0.121,0.047,0.103,0.224,0.222,0.037,0.018,0.079,0.018,0.129,379
Client  94,0.161,0.166,0.199,0.030,0.033,0.035,0.035,0.202,0.106,0.033,367
Client  95,0.081,0.135,0.010,0.111,0.164,0.006,0.143,0.265,0.00,0.085,495
Client  96,0.010,0.262,0.081,0.008,0.043,0.096,0.242,0.020,0.128,0.111,397
Client  97,0.055,0.027,0.018,0.191,0.205,0.320,0.068,0.011,0.093,0.011,440
Client  98,0.307,0.043,0.083,0.002,0.026,0.201,0.021,0.120,0.196,0.00,606
Client  99,0.129,0.160,0.055,0.311,0.091,0.050,0.020,0.182,0.002,0.00,505
Num_samples of Training set per client: [501, 601, 610, 334, 478, 426, 509, 625, 592, 735, 585, 234, 571, 374, 505, 413, 679, 671, 549, 597, 418, 591, 561, 452, 540, 447, 468, 568, 404, 595, 421, 403, 385, 509, 507, 606, 479, 628, 330, 369, 649, 413, 520, 355, 518, 230, 503, 429, 546, 513, 347, 500, 478, 510, 568, 502, 687, 507, 506, 509, 705, 508, 648, 418, 542, 588, 540, 503, 414, 494, 540, 546, 537, 510, 460, 499, 557, 300, 386, 513, 536, 520, 572, 503, 566, 553, 521, 532, 513, 517, 504, 399, 307, 379, 367, 495, 397, 440, 606, 505]
Total_training_samples: 50000
Global test set: 10000
Finish Generating Samples, distribution saved
MobileNetV2(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layers): Sequential(
    (0): Block(
      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Block(
      (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (6): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (7): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (8): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (9): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (10): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (12): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (13): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (14): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (15): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (16): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv2): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear): Linear(in_features=1280, out_features=10, bias=True)
)
number of parameters: 2296922
clients initializting...
output size: 10
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<01:33,  1.06it/s]  2%|▏         | 2/100 [00:05<03:05,  1.89s/it]  3%|▎         | 3/100 [00:05<02:34,  1.60s/it]  4%|▍         | 4/100 [00:06<02:14,  1.40s/it]  5%|▌         | 5/100 [00:07<01:59,  1.26s/it]  6%|▌         | 6/100 [00:08<01:49,  1.16s/it]  7%|▋         | 7/100 [00:09<01:41,  1.09s/it]  8%|▊         | 8/100 [00:10<01:35,  1.04s/it]  9%|▉         | 9/100 [00:11<01:28,  1.03it/s] 10%|█         | 10/100 [00:12<01:26,  1.04it/s] 11%|█         | 11/100 [00:13<01:23,  1.06it/s] 12%|█▏        | 12/100 [00:14<01:22,  1.06it/s] 13%|█▎        | 13/100 [00:15<01:20,  1.08it/s] 14%|█▍        | 14/100 [00:15<01:19,  1.08it/s] 15%|█▌        | 15/100 [00:16<01:16,  1.11it/s] 16%|█▌        | 16/100 [00:17<01:16,  1.10it/s] 17%|█▋        | 17/100 [00:18<01:16,  1.08it/s] 18%|█▊        | 18/100 [00:19<01:15,  1.09it/s] 19%|█▉        | 19/100 [00:20<01:12,  1.11it/s] 20%|██        | 20/100 [00:21<01:07,  1.18it/s] 21%|██        | 21/100 [00:22<01:08,  1.15it/s] 22%|██▏       | 22/100 [00:22<01:07,  1.16it/s] 23%|██▎       | 23/100 [00:23<01:07,  1.15it/s] 24%|██▍       | 24/100 [00:24<01:08,  1.11it/s] 25%|██▌       | 25/100 [00:25<01:08,  1.09it/s] 26%|██▌       | 26/100 [00:26<01:08,  1.07it/s] 27%|██▋       | 27/100 [00:30<02:18,  1.90s/it] 28%|██▊       | 28/100 [00:31<01:56,  1.61s/it] 29%|██▉       | 29/100 [00:32<01:40,  1.41s/it] 30%|███       | 30/100 [00:33<01:28,  1.26s/it] 31%|███       | 31/100 [00:34<01:20,  1.17s/it] 32%|███▏      | 32/100 [00:35<01:14,  1.10s/it] 33%|███▎      | 33/100 [00:36<01:10,  1.05s/it] 34%|███▍      | 34/100 [00:37<01:05,  1.01it/s] 35%|███▌      | 35/100 [00:38<01:01,  1.05it/s] 36%|███▌      | 36/100 [00:39<01:00,  1.07it/s] 37%|███▋      | 37/100 [00:40<00:59,  1.05it/s] 38%|███▊      | 38/100 [00:41<00:58,  1.07it/s] 39%|███▉      | 39/100 [00:41<00:57,  1.07it/s] 40%|████      | 40/100 [00:42<00:56,  1.07it/s] 41%|████      | 41/100 [00:43<00:55,  1.05it/s] 42%|████▏     | 42/100 [00:44<00:55,  1.05it/s] 43%|████▎     | 43/100 [00:45<00:53,  1.06it/s] 44%|████▍     | 44/100 [00:46<00:52,  1.06it/s] 45%|████▌     | 45/100 [00:47<00:50,  1.09it/s] 46%|████▌     | 46/100 [00:48<00:49,  1.08it/s] 47%|████▋     | 47/100 [00:49<00:48,  1.10it/s] 48%|████▊     | 48/100 [00:50<00:48,  1.08it/s] 49%|████▉     | 49/100 [00:51<00:46,  1.10it/s] 50%|█████     | 50/100 [00:52<00:46,  1.09it/s] 51%|█████     | 51/100 [00:53<00:45,  1.08it/s] 52%|█████▏    | 52/100 [00:57<01:30,  1.88s/it] 53%|█████▎    | 53/100 [00:58<01:15,  1.60s/it] 54%|█████▍    | 54/100 [00:58<01:02,  1.36s/it] 55%|█████▌    | 55/100 [00:59<00:56,  1.25s/it] 56%|█████▌    | 56/100 [01:00<00:48,  1.11s/it] 57%|█████▋    | 57/100 [01:01<00:45,  1.05s/it] 58%|█████▊    | 58/100 [01:02<00:42,  1.02s/it] 59%|█████▉    | 59/100 [01:03<00:38,  1.05it/s] 60%|██████    | 60/100 [01:04<00:37,  1.08it/s] 61%|██████    | 61/100 [01:05<00:36,  1.08it/s] 62%|██████▏   | 62/100 [01:06<00:35,  1.08it/s] 63%|██████▎   | 63/100 [01:07<00:34,  1.07it/s] 64%|██████▍   | 64/100 [01:08<00:33,  1.06it/s] 65%|██████▌   | 65/100 [01:09<00:33,  1.05it/s] 66%|██████▌   | 66/100 [01:09<00:31,  1.08it/s] 67%|██████▋   | 67/100 [01:10<00:30,  1.09it/s] 68%|██████▊   | 68/100 [01:11<00:27,  1.14it/s] 69%|██████▉   | 69/100 [01:12<00:27,  1.11it/s] 70%|███████   | 70/100 [01:13<00:27,  1.09it/s] 71%|███████   | 71/100 [01:14<00:25,  1.14it/s] 72%|███████▏  | 72/100 [01:15<00:24,  1.13it/s] 73%|███████▎  | 73/100 [01:16<00:24,  1.12it/s] 74%|███████▍  | 74/100 [01:16<00:23,  1.13it/s] 75%|███████▌  | 75/100 [01:17<00:22,  1.10it/s] 76%|███████▌  | 76/100 [01:18<00:22,  1.08it/s] 77%|███████▋  | 77/100 [01:19<00:20,  1.10it/s] 78%|███████▊  | 78/100 [01:23<00:42,  1.91s/it] 79%|███████▉  | 79/100 [01:24<00:34,  1.62s/it] 80%|████████  | 80/100 [01:25<00:28,  1.41s/it] 81%|████████  | 81/100 [01:26<00:22,  1.21s/it] 82%|████████▏ | 82/100 [01:27<00:20,  1.12s/it] 83%|████████▎ | 83/100 [01:28<00:18,  1.07s/it] 84%|████████▍ | 84/100 [01:29<00:16,  1.02s/it] 85%|████████▌ | 85/100 [01:30<00:14,  1.02it/s] 86%|████████▌ | 86/100 [01:31<00:13,  1.03it/s] 87%|████████▋ | 87/100 [01:32<00:12,  1.07it/s] 88%|████████▊ | 88/100 [01:32<00:11,  1.08it/s] 89%|████████▉ | 89/100 [01:33<00:10,  1.09it/s] 90%|█████████ | 90/100 [01:34<00:08,  1.14it/s] 91%|█████████ | 91/100 [01:35<00:07,  1.18it/s] 92%|█████████▏| 92/100 [01:36<00:07,  1.13it/s] 93%|█████████▎| 93/100 [01:37<00:06,  1.11it/s] 94%|█████████▍| 94/100 [01:38<00:05,  1.09it/s] 95%|█████████▌| 95/100 [01:39<00:04,  1.09it/s] 96%|█████████▌| 96/100 [01:40<00:03,  1.09it/s] 97%|█████████▋| 97/100 [01:41<00:02,  1.08it/s] 98%|█████████▊| 98/100 [01:41<00:01,  1.08it/s] 99%|█████████▉| 99/100 [01:42<00:00,  1.08it/s]100%|██████████| 100/100 [01:43<00:00,  1.09it/s]100%|██████████| 100/100 [01:43<00:00,  1.04s/it]
Number of users per round / total users: 10  /  100
Finished creating FedAvg server.
=== FedAvg ===
-------------Round number:  0  -------------
loss: CE learning rate: 0.001
training loss: tensor(0.9215)
loss: CE learning rate: 0.001
training loss: tensor(0.9987)
loss: CE learning rate: 0.001
training loss: tensor(1.1716)
loss: CE learning rate: 0.001
training loss: tensor(0.6742)
loss: CE learning rate: 0.001
training loss: tensor(1.1154)
loss: CE learning rate: 0.001
training loss: tensor(1.0932)
loss: CE learning rate: 0.001
training loss: tensor(1.1121)
loss: CE learning rate: 0.001
training loss: tensor(0.9127)
loss: CE learning rate: 0.001
training loss: tensor(1.1029)
loss: CE learning rate: 0.001
training loss: tensor(1.1311)
         GM acc on global data: 0.1003 length of data: 10000
save a model
-------------Round number:  1  -------------
loss: CE learning rate: 0.000982
training loss: tensor(0.5386)
loss: CE learning rate: 0.000982
training loss: tensor(0.7548)
loss: CE learning rate: 0.000982
training loss: tensor(0.7526)
loss: CE learning rate: 0.000982
training loss: tensor(0.9762)
loss: CE learning rate: 0.000982
training loss: tensor(0.8326)
loss: CE learning rate: 0.000982
training loss: tensor(0.9844)
loss: CE learning rate: 0.000982
training loss: tensor(0.9583)
loss: CE learning rate: 0.000982
training loss: tensor(0.9528)
loss: CE learning rate: 0.000982
training loss: tensor(0.8437)
loss: CE learning rate: 0.000982
training loss: tensor(0.9479)
         GM acc on global data: 0.1209 length of data: 10000
save a model
-------------Round number:  2  -------------
loss: CE learning rate: 0.000964
training loss: tensor(0.6989)
loss: CE learning rate: 0.000964
training loss: tensor(0.7413)
loss: CE learning rate: 0.000964
training loss: tensor(0.7073)
loss: CE learning rate: 0.000964
training loss: tensor(0.8362)
loss: CE learning rate: 0.000964
training loss: tensor(0.8720)
loss: CE learning rate: 0.000964
training loss: tensor(0.9709)
loss: CE learning rate: 0.000964
training loss: tensor(0.7873)
loss: CE learning rate: 0.000964
training loss: tensor(0.8612)
loss: CE learning rate: 0.000964
training loss: tensor(0.8549)
loss: CE learning rate: 0.000964
training loss: tensor(0.9491)
         GM acc on global data: 0.1713 length of data: 10000
save a model
-------------Round number:  3  -------------
loss: CE learning rate: 0.000946
training loss: tensor(0.5348)
loss: CE learning rate: 0.000946
training loss: tensor(0.4794)
loss: CE learning rate: 0.000946
training loss: tensor(0.5835)
loss: CE learning rate: 0.000946
training loss: tensor(0.6936)
loss: CE learning rate: 0.000946
training loss: tensor(0.6287)
loss: CE learning rate: 0.000946
training loss: tensor(0.5448)
loss: CE learning rate: 0.000946
training loss: tensor(0.5959)
loss: CE learning rate: 0.000946
training loss: tensor(0.6522)
loss: CE learning rate: 0.000946
training loss: tensor(0.6380)
loss: CE learning rate: 0.000946
training loss: tensor(0.5233)
         GM acc on global data: 0.2505 length of data: 10000
save a model
-------------Round number:  4  -------------
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5959)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5847)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5496)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5294)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.6153)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.6001)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.4763)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5450)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.6774)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5866)
         GM acc on global data: 0.2659 length of data: 10000
save a model
-------------Round number:  5  -------------
loss: CE learning rate: 0.00091
training loss: tensor(0.4821)
loss: CE learning rate: 0.00091
training loss: tensor(0.7504)
loss: CE learning rate: 0.00091
training loss: tensor(0.4836)
loss: CE learning rate: 0.00091
training loss: tensor(0.5161)
loss: CE learning rate: 0.00091
training loss: tensor(0.5591)
loss: CE learning rate: 0.00091
training loss: tensor(0.5813)
loss: CE learning rate: 0.00091
training loss: tensor(0.4948)
loss: CE learning rate: 0.00091
training loss: tensor(0.4490)
loss: CE learning rate: 0.00091
training loss: tensor(0.5212)
loss: CE learning rate: 0.00091
training loss: tensor(0.5092)
         GM acc on global data: 0.2841 length of data: 10000
save a model
-------------Round number:  6  -------------
loss: CE learning rate: 0.000892
training loss: tensor(0.3853)
loss: CE learning rate: 0.000892
training loss: tensor(0.4479)
loss: CE learning rate: 0.000892
training loss: tensor(0.5417)
loss: CE learning rate: 0.000892
training loss: tensor(0.4224)
loss: CE learning rate: 0.000892
training loss: tensor(0.6368)
loss: CE learning rate: 0.000892
training loss: tensor(0.4796)
loss: CE learning rate: 0.000892
training loss: tensor(0.3893)
loss: CE learning rate: 0.000892
training loss: tensor(0.4096)
loss: CE learning rate: 0.000892
training loss: tensor(0.5537)
loss: CE learning rate: 0.000892
training loss: tensor(0.5475)
         GM acc on global data: 0.3296 length of data: 10000
save a model
-------------Round number:  7  -------------
loss: CE learning rate: 0.000874
training loss: tensor(0.3973)
loss: CE learning rate: 0.000874
training loss: tensor(0.7974)
loss: CE learning rate: 0.000874
training loss: tensor(0.7401)
loss: CE learning rate: 0.000874
training loss: tensor(0.5734)
loss: CE learning rate: 0.000874
training loss: tensor(0.9199)
loss: CE learning rate: 0.000874
training loss: tensor(0.3917)
loss: CE learning rate: 0.000874
training loss: tensor(0.4815)
loss: CE learning rate: 0.000874
training loss: tensor(0.4748)
loss: CE learning rate: 0.000874
training loss: tensor(0.4349)
loss: CE learning rate: 0.000874
training loss: tensor(0.4657)
         GM acc on global data: 0.3081 length of data: 10000
-------------Round number:  8  -------------
loss: CE learning rate: 0.000856
training loss: tensor(0.3090)
loss: CE learning rate: 0.000856
training loss: tensor(0.4913)
loss: CE learning rate: 0.000856
training loss: tensor(0.5020)
loss: CE learning rate: 0.000856
training loss: tensor(0.5071)
loss: CE learning rate: 0.000856
training loss: tensor(0.2890)
loss: CE learning rate: 0.000856
training loss: tensor(0.3936)
loss: CE learning rate: 0.000856
training loss: tensor(0.2968)
loss: CE learning rate: 0.000856
training loss: tensor(0.4065)
loss: CE learning rate: 0.000856
training loss: tensor(0.5486)
loss: CE learning rate: 0.000856
training loss: tensor(0.3922)
         GM acc on global data: 0.3104 length of data: 10000
-------------Round number:  9  -------------
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.4510)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.5178)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.3143)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.8775)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.5876)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.4416)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.5536)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.3647)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.5077)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.3192)
         GM acc on global data: 0.3021 length of data: 10000
-------------Round number:  10  -------------
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.4231)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.4085)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3579)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3072)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3047)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.4086)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.7409)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3711)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.5285)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.5160)
         GM acc on global data: 0.2996 length of data: 10000
-------------Round number:  11  -------------
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.2151)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.6353)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3386)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3720)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3068)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3603)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.4110)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3351)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.4165)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.2624)
         GM acc on global data: 0.3514 length of data: 10000
save a model
-------------Round number:  12  -------------
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.2653)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.2717)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.4209)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.2964)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.6697)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.4391)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.4863)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.3594)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.3909)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.7080)
         GM acc on global data: 0.4026 length of data: 10000
save a model
-------------Round number:  13  -------------
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.2272)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3061)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3695)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.2258)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3859)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.4132)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.4688)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3813)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3540)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.4040)
         GM acc on global data: 0.4439 length of data: 10000
save a model
-------------Round number:  14  -------------
loss: CE learning rate: 0.000748
training loss: tensor(0.3856)
loss: CE learning rate: 0.000748
training loss: tensor(0.3298)
loss: CE learning rate: 0.000748
training loss: tensor(0.3519)
loss: CE learning rate: 0.000748
training loss: tensor(0.3114)
loss: CE learning rate: 0.000748
training loss: tensor(0.2499)
loss: CE learning rate: 0.000748
training loss: tensor(0.3773)
loss: CE learning rate: 0.000748
training loss: tensor(0.3911)
loss: CE learning rate: 0.000748
training loss: tensor(0.4827)
loss: CE learning rate: 0.000748
training loss: tensor(0.3945)
loss: CE learning rate: 0.000748
training loss: tensor(0.3674)
         GM acc on global data: 0.3909 length of data: 10000
-------------Round number:  15  -------------
loss: CE learning rate: 0.00073
training loss: tensor(0.3228)
loss: CE learning rate: 0.00073
training loss: tensor(0.3637)
loss: CE learning rate: 0.00073
training loss: tensor(0.3071)
loss: CE learning rate: 0.00073
training loss: tensor(0.2550)
loss: CE learning rate: 0.00073
training loss: tensor(0.2203)
loss: CE learning rate: 0.00073
training loss: tensor(0.5541)
loss: CE learning rate: 0.00073
training loss: tensor(0.3066)
loss: CE learning rate: 0.00073
training loss: tensor(0.1787)
loss: CE learning rate: 0.00073
training loss: tensor(0.4834)
loss: CE learning rate: 0.00073
training loss: tensor(0.3262)
         GM acc on global data: 0.3766 length of data: 10000
-------------Round number:  16  -------------
loss: CE learning rate: 0.000712
training loss: tensor(0.5783)
loss: CE learning rate: 0.000712
training loss: tensor(0.4226)
loss: CE learning rate: 0.000712
training loss: tensor(0.3836)
loss: CE learning rate: 0.000712
training loss: tensor(0.4311)
loss: CE learning rate: 0.000712
training loss: tensor(0.3755)
loss: CE learning rate: 0.000712
training loss: tensor(0.3290)
loss: CE learning rate: 0.000712
training loss: tensor(0.4249)
loss: CE learning rate: 0.000712
training loss: tensor(0.7079)
loss: CE learning rate: 0.000712
training loss: tensor(0.4580)
loss: CE learning rate: 0.000712
training loss: tensor(0.3713)
         GM acc on global data: 0.4305 length of data: 10000
-------------Round number:  17  -------------
loss: CE learning rate: 0.000694
training loss: tensor(0.2355)
loss: CE learning rate: 0.000694
training loss: tensor(0.3314)
loss: CE learning rate: 0.000694
training loss: tensor(0.1804)
loss: CE learning rate: 0.000694
training loss: tensor(0.4056)
loss: CE learning rate: 0.000694
training loss: tensor(0.3310)
loss: CE learning rate: 0.000694
training loss: tensor(0.3976)
loss: CE learning rate: 0.000694
training loss: tensor(0.3943)
loss: CE learning rate: 0.000694
training loss: tensor(0.3027)
loss: CE learning rate: 0.000694
training loss: tensor(0.2849)
loss: CE learning rate: 0.000694
training loss: tensor(0.1932)
         GM acc on global data: 0.4523 length of data: 10000
save a model
-------------Round number:  18  -------------
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3134)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.2021)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.2909)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3770)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3096)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3193)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3236)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.2963)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3009)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3513)
         GM acc on global data: 0.4424 length of data: 10000
-------------Round number:  19  -------------
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3449)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.2644)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3329)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.2862)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3640)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.4507)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.2741)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3132)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.2826)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3577)
         GM acc on global data: 0.4666 length of data: 10000
save a model
-------------Round number:  20  -------------
loss: CE learning rate: 0.00064
training loss: tensor(0.2676)
loss: CE learning rate: 0.00064
training loss: tensor(0.3697)
loss: CE learning rate: 0.00064
training loss: tensor(0.2274)
loss: CE learning rate: 0.00064
training loss: tensor(0.2636)
loss: CE learning rate: 0.00064
training loss: tensor(0.2847)
loss: CE learning rate: 0.00064
training loss: tensor(0.1796)
loss: CE learning rate: 0.00064
training loss: tensor(0.1851)
loss: CE learning rate: 0.00064
training loss: tensor(0.2768)
loss: CE learning rate: 0.00064
training loss: tensor(0.2849)
loss: CE learning rate: 0.00064
training loss: tensor(0.8706)
         GM acc on global data: 0.4499 length of data: 10000
-------------Round number:  21  -------------
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.4291)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.4820)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.2395)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.2837)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.3711)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.1780)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.3633)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.3578)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.3186)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.4556)
         GM acc on global data: 0.4639 length of data: 10000
-------------Round number:  22  -------------
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.1944)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.6475)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2915)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2292)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.1603)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.1496)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.3908)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2354)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2964)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.1528)
         GM acc on global data: 0.4509 length of data: 10000
-------------Round number:  23  -------------
loss: CE learning rate: 0.000586
training loss: tensor(0.7503)
loss: CE learning rate: 0.000586
training loss: tensor(0.3466)
loss: CE learning rate: 0.000586
training loss: tensor(0.2443)
loss: CE learning rate: 0.000586
training loss: tensor(0.9397)
loss: CE learning rate: 0.000586
training loss: tensor(0.5383)
loss: CE learning rate: 0.000586
training loss: tensor(0.0934)
loss: CE learning rate: 0.000586
training loss: tensor(0.3941)
loss: CE learning rate: 0.000586
training loss: tensor(0.2757)
loss: CE learning rate: 0.000586
training loss: tensor(0.3221)
loss: CE learning rate: 0.000586
training loss: tensor(0.2389)
         GM acc on global data: 0.4827 length of data: 10000
save a model
-------------Round number:  24  -------------
loss: CE learning rate: 0.000568
training loss: tensor(0.3259)
loss: CE learning rate: 0.000568
training loss: tensor(0.2286)
loss: CE learning rate: 0.000568
training loss: tensor(0.0725)
loss: CE learning rate: 0.000568
training loss: tensor(0.3330)
loss: CE learning rate: 0.000568
training loss: tensor(0.3105)
loss: CE learning rate: 0.000568
training loss: tensor(0.2642)
loss: CE learning rate: 0.000568
training loss: tensor(0.6234)
loss: CE learning rate: 0.000568
training loss: tensor(0.1900)
loss: CE learning rate: 0.000568
training loss: tensor(0.2446)
loss: CE learning rate: 0.000568
training loss: tensor(0.3764)
         GM acc on global data: 0.462 length of data: 10000
-------------Round number:  25  -------------
loss: CE learning rate: 0.00055
training loss: tensor(0.4223)
loss: CE learning rate: 0.00055
training loss: tensor(0.2736)
loss: CE learning rate: 0.00055
training loss: tensor(0.2780)
loss: CE learning rate: 0.00055
training loss: tensor(0.3522)
loss: CE learning rate: 0.00055
training loss: tensor(0.3862)
loss: CE learning rate: 0.00055
training loss: tensor(0.3398)
loss: CE learning rate: 0.00055
training loss: tensor(0.2813)
loss: CE learning rate: 0.00055
training loss: tensor(0.4528)
loss: CE learning rate: 0.00055
training loss: tensor(0.2756)
loss: CE learning rate: 0.00055
training loss: tensor(0.2761)
         GM acc on global data: 0.4713 length of data: 10000
-------------Round number:  26  -------------
loss: CE learning rate: 0.000532
training loss: tensor(0.2806)
loss: CE learning rate: 0.000532
training loss: tensor(0.2505)
loss: CE learning rate: 0.000532
training loss: tensor(0.2302)
loss: CE learning rate: 0.000532
training loss: tensor(0.3367)
loss: CE learning rate: 0.000532
training loss: tensor(0.3725)
loss: CE learning rate: 0.000532
training loss: tensor(0.4279)
loss: CE learning rate: 0.000532
training loss: tensor(0.3534)
loss: CE learning rate: 0.000532
training loss: tensor(0.3548)
loss: CE learning rate: 0.000532
training loss: tensor(0.1624)
loss: CE learning rate: 0.000532
training loss: tensor(0.2387)
         GM acc on global data: 0.4504 length of data: 10000
-------------Round number:  27  -------------
loss: CE learning rate: 0.000514
training loss: tensor(0.3606)
loss: CE learning rate: 0.000514
training loss: tensor(0.6360)
loss: CE learning rate: 0.000514
training loss: tensor(0.3305)
loss: CE learning rate: 0.000514
training loss: tensor(0.4467)
loss: CE learning rate: 0.000514
training loss: tensor(0.5114)
loss: CE learning rate: 0.000514
training loss: tensor(0.2267)
loss: CE learning rate: 0.000514
training loss: tensor(0.3325)
loss: CE learning rate: 0.000514
training loss: tensor(0.2276)
loss: CE learning rate: 0.000514
training loss: tensor(0.3582)
loss: CE learning rate: 0.000514
training loss: tensor(0.2574)
         GM acc on global data: 0.483 length of data: 10000
save a model
-------------Round number:  28  -------------
loss: CE learning rate: 0.000496
training loss: tensor(0.2736)
loss: CE learning rate: 0.000496
training loss: tensor(0.1742)
loss: CE learning rate: 0.000496
training loss: tensor(0.2181)
loss: CE learning rate: 0.000496
training loss: tensor(0.2156)
loss: CE learning rate: 0.000496
training loss: tensor(0.3543)
loss: CE learning rate: 0.000496
training loss: tensor(0.2442)
loss: CE learning rate: 0.000496
training loss: tensor(0.2536)
loss: CE learning rate: 0.000496
training loss: tensor(0.2355)
loss: CE learning rate: 0.000496
training loss: tensor(0.3491)
loss: CE learning rate: 0.000496
training loss: tensor(0.1367)
         GM acc on global data: 0.4889 length of data: 10000
save a model
-------------Round number:  29  -------------
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2615)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2205)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2796)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.3642)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2886)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.1554)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.3240)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2550)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.4035)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.1752)
         GM acc on global data: 0.5159 length of data: 10000
save a model
-------------Round number:  30  -------------
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2345)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2923)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2248)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2416)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.1268)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2654)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.3351)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.5810)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2746)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2900)
         GM acc on global data: 0.4877 length of data: 10000
-------------Round number:  31  -------------
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2167)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.1055)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2393)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.3420)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2412)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.1820)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.0834)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.1077)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.5876)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2148)
         GM acc on global data: 0.4891 length of data: 10000
-------------Round number:  32  -------------
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.2414)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.4194)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.1628)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.4148)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.2199)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.0937)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.1848)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.4951)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.3028)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.4082)
         GM acc on global data: 0.502 length of data: 10000
-------------Round number:  33  -------------
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.1232)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2413)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.1621)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2012)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.5851)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.0931)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2802)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.4132)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.1962)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2526)
         GM acc on global data: 0.4814 length of data: 10000
-------------Round number:  34  -------------
loss: CE learning rate: 0.000388
training loss: tensor(0.1559)
loss: CE learning rate: 0.000388
training loss: tensor(0.2131)
loss: CE learning rate: 0.000388
training loss: tensor(0.3870)
loss: CE learning rate: 0.000388
training loss: tensor(0.5383)
loss: CE learning rate: 0.000388
training loss: tensor(1.4790)
loss: CE learning rate: 0.000388
training loss: tensor(0.3637)
loss: CE learning rate: 0.000388
training loss: tensor(0.2931)
loss: CE learning rate: 0.000388
training loss: tensor(0.2511)
loss: CE learning rate: 0.000388
training loss: tensor(0.3903)
loss: CE learning rate: 0.000388
training loss: tensor(0.2927)
         GM acc on global data: 0.5146 length of data: 10000
-------------Round number:  35  -------------
loss: CE learning rate: 0.00037
training loss: tensor(0.3592)
loss: CE learning rate: 0.00037
training loss: tensor(0.0729)
loss: CE learning rate: 0.00037
training loss: tensor(0.2057)
loss: CE learning rate: 0.00037
training loss: tensor(0.1627)
loss: CE learning rate: 0.00037
training loss: tensor(0.1148)
loss: CE learning rate: 0.00037
training loss: tensor(0.1800)
loss: CE learning rate: 0.00037
training loss: tensor(0.1610)
loss: CE learning rate: 0.00037
training loss: tensor(0.3712)
loss: CE learning rate: 0.00037
training loss: tensor(0.2710)
loss: CE learning rate: 0.00037
training loss: tensor(0.2719)
         GM acc on global data: 0.5217 length of data: 10000
save a model
-------------Round number:  36  -------------
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.2117)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.2548)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.1163)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.1010)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.4086)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.3636)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.3776)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.6797)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.2389)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.1324)
         GM acc on global data: 0.5291 length of data: 10000
save a model
-------------Round number:  37  -------------
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.3474)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.8200)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2613)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.6652)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2698)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.1415)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2659)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.1513)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2436)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2011)
         GM acc on global data: 0.5199 length of data: 10000
-------------Round number:  38  -------------
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.2783)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.2647)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.8510)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.4027)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.4354)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.3204)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.2407)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.1438)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.2803)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.3575)
         GM acc on global data: 0.5359 length of data: 10000
save a model
-------------Round number:  39  -------------
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2056)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2703)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2351)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.1999)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2408)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2993)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.1718)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2445)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.1267)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.7579)
         GM acc on global data: 0.5215 length of data: 10000
-------------Round number:  40  -------------
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.4370)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.2791)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.3534)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.3447)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.4721)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.5473)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.2923)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.3602)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.3247)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.2315)
         GM acc on global data: 0.5304 length of data: 10000
-------------Round number:  41  -------------
loss: CE learning rate: 0.000262
training loss: tensor(0.2191)
loss: CE learning rate: 0.000262
training loss: tensor(0.4997)
loss: CE learning rate: 0.000262
training loss: tensor(0.0860)
loss: CE learning rate: 0.000262
training loss: tensor(0.2390)
loss: CE learning rate: 0.000262
training loss: tensor(0.3357)
loss: CE learning rate: 0.000262
training loss: tensor(0.8733)
loss: CE learning rate: 0.000262
training loss: tensor(0.2663)
loss: CE learning rate: 0.000262
training loss: tensor(0.1366)
loss: CE learning rate: 0.000262
training loss: tensor(0.0861)
loss: CE learning rate: 0.000262
training loss: tensor(0.3718)
         GM acc on global data: 0.5382 length of data: 10000
save a model
-------------Round number:  42  -------------
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.3211)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.2600)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.1907)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.2352)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.2591)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.2329)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.1464)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.2618)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.1405)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.0600)
         GM acc on global data: 0.5208 length of data: 10000
-------------Round number:  43  -------------
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2749)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.0897)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2325)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.0580)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2634)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2354)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.1934)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2489)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.5289)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.4011)
         GM acc on global data: 0.5257 length of data: 10000
-------------Round number:  44  -------------
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.1264)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.4274)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.0819)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.3338)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.1870)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.3507)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.3337)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.5389)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.2207)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.4201)
         GM acc on global data: 0.5341 length of data: 10000
-------------Round number:  45  -------------
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.0343)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.4820)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.1421)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.3570)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.0386)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.4831)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.6600)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.3606)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.0602)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.2226)
         GM acc on global data: 0.535 length of data: 10000
-------------Round number:  46  -------------
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.0746)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.0834)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.0117)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.0727)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.2302)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.3277)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.1808)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.1699)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.1788)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.1598)
         GM acc on global data: 0.5196 length of data: 10000
-------------Round number:  47  -------------
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.1187)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.4109)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.3869)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.1787)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.2549)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.4676)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.4338)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.1690)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.2072)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.3652)
         GM acc on global data: 0.5212 length of data: 10000
-------------Round number:  48  -------------
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2642)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2229)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.0398)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2349)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.4466)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.3975)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.1062)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.3059)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2449)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.1700)
         GM acc on global data: 0.5234 length of data: 10000
-------------Round number:  49  -------------
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.2454)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.3812)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.4177)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.2047)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.2902)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.2971)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.4217)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.0491)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.1477)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.3996)
         GM acc on global data: 0.5257 length of data: 10000
-------------Round number:  50  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3591)
loss: CE learning rate: 0.0001
training loss: tensor(0.2482)
loss: CE learning rate: 0.0001
training loss: tensor(0.2243)
loss: CE learning rate: 0.0001
training loss: tensor(0.2839)
loss: CE learning rate: 0.0001
training loss: tensor(0.1690)
loss: CE learning rate: 0.0001
training loss: tensor(0.0955)
loss: CE learning rate: 0.0001
training loss: tensor(0.2668)
loss: CE learning rate: 0.0001
training loss: tensor(0.0719)
loss: CE learning rate: 0.0001
training loss: tensor(0.1719)
loss: CE learning rate: 0.0001
training loss: tensor(0.4677)
         GM acc on global data: 0.524 length of data: 10000
-------------Round number:  51  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0226)
loss: CE learning rate: 0.0001
training loss: tensor(0.4349)
loss: CE learning rate: 0.0001
training loss: tensor(0.3013)
loss: CE learning rate: 0.0001
training loss: tensor(0.2313)
loss: CE learning rate: 0.0001
training loss: tensor(0.5705)
loss: CE learning rate: 0.0001
training loss: tensor(0.7159)
loss: CE learning rate: 0.0001
training loss: tensor(0.1693)
loss: CE learning rate: 0.0001
training loss: tensor(0.3653)
loss: CE learning rate: 0.0001
training loss: tensor(0.2481)
loss: CE learning rate: 0.0001
training loss: tensor(0.1379)
         GM acc on global data: 0.5221 length of data: 10000
-------------Round number:  52  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1462)
loss: CE learning rate: 0.0001
training loss: tensor(0.1699)
loss: CE learning rate: 0.0001
training loss: tensor(0.1669)
loss: CE learning rate: 0.0001
training loss: tensor(0.4172)
loss: CE learning rate: 0.0001
training loss: tensor(0.0436)
loss: CE learning rate: 0.0001
training loss: tensor(0.1632)
loss: CE learning rate: 0.0001
training loss: tensor(0.0131)
loss: CE learning rate: 0.0001
training loss: tensor(0.2851)
loss: CE learning rate: 0.0001
training loss: tensor(0.3752)
loss: CE learning rate: 0.0001
training loss: tensor(0.2409)
         GM acc on global data: 0.5066 length of data: 10000
-------------Round number:  53  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2430)
loss: CE learning rate: 0.0001
training loss: tensor(0.9024)
loss: CE learning rate: 0.0001
training loss: tensor(0.4551)
loss: CE learning rate: 0.0001
training loss: tensor(0.4169)
loss: CE learning rate: 0.0001
training loss: tensor(0.0078)
loss: CE learning rate: 0.0001
training loss: tensor(1.2034)
loss: CE learning rate: 0.0001
training loss: tensor(1.1996)
loss: CE learning rate: 0.0001
training loss: tensor(0.1052)
loss: CE learning rate: 0.0001
training loss: tensor(0.2520)
loss: CE learning rate: 0.0001
training loss: tensor(0.5009)
         GM acc on global data: 0.5125 length of data: 10000
-------------Round number:  54  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3049)
loss: CE learning rate: 0.0001
training loss: tensor(0.0913)
loss: CE learning rate: 0.0001
training loss: tensor(0.3155)
loss: CE learning rate: 0.0001
training loss: tensor(0.3367)
loss: CE learning rate: 0.0001
training loss: tensor(0.2010)
loss: CE learning rate: 0.0001
training loss: tensor(0.2713)
loss: CE learning rate: 0.0001
training loss: tensor(0.3751)
loss: CE learning rate: 0.0001
training loss: tensor(0.1035)
loss: CE learning rate: 0.0001
training loss: tensor(0.1492)
loss: CE learning rate: 0.0001
training loss: tensor(0.0278)
         GM acc on global data: 0.5019 length of data: 10000
-------------Round number:  55  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4356)
loss: CE learning rate: 0.0001
training loss: tensor(0.4801)
loss: CE learning rate: 0.0001
training loss: tensor(0.1810)
loss: CE learning rate: 0.0001
training loss: tensor(0.1392)
loss: CE learning rate: 0.0001
training loss: tensor(0.2112)
loss: CE learning rate: 0.0001
training loss: tensor(0.0684)
loss: CE learning rate: 0.0001
training loss: tensor(0.2285)
loss: CE learning rate: 0.0001
training loss: tensor(0.2161)
loss: CE learning rate: 0.0001
training loss: tensor(0.0526)
loss: CE learning rate: 0.0001
training loss: tensor(0.8411)
         GM acc on global data: 0.5142 length of data: 10000
-------------Round number:  56  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3459)
loss: CE learning rate: 0.0001
training loss: tensor(0.3716)
loss: CE learning rate: 0.0001
training loss: tensor(0.6068)
loss: CE learning rate: 0.0001
training loss: tensor(0.9388)
loss: CE learning rate: 0.0001
training loss: tensor(0.2650)
loss: CE learning rate: 0.0001
training loss: tensor(0.2469)
loss: CE learning rate: 0.0001
training loss: tensor(0.1170)
loss: CE learning rate: 0.0001
training loss: tensor(0.1807)
loss: CE learning rate: 0.0001
training loss: tensor(0.1493)
loss: CE learning rate: 0.0001
training loss: tensor(0.0686)
         GM acc on global data: 0.5284 length of data: 10000
-------------Round number:  57  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1443)
loss: CE learning rate: 0.0001
training loss: tensor(0.0366)
loss: CE learning rate: 0.0001
training loss: tensor(0.2794)
loss: CE learning rate: 0.0001
training loss: tensor(0.3132)
loss: CE learning rate: 0.0001
training loss: tensor(0.1720)
loss: CE learning rate: 0.0001
training loss: tensor(0.0908)
loss: CE learning rate: 0.0001
training loss: tensor(0.1162)
loss: CE learning rate: 0.0001
training loss: tensor(0.1555)
loss: CE learning rate: 0.0001
training loss: tensor(0.3726)
loss: CE learning rate: 0.0001
training loss: tensor(0.1730)
         GM acc on global data: 0.5257 length of data: 10000
-------------Round number:  58  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3440)
loss: CE learning rate: 0.0001
training loss: tensor(0.3946)
loss: CE learning rate: 0.0001
training loss: tensor(0.1370)
loss: CE learning rate: 0.0001
training loss: tensor(0.2099)
loss: CE learning rate: 0.0001
training loss: tensor(0.0879)
loss: CE learning rate: 0.0001
training loss: tensor(0.2610)
loss: CE learning rate: 0.0001
training loss: tensor(0.1011)
loss: CE learning rate: 0.0001
training loss: tensor(0.0901)
loss: CE learning rate: 0.0001
training loss: tensor(0.3301)
loss: CE learning rate: 0.0001
training loss: tensor(0.2804)
         GM acc on global data: 0.5176 length of data: 10000
-------------Round number:  59  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2128)
loss: CE learning rate: 0.0001
training loss: tensor(0.1979)
loss: CE learning rate: 0.0001
training loss: tensor(0.1117)
loss: CE learning rate: 0.0001
training loss: tensor(0.2082)
loss: CE learning rate: 0.0001
training loss: tensor(0.3281)
loss: CE learning rate: 0.0001
training loss: tensor(0.1356)
loss: CE learning rate: 0.0001
training loss: tensor(0.3084)
loss: CE learning rate: 0.0001
training loss: tensor(0.4240)
loss: CE learning rate: 0.0001
training loss: tensor(0.1204)
loss: CE learning rate: 0.0001
training loss: tensor(0.3946)
         GM acc on global data: 0.5245 length of data: 10000
-------------Round number:  60  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4984)
loss: CE learning rate: 0.0001
training loss: tensor(0.2624)
loss: CE learning rate: 0.0001
training loss: tensor(0.2250)
loss: CE learning rate: 0.0001
training loss: tensor(0.1049)
loss: CE learning rate: 0.0001
training loss: tensor(0.2177)
loss: CE learning rate: 0.0001
training loss: tensor(0.2674)
loss: CE learning rate: 0.0001
training loss: tensor(0.2307)
loss: CE learning rate: 0.0001
training loss: tensor(0.2530)
loss: CE learning rate: 0.0001
training loss: tensor(0.2022)
loss: CE learning rate: 0.0001
training loss: tensor(0.1860)
         GM acc on global data: 0.5176 length of data: 10000
-------------Round number:  61  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1694)
loss: CE learning rate: 0.0001
training loss: tensor(0.2968)
loss: CE learning rate: 0.0001
training loss: tensor(0.1541)
loss: CE learning rate: 0.0001
training loss: tensor(0.0769)
loss: CE learning rate: 0.0001
training loss: tensor(0.1459)
loss: CE learning rate: 0.0001
training loss: tensor(0.1837)
loss: CE learning rate: 0.0001
training loss: tensor(0.3893)
loss: CE learning rate: 0.0001
training loss: tensor(0.1004)
loss: CE learning rate: 0.0001
training loss: tensor(0.1673)
loss: CE learning rate: 0.0001
training loss: tensor(0.1516)
         GM acc on global data: 0.5239 length of data: 10000
-------------Round number:  62  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1744)
loss: CE learning rate: 0.0001
training loss: tensor(0.1752)
loss: CE learning rate: 0.0001
training loss: tensor(0.0980)
loss: CE learning rate: 0.0001
training loss: tensor(0.1584)
loss: CE learning rate: 0.0001
training loss: tensor(0.0395)
loss: CE learning rate: 0.0001
training loss: tensor(0.2699)
loss: CE learning rate: 0.0001
training loss: tensor(0.4485)
loss: CE learning rate: 0.0001
training loss: tensor(0.2023)
loss: CE learning rate: 0.0001
training loss: tensor(0.1373)
loss: CE learning rate: 0.0001
training loss: tensor(0.0740)
         GM acc on global data: 0.5163 length of data: 10000
-------------Round number:  63  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2972)
loss: CE learning rate: 0.0001
training loss: tensor(0.1079)
loss: CE learning rate: 0.0001
training loss: tensor(0.1221)
loss: CE learning rate: 0.0001
training loss: tensor(0.0283)
loss: CE learning rate: 0.0001
training loss: tensor(0.0861)
loss: CE learning rate: 0.0001
training loss: tensor(0.1355)
loss: CE learning rate: 0.0001
training loss: tensor(0.0649)
loss: CE learning rate: 0.0001
training loss: tensor(0.1099)
loss: CE learning rate: 0.0001
training loss: tensor(0.2743)
loss: CE learning rate: 0.0001
training loss: tensor(0.3374)
         GM acc on global data: 0.509 length of data: 10000
-------------Round number:  64  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0338)
loss: CE learning rate: 0.0001
training loss: tensor(0.0302)
loss: CE learning rate: 0.0001
training loss: tensor(0.3477)
loss: CE learning rate: 0.0001
training loss: tensor(0.3554)
loss: CE learning rate: 0.0001
training loss: tensor(0.4939)
loss: CE learning rate: 0.0001
training loss: tensor(0.1903)
loss: CE learning rate: 0.0001
training loss: tensor(0.0243)
loss: CE learning rate: 0.0001
training loss: tensor(0.1317)
loss: CE learning rate: 0.0001
training loss: tensor(0.1702)
loss: CE learning rate: 0.0001
training loss: tensor(1.0839)
         GM acc on global data: 0.5152 length of data: 10000
-------------Round number:  65  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0125)
loss: CE learning rate: 0.0001
training loss: tensor(0.2057)
loss: CE learning rate: 0.0001
training loss: tensor(0.0886)
loss: CE learning rate: 0.0001
training loss: tensor(0.3427)
loss: CE learning rate: 0.0001
training loss: tensor(0.1403)
loss: CE learning rate: 0.0001
training loss: tensor(0.2730)
loss: CE learning rate: 0.0001
training loss: tensor(0.0543)
loss: CE learning rate: 0.0001
training loss: tensor(0.1868)
loss: CE learning rate: 0.0001
training loss: tensor(0.2047)
loss: CE learning rate: 0.0001
training loss: tensor(0.0709)
         GM acc on global data: 0.516 length of data: 10000
-------------Round number:  66  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1041)
loss: CE learning rate: 0.0001
training loss: tensor(0.2351)
loss: CE learning rate: 0.0001
training loss: tensor(0.2015)
loss: CE learning rate: 0.0001
training loss: tensor(0.2352)
loss: CE learning rate: 0.0001
training loss: tensor(0.1442)
loss: CE learning rate: 0.0001
training loss: tensor(0.3093)
loss: CE learning rate: 0.0001
training loss: tensor(0.0720)
loss: CE learning rate: 0.0001
training loss: tensor(0.1081)
loss: CE learning rate: 0.0001
training loss: tensor(0.4268)
loss: CE learning rate: 0.0001
training loss: tensor(0.7650)
         GM acc on global data: 0.5235 length of data: 10000
-------------Round number:  67  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0293)
loss: CE learning rate: 0.0001
training loss: tensor(0.0776)
loss: CE learning rate: 0.0001
training loss: tensor(0.1655)
loss: CE learning rate: 0.0001
training loss: tensor(0.1649)
loss: CE learning rate: 0.0001
training loss: tensor(0.5141)
loss: CE learning rate: 0.0001
training loss: tensor(0.0331)
loss: CE learning rate: 0.0001
training loss: tensor(0.2369)
loss: CE learning rate: 0.0001
training loss: tensor(0.0848)
loss: CE learning rate: 0.0001
training loss: tensor(0.1273)
loss: CE learning rate: 0.0001
training loss: tensor(0.2693)
         GM acc on global data: 0.5198 length of data: 10000
-------------Round number:  68  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0173)
loss: CE learning rate: 0.0001
training loss: tensor(0.1871)
loss: CE learning rate: 0.0001
training loss: tensor(0.2110)
loss: CE learning rate: 0.0001
training loss: tensor(0.1842)
loss: CE learning rate: 0.0001
training loss: tensor(0.2270)
loss: CE learning rate: 0.0001
training loss: tensor(0.0074)
loss: CE learning rate: 0.0001
training loss: tensor(0.1098)
loss: CE learning rate: 0.0001
training loss: tensor(0.1670)
loss: CE learning rate: 0.0001
training loss: tensor(0.2229)
loss: CE learning rate: 0.0001
training loss: tensor(0.0809)
         GM acc on global data: 0.5173 length of data: 10000
-------------Round number:  69  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1737)
loss: CE learning rate: 0.0001
training loss: tensor(0.1148)
loss: CE learning rate: 0.0001
training loss: tensor(0.4756)
loss: CE learning rate: 0.0001
training loss: tensor(0.1391)
loss: CE learning rate: 0.0001
training loss: tensor(0.0618)
loss: CE learning rate: 0.0001
training loss: tensor(0.0699)
loss: CE learning rate: 0.0001
training loss: tensor(0.5611)
loss: CE learning rate: 0.0001
training loss: tensor(0.2206)
loss: CE learning rate: 0.0001
training loss: tensor(0.1340)
loss: CE learning rate: 0.0001
training loss: tensor(0.3631)
         GM acc on global data: 0.5207 length of data: 10000
-------------Round number:  70  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1869)
loss: CE learning rate: 0.0001
training loss: tensor(0.3855)
loss: CE learning rate: 0.0001
training loss: tensor(0.5618)
loss: CE learning rate: 0.0001
training loss: tensor(0.9195)
loss: CE learning rate: 0.0001
training loss: tensor(0.3284)
loss: CE learning rate: 0.0001
training loss: tensor(0.0081)
loss: CE learning rate: 0.0001
training loss: tensor(0.0792)
loss: CE learning rate: 0.0001
training loss: tensor(0.1244)
loss: CE learning rate: 0.0001
training loss: tensor(0.1060)
loss: CE learning rate: 0.0001
training loss: tensor(0.0239)
         GM acc on global data: 0.5111 length of data: 10000
-------------Round number:  71  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2661)
loss: CE learning rate: 0.0001
training loss: tensor(0.1599)
loss: CE learning rate: 0.0001
training loss: tensor(0.0744)
loss: CE learning rate: 0.0001
training loss: tensor(0.0741)
loss: CE learning rate: 0.0001
training loss: tensor(0.1246)
loss: CE learning rate: 0.0001
training loss: tensor(0.2333)
loss: CE learning rate: 0.0001
training loss: tensor(0.3783)
loss: CE learning rate: 0.0001
training loss: tensor(0.1557)
loss: CE learning rate: 0.0001
training loss: tensor(0.0978)
loss: CE learning rate: 0.0001
training loss: tensor(0.0142)
         GM acc on global data: 0.508 length of data: 10000
-------------Round number:  72  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0761)
loss: CE learning rate: 0.0001
training loss: tensor(0.0914)
loss: CE learning rate: 0.0001
training loss: tensor(0.2469)
loss: CE learning rate: 0.0001
training loss: tensor(0.1549)
loss: CE learning rate: 0.0001
training loss: tensor(0.0979)
loss: CE learning rate: 0.0001
training loss: tensor(0.0754)
loss: CE learning rate: 0.0001
training loss: tensor(0.3950)
loss: CE learning rate: 0.0001
training loss: tensor(0.1800)
loss: CE learning rate: 0.0001
training loss: tensor(0.1359)
loss: CE learning rate: 0.0001
training loss: tensor(0.0131)
         GM acc on global data: 0.5178 length of data: 10000
-------------Round number:  73  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.7711)
loss: CE learning rate: 0.0001
training loss: tensor(0.0887)
loss: CE learning rate: 0.0001
training loss: tensor(0.1151)
loss: CE learning rate: 0.0001
training loss: tensor(0.0436)
loss: CE learning rate: 0.0001
training loss: tensor(0.2006)
loss: CE learning rate: 0.0001
training loss: tensor(0.3329)
loss: CE learning rate: 0.0001
training loss: tensor(0.5985)
loss: CE learning rate: 0.0001
training loss: tensor(0.0352)
loss: CE learning rate: 0.0001
training loss: tensor(0.1152)
loss: CE learning rate: 0.0001
training loss: tensor(0.0710)
         GM acc on global data: 0.5118 length of data: 10000
-------------Round number:  74  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0886)
loss: CE learning rate: 0.0001
training loss: tensor(0.1201)
loss: CE learning rate: 0.0001
training loss: tensor(0.1198)
loss: CE learning rate: 0.0001
training loss: tensor(0.0706)
loss: CE learning rate: 0.0001
training loss: tensor(0.2589)
loss: CE learning rate: 0.0001
training loss: tensor(0.2276)
loss: CE learning rate: 0.0001
training loss: tensor(0.1955)
loss: CE learning rate: 0.0001
training loss: tensor(0.0375)
loss: CE learning rate: 0.0001
training loss: tensor(0.2011)
loss: CE learning rate: 0.0001
training loss: tensor(0.1227)
         GM acc on global data: 0.5187 length of data: 10000
-------------Round number:  75  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2493)
loss: CE learning rate: 0.0001
training loss: tensor(0.1219)
loss: CE learning rate: 0.0001
training loss: tensor(0.1165)
loss: CE learning rate: 0.0001
training loss: tensor(0.2645)
loss: CE learning rate: 0.0001
training loss: tensor(0.1343)
loss: CE learning rate: 0.0001
training loss: tensor(0.3672)
loss: CE learning rate: 0.0001
training loss: tensor(0.0075)
loss: CE learning rate: 0.0001
training loss: tensor(0.0092)
loss: CE learning rate: 0.0001
training loss: tensor(0.1665)
loss: CE learning rate: 0.0001
training loss: tensor(0.5347)
         GM acc on global data: 0.5179 length of data: 10000
-------------Round number:  76  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1247)
loss: CE learning rate: 0.0001
training loss: tensor(0.0046)
loss: CE learning rate: 0.0001
training loss: tensor(1.0083)
loss: CE learning rate: 0.0001
training loss: tensor(0.2996)
loss: CE learning rate: 0.0001
training loss: tensor(0.1226)
loss: CE learning rate: 0.0001
training loss: tensor(0.1489)
loss: CE learning rate: 0.0001
training loss: tensor(0.1438)
loss: CE learning rate: 0.0001
training loss: tensor(0.1916)
loss: CE learning rate: 0.0001
training loss: tensor(0.1253)
loss: CE learning rate: 0.0001
training loss: tensor(0.4570)
         GM acc on global data: 0.5108 length of data: 10000
-------------Round number:  77  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1760)
loss: CE learning rate: 0.0001
training loss: tensor(0.1322)
loss: CE learning rate: 0.0001
training loss: tensor(0.2241)
loss: CE learning rate: 0.0001
training loss: tensor(0.4104)
loss: CE learning rate: 0.0001
training loss: tensor(0.1078)
loss: CE learning rate: 0.0001
training loss: tensor(0.0881)
loss: CE learning rate: 0.0001
training loss: tensor(0.2014)
loss: CE learning rate: 0.0001
training loss: tensor(0.1317)
loss: CE learning rate: 0.0001
training loss: tensor(0.1741)
loss: CE learning rate: 0.0001
training loss: tensor(0.3312)
         GM acc on global data: 0.5146 length of data: 10000
-------------Round number:  78  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0299)
loss: CE learning rate: 0.0001
training loss: tensor(0.1923)
loss: CE learning rate: 0.0001
training loss: tensor(0.1119)
loss: CE learning rate: 0.0001
training loss: tensor(0.0777)
loss: CE learning rate: 0.0001
training loss: tensor(0.1958)
loss: CE learning rate: 0.0001
training loss: tensor(0.0197)
loss: CE learning rate: 0.0001
training loss: tensor(0.1155)
loss: CE learning rate: 0.0001
training loss: tensor(0.0894)
loss: CE learning rate: 0.0001
training loss: tensor(0.0954)
loss: CE learning rate: 0.0001
training loss: tensor(0.1929)
         GM acc on global data: 0.5198 length of data: 10000
-------------Round number:  79  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2921)
loss: CE learning rate: 0.0001
training loss: tensor(0.0881)
loss: CE learning rate: 0.0001
training loss: tensor(0.1026)
loss: CE learning rate: 0.0001
training loss: tensor(0.2863)
loss: CE learning rate: 0.0001
training loss: tensor(0.0836)
loss: CE learning rate: 0.0001
training loss: tensor(0.8252)
loss: CE learning rate: 0.0001
training loss: tensor(0.1316)
loss: CE learning rate: 0.0001
training loss: tensor(0.0923)
loss: CE learning rate: 0.0001
training loss: tensor(0.2589)
loss: CE learning rate: 0.0001
training loss: tensor(0.1945)
         GM acc on global data: 0.5199 length of data: 10000
-------------Round number:  80  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0033)
loss: CE learning rate: 0.0001
training loss: tensor(0.4210)
loss: CE learning rate: 0.0001
training loss: tensor(0.0451)
loss: CE learning rate: 0.0001
training loss: tensor(0.1387)
loss: CE learning rate: 0.0001
training loss: tensor(0.4774)
loss: CE learning rate: 0.0001
training loss: tensor(0.1057)
loss: CE learning rate: 0.0001
training loss: tensor(0.0864)
loss: CE learning rate: 0.0001
training loss: tensor(0.0519)
loss: CE learning rate: 0.0001
training loss: tensor(0.2006)
loss: CE learning rate: 0.0001
training loss: tensor(0.1256)
         GM acc on global data: 0.5084 length of data: 10000
-------------Round number:  81  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0226)
loss: CE learning rate: 0.0001
training loss: tensor(0.2078)
loss: CE learning rate: 0.0001
training loss: tensor(0.1730)
loss: CE learning rate: 0.0001
training loss: tensor(1.0294)
loss: CE learning rate: 0.0001
training loss: tensor(0.3077)
loss: CE learning rate: 0.0001
training loss: tensor(0.2478)
loss: CE learning rate: 0.0001
training loss: tensor(0.0890)
loss: CE learning rate: 0.0001
training loss: tensor(0.0531)
loss: CE learning rate: 0.0001
training loss: tensor(0.0888)
loss: CE learning rate: 0.0001
training loss: tensor(0.1416)
         GM acc on global data: 0.5086 length of data: 10000
-------------Round number:  82  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2101)
loss: CE learning rate: 0.0001
training loss: tensor(0.0270)
loss: CE learning rate: 0.0001
training loss: tensor(0.2006)
loss: CE learning rate: 0.0001
training loss: tensor(0.0145)
loss: CE learning rate: 0.0001
training loss: tensor(0.1174)
loss: CE learning rate: 0.0001
training loss: tensor(0.0890)
loss: CE learning rate: 0.0001
training loss: tensor(0.3075)
loss: CE learning rate: 0.0001
training loss: tensor(0.1253)
loss: CE learning rate: 0.0001
training loss: tensor(0.2018)
loss: CE learning rate: 0.0001
training loss: tensor(0.1467)
         GM acc on global data: 0.5126 length of data: 10000
-------------Round number:  83  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1212)
loss: CE learning rate: 0.0001
training loss: tensor(0.2695)
loss: CE learning rate: 0.0001
training loss: tensor(0.0203)
loss: CE learning rate: 0.0001
training loss: tensor(0.0468)
loss: CE learning rate: 0.0001
training loss: tensor(0.2588)
loss: CE learning rate: 0.0001
training loss: tensor(0.1525)
loss: CE learning rate: 0.0001
training loss: tensor(0.0453)
loss: CE learning rate: 0.0001
training loss: tensor(0.0293)
loss: CE learning rate: 0.0001
training loss: tensor(0.2974)
loss: CE learning rate: 0.0001
training loss: tensor(0.2652)
         GM acc on global data: 0.5163 length of data: 10000
-------------Round number:  84  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1063)
loss: CE learning rate: 0.0001
training loss: tensor(0.0853)
loss: CE learning rate: 0.0001
training loss: tensor(0.0987)
loss: CE learning rate: 0.0001
training loss: tensor(0.1880)
loss: CE learning rate: 0.0001
training loss: tensor(0.1623)
loss: CE learning rate: 0.0001
training loss: tensor(0.3253)
loss: CE learning rate: 0.0001
training loss: tensor(0.6429)
loss: CE learning rate: 0.0001
training loss: tensor(0.0447)
loss: CE learning rate: 0.0001
training loss: tensor(0.0517)
loss: CE learning rate: 0.0001
training loss: tensor(0.0924)
         GM acc on global data: 0.5149 length of data: 10000
-------------Round number:  85  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2330)
loss: CE learning rate: 0.0001
training loss: tensor(0.1596)
loss: CE learning rate: 0.0001
training loss: tensor(0.0796)
loss: CE learning rate: 0.0001
training loss: tensor(0.1850)
loss: CE learning rate: 0.0001
training loss: tensor(0.1534)
loss: CE learning rate: 0.0001
training loss: tensor(0.3602)
loss: CE learning rate: 0.0001
training loss: tensor(0.1050)
loss: CE learning rate: 0.0001
training loss: tensor(0.1743)
loss: CE learning rate: 0.0001
training loss: tensor(0.2126)
loss: CE learning rate: 0.0001
training loss: tensor(0.2318)
         GM acc on global data: 0.5268 length of data: 10000
-------------Round number:  86  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0123)
loss: CE learning rate: 0.0001
training loss: tensor(0.1270)
loss: CE learning rate: 0.0001
training loss: tensor(0.1231)
loss: CE learning rate: 0.0001
training loss: tensor(0.3384)
loss: CE learning rate: 0.0001
training loss: tensor(0.0868)
loss: CE learning rate: 0.0001
training loss: tensor(0.0326)
loss: CE learning rate: 0.0001
training loss: tensor(0.7558)
loss: CE learning rate: 0.0001
training loss: tensor(0.3782)
loss: CE learning rate: 0.0001
training loss: tensor(0.0470)
loss: CE learning rate: 0.0001
training loss: tensor(0.2394)
         GM acc on global data: 0.5208 length of data: 10000
-------------Round number:  87  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0763)
loss: CE learning rate: 0.0001
training loss: tensor(0.0710)
loss: CE learning rate: 0.0001
training loss: tensor(0.0741)
loss: CE learning rate: 0.0001
training loss: tensor(1.0083)
loss: CE learning rate: 0.0001
training loss: tensor(0.1226)
loss: CE learning rate: 0.0001
training loss: tensor(0.0982)
loss: CE learning rate: 0.0001
training loss: tensor(0.3859)
loss: CE learning rate: 0.0001
training loss: tensor(0.0114)
loss: CE learning rate: 0.0001
training loss: tensor(0.1487)
loss: CE learning rate: 0.0001
training loss: tensor(0.0710)
         GM acc on global data: 0.5178 length of data: 10000
-------------Round number:  88  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0294)
loss: CE learning rate: 0.0001
training loss: tensor(0.1847)
loss: CE learning rate: 0.0001
training loss: tensor(0.2581)
loss: CE learning rate: 0.0001
training loss: tensor(0.0654)
loss: CE learning rate: 0.0001
training loss: tensor(0.0138)
loss: CE learning rate: 0.0001
training loss: tensor(0.0815)
loss: CE learning rate: 0.0001
training loss: tensor(0.1295)
loss: CE learning rate: 0.0001
training loss: tensor(0.0930)
loss: CE learning rate: 0.0001
training loss: tensor(0.1200)
loss: CE learning rate: 0.0001
training loss: tensor(0.3848)
         GM acc on global data: 0.5179 length of data: 10000
-------------Round number:  89  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1406)
loss: CE learning rate: 0.0001
training loss: tensor(0.1872)
loss: CE learning rate: 0.0001
training loss: tensor(0.1228)
loss: CE learning rate: 0.0001
training loss: tensor(0.0587)
loss: CE learning rate: 0.0001
training loss: tensor(0.1039)
loss: CE learning rate: 0.0001
training loss: tensor(0.0854)
loss: CE learning rate: 0.0001
training loss: tensor(0.1303)
loss: CE learning rate: 0.0001
training loss: tensor(0.2565)
loss: CE learning rate: 0.0001
training loss: tensor(0.1182)
loss: CE learning rate: 0.0001
training loss: tensor(0.0928)
         GM acc on global data: 0.5105 length of data: 10000
-------------Round number:  90  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1314)
loss: CE learning rate: 0.0001
training loss: tensor(0.0384)
loss: CE learning rate: 0.0001
training loss: tensor(0.3496)
loss: CE learning rate: 0.0001
training loss: tensor(0.1161)
loss: CE learning rate: 0.0001
training loss: tensor(0.0727)
loss: CE learning rate: 0.0001
training loss: tensor(0.0839)
loss: CE learning rate: 0.0001
training loss: tensor(0.0593)
loss: CE learning rate: 0.0001
training loss: tensor(0.1849)
loss: CE learning rate: 0.0001
training loss: tensor(0.1441)
loss: CE learning rate: 0.0001
training loss: tensor(0.2849)
         GM acc on global data: 0.5091 length of data: 10000
-------------Round number:  91  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1184)
loss: CE learning rate: 0.0001
training loss: tensor(0.0717)
loss: CE learning rate: 0.0001
training loss: tensor(0.1800)
loss: CE learning rate: 0.0001
training loss: tensor(0.1285)
loss: CE learning rate: 0.0001
training loss: tensor(0.7563)
loss: CE learning rate: 0.0001
training loss: tensor(0.1168)
loss: CE learning rate: 0.0001
training loss: tensor(0.1080)
loss: CE learning rate: 0.0001
training loss: tensor(0.5823)
loss: CE learning rate: 0.0001
training loss: tensor(0.1354)
loss: CE learning rate: 0.0001
training loss: tensor(0.2270)
         GM acc on global data: 0.5298 length of data: 10000
-------------Round number:  92  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0214)
loss: CE learning rate: 0.0001
training loss: tensor(0.2202)
loss: CE learning rate: 0.0001
training loss: tensor(0.0615)
loss: CE learning rate: 0.0001
training loss: tensor(0.2120)
loss: CE learning rate: 0.0001
training loss: tensor(0.3765)
loss: CE learning rate: 0.0001
training loss: tensor(0.0756)
loss: CE learning rate: 0.0001
training loss: tensor(0.3685)
loss: CE learning rate: 0.0001
training loss: tensor(0.1468)
loss: CE learning rate: 0.0001
training loss: tensor(0.5195)
loss: CE learning rate: 0.0001
training loss: tensor(0.0671)
         GM acc on global data: 0.5171 length of data: 10000
-------------Round number:  93  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0653)
loss: CE learning rate: 0.0001
training loss: tensor(0.0510)
loss: CE learning rate: 0.0001
training loss: tensor(0.0941)
loss: CE learning rate: 0.0001
training loss: tensor(0.0633)
loss: CE learning rate: 0.0001
training loss: tensor(0.0918)
loss: CE learning rate: 0.0001
training loss: tensor(0.1649)
loss: CE learning rate: 0.0001
training loss: tensor(0.1324)
loss: CE learning rate: 0.0001
training loss: tensor(0.1772)
loss: CE learning rate: 0.0001
training loss: tensor(0.1893)
loss: CE learning rate: 0.0001
training loss: tensor(0.1314)
         GM acc on global data: 0.5225 length of data: 10000
-------------Round number:  94  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4540)
loss: CE learning rate: 0.0001
training loss: tensor(0.0360)
loss: CE learning rate: 0.0001
training loss: tensor(0.0780)
loss: CE learning rate: 0.0001
training loss: tensor(0.0687)
loss: CE learning rate: 0.0001
training loss: tensor(0.1164)
loss: CE learning rate: 0.0001
training loss: tensor(0.1379)
loss: CE learning rate: 0.0001
training loss: tensor(0.2200)
loss: CE learning rate: 0.0001
training loss: tensor(0.1972)
loss: CE learning rate: 0.0001
training loss: tensor(0.0907)
loss: CE learning rate: 0.0001
training loss: tensor(0.0154)
         GM acc on global data: 0.5163 length of data: 10000
-------------Round number:  95  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4163)
loss: CE learning rate: 0.0001
training loss: tensor(0.0471)
loss: CE learning rate: 0.0001
training loss: tensor(0.0972)
loss: CE learning rate: 0.0001
training loss: tensor(0.3920)
loss: CE learning rate: 0.0001
training loss: tensor(0.0206)
loss: CE learning rate: 0.0001
training loss: tensor(0.4128)
loss: CE learning rate: 0.0001
training loss: tensor(0.2293)
loss: CE learning rate: 0.0001
training loss: tensor(0.0131)
loss: CE learning rate: 0.0001
training loss: tensor(0.1502)
loss: CE learning rate: 0.0001
training loss: tensor(0.0940)
         GM acc on global data: 0.5193 length of data: 10000
-------------Round number:  96  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1081)
loss: CE learning rate: 0.0001
training loss: tensor(0.1747)
loss: CE learning rate: 0.0001
training loss: tensor(0.0873)
loss: CE learning rate: 0.0001
training loss: tensor(0.1811)
loss: CE learning rate: 0.0001
training loss: tensor(0.0876)
loss: CE learning rate: 0.0001
training loss: tensor(0.0093)
loss: CE learning rate: 0.0001
training loss: tensor(0.0405)
loss: CE learning rate: 0.0001
training loss: tensor(0.0893)
loss: CE learning rate: 0.0001
training loss: tensor(0.1792)
loss: CE learning rate: 0.0001
training loss: tensor(0.6939)
         GM acc on global data: 0.5238 length of data: 10000
-------------Round number:  97  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0682)
loss: CE learning rate: 0.0001
training loss: tensor(0.2048)
loss: CE learning rate: 0.0001
training loss: tensor(0.0920)
loss: CE learning rate: 0.0001
training loss: tensor(0.0576)
loss: CE learning rate: 0.0001
training loss: tensor(0.0303)
loss: CE learning rate: 0.0001
training loss: tensor(0.0094)
loss: CE learning rate: 0.0001
training loss: tensor(0.0869)
loss: CE learning rate: 0.0001
training loss: tensor(0.0858)
loss: CE learning rate: 0.0001
training loss: tensor(0.1210)
loss: CE learning rate: 0.0001
training loss: tensor(0.1514)
         GM acc on global data: 0.5092 length of data: 10000
-------------Round number:  98  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3753)
loss: CE learning rate: 0.0001
training loss: tensor(0.1256)
loss: CE learning rate: 0.0001
training loss: tensor(0.1485)
loss: CE learning rate: 0.0001
training loss: tensor(0.0926)
loss: CE learning rate: 0.0001
training loss: tensor(0.0093)
loss: CE learning rate: 0.0001
training loss: tensor(0.0671)
loss: CE learning rate: 0.0001
training loss: tensor(0.0869)
loss: CE learning rate: 0.0001
training loss: tensor(0.2961)
loss: CE learning rate: 0.0001
training loss: tensor(0.0520)
loss: CE learning rate: 0.0001
training loss: tensor(0.1390)
         GM acc on global data: 0.5003 length of data: 10000
-------------Round number:  99  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0342)
loss: CE learning rate: 0.0001
training loss: tensor(0.0483)
loss: CE learning rate: 0.0001
training loss: tensor(0.0505)
loss: CE learning rate: 0.0001
training loss: tensor(0.8519)
loss: CE learning rate: 0.0001
training loss: tensor(0.1423)
loss: CE learning rate: 0.0001
training loss: tensor(0.2342)
loss: CE learning rate: 0.0001
training loss: tensor(0.1462)
loss: CE learning rate: 0.0001
training loss: tensor(0.3724)
loss: CE learning rate: 0.0001
training loss: tensor(0.0815)
loss: CE learning rate: 0.0001
training loss: tensor(0.0101)
         GM acc on global data: 0.5147 length of data: 10000
