nohup: ignoring input
================================================================================
Summary of training process:
Dataset:                 Cifar10
Batch size:              64
Learing rate :           0.001
personal learning rate : 0.001
Number of total clients: 100
Split parameter        : 0.1
Clients per round      : 10
Number of global rounds: 100
Number of local rounds : 10
Feature from layer     : 11
Feature reduction      : 64
Local training loss    : CE
Loss of beta           : 1.0
Algorithm              : FedAvg
Modelname              : MOBNET
Mode                   : training
================================================================================
Files already downloaded and verified
Files already downloaded and verified
Here
Done
Class frequencies:
client,class0,class1,class2,class3,class4,class5,class6,class7,class8,class9,Amount
Client   0,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,1.000,171
Client   1,0.231,0.038,0.00,0.00,0.00,0.077,0.00,0.654,0.00,0.00,52
Client   2,0.374,0.015,0.00,0.00,0.519,0.011,0.007,0.015,0.00,0.059,457
Client   3,0.00,0.00,0.00,0.232,0.768,0.00,0.00,0.00,0.00,0.00,56
Client   4,0.660,0.071,0.032,0.122,0.00,0.00,0.083,0.00,0.00,0.032,156
Client   5,0.00,0.00,0.00,0.00,0.029,0.00,0.00,0.00,0.121,0.850,479
Client   6,0.00,0.080,0.00,0.00,0.009,0.017,0.894,0.00,0.00,0.00,538
Client   7,0.00,0.00,0.00,0.00,0.991,0.00,0.00,0.009,0.00,0.00,110
Client   8,0.131,0.116,0.00,0.00,0.00,0.00,0.00,0.00,0.753,0.00,1059
Client   9,0.00,0.00,0.175,0.00,0.467,0.017,0.192,0.150,0.00,0.00,120
Client  10,0.00,0.331,0.540,0.00,0.00,0.00,0.129,0.00,0.00,0.00,513
Client  11,0.00,0.00,0.250,0.00,0.004,0.358,0.254,0.008,0.121,0.004,240
Client  12,0.00,0.00,0.00,0.00,0.00,0.562,0.00,0.00,0.396,0.042,48
Client  13,0.00,0.00,0.00,0.371,0.185,0.00,0.444,0.00,0.00,0.00,453
Client  14,0.068,0.00,0.003,0.011,0.00,0.042,0.641,0.234,0.00,0.00,354
Client  15,0.00,0.005,0.00,0.00,0.995,0.00,0.00,0.00,0.00,0.00,213
Client  16,1.000,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,830
Client  17,0.335,0.00,0.004,0.00,0.007,0.00,0.00,0.654,0.00,0.00,543
Client  18,0.00,0.743,0.00,0.127,0.053,0.076,0.00,0.00,0.00,0.00,526
Client  19,0.008,0.00,0.057,0.676,0.185,0.074,0.00,0.00,0.00,0.00,524
Client  20,0.00,0.00,0.243,0.108,0.00,0.00,0.00,0.216,0.432,0.00,37
Client  21,0.00,0.00,0.00,0.141,0.848,0.00,0.004,0.007,0.00,0.00,277
Client  22,0.00,0.192,0.00,0.00,0.00,0.010,0.00,0.028,0.063,0.708,718
Client  23,0.00,0.100,0.529,0.00,0.00,0.214,0.129,0.00,0.00,0.029,70
Client  24,0.014,0.00,0.002,0.002,0.00,0.00,0.00,0.982,0.00,0.00,1084
Client  25,0.00,0.00,0.006,0.105,0.00,0.169,0.010,0.025,0.684,0.00,484
Client  26,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.194,0.00,0.806,72
Client  27,0.00,0.00,0.00,0.00,0.011,0.086,0.004,0.00,0.019,0.881,269
Client  28,0.237,0.00,0.00,0.001,0.038,0.167,0.001,0.011,0.546,0.00,951
Client  29,0.00,1.000,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,845
Client  30,0.004,0.00,0.218,0.743,0.00,0.00,0.00,0.023,0.008,0.004,257
Client  31,0.014,0.730,0.00,0.005,0.194,0.005,0.052,0.00,0.00,0.00,211
Client  32,0.00,0.00,0.00,0.00,0.004,0.00,0.743,0.222,0.031,0.00,257
Client  33,0.559,0.084,0.002,0.00,0.013,0.00,0.00,0.234,0.108,0.00,546
Client  34,0.00,0.010,0.093,0.00,0.202,0.010,0.00,0.00,0.685,0.00,1363
Client  35,0.913,0.008,0.00,0.011,0.00,0.065,0.00,0.00,0.00,0.004,263
Client  36,0.029,0.00,0.001,0.00,0.139,0.006,0.001,0.003,0.821,0.00,979
Client  37,0.00,0.00,0.259,0.194,0.237,0.245,0.029,0.00,0.00,0.036,139
Client  38,0.075,0.00,0.00,0.00,0.00,0.925,0.00,0.00,0.00,0.00,239
Client  39,0.00,0.334,0.00,0.00,0.00,0.00,0.00,0.666,0.00,0.00,1283
Client  40,0.00,0.00,0.00,0.00,0.00,0.353,0.00,0.211,0.436,0.00,337
Client  41,1.000,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,502
Client  42,0.218,0.00,0.020,0.259,0.00,0.041,0.00,0.463,0.00,0.00,147
Client  43,0.073,0.008,0.024,0.062,0.073,0.00,0.00,0.00,0.003,0.758,372
Client  44,0.003,0.00,0.00,0.064,0.269,0.00,0.114,0.247,0.302,0.00,668
Client  45,0.00,0.025,0.00,0.066,0.689,0.00,0.213,0.00,0.008,0.00,122
Client  46,0.002,0.258,0.019,0.00,0.042,0.274,0.00,0.406,0.00,0.00,530
Client  47,1.000,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,689
Client  48,0.023,0.00,0.003,0.036,0.00,0.00,0.227,0.006,0.605,0.100,309
Client  49,0.102,0.377,0.001,0.519,0.00,0.00,0.00,0.00,0.00,0.00,695
Client  50,0.00,0.00,0.00,0.136,0.00,0.00,0.00,0.017,0.00,0.847,118
Client  51,0.331,0.035,0.00,0.052,0.00,0.083,0.034,0.00,0.012,0.453,762
Client  52,0.007,0.276,0.717,0.00,0.00,0.00,0.00,0.00,0.00,0.00,1078
Client  53,0.00,0.073,0.153,0.00,0.00,0.00,0.271,0.00,0.504,0.00,262
Client  54,0.003,0.003,0.127,0.173,0.663,0.025,0.00,0.006,0.00,0.00,323
Client  55,0.000,0.00,0.000,0.00,0.00,0.999,0.00,0.00,0.00,0.00,2015
Client  56,0.00,0.00,0.014,0.986,0.00,0.00,0.00,0.00,0.00,0.00,876
Client  57,0.424,0.299,0.002,0.275,0.00,0.00,0.00,0.00,0.00,0.00,655
Client  58,0.003,0.997,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,787
Client  59,0.00,0.00,0.908,0.00,0.00,0.00,0.031,0.00,0.061,0.00,131
Client  60,0.067,0.00,0.00,0.00,0.067,0.200,0.00,0.200,0.333,0.133,15
Client  61,0.00,0.027,0.001,0.00,0.00,0.00,0.430,0.177,0.364,0.00,711
Client  62,0.00,0.00,0.00,0.00,0.006,0.983,0.00,0.00,0.011,0.00,359
Client  63,0.00,0.00,0.519,0.002,0.005,0.229,0.246,0.00,0.00,0.00,582
Client  64,0.00,0.00,0.00,0.00,0.00,0.064,0.207,0.729,0.00,0.00,299
Client  65,0.295,0.003,0.00,0.00,0.00,0.00,0.00,0.011,0.691,0.00,356
Client  66,0.375,0.267,0.003,0.00,0.182,0.020,0.00,0.003,0.131,0.020,352
Client  67,0.053,0.00,0.037,0.00,0.910,0.00,0.00,0.00,0.00,0.00,543
Client  68,0.00,0.001,0.999,0.00,0.00,0.00,0.00,0.00,0.00,0.00,993
Client  69,0.00,0.185,0.00,0.00,0.228,0.587,0.00,0.00,0.00,0.00,584
Client  70,0.00,0.00,0.011,0.00,0.122,0.00,0.867,0.00,0.00,0.00,90
Client  71,0.00,0.931,0.00,0.00,0.038,0.013,0.00,0.00,0.019,0.00,159
Client  72,0.00,0.00,0.034,0.00,0.00,0.00,0.00,0.966,0.00,0.00,206
Client  73,0.00,0.103,0.00,0.068,0.017,0.222,0.00,0.00,0.111,0.479,117
Client  74,0.00,0.00,0.003,0.861,0.00,0.00,0.00,0.136,0.00,0.00,309
Client  75,0.00,0.00,0.00,0.001,0.009,0.00,0.991,0.00,0.00,0.00,2441
Client  76,0.026,0.00,0.00,0.00,0.071,0.00,0.00,0.00,0.002,0.900,2848
Client  77,0.00,0.00,0.026,0.00,0.00,0.974,0.00,0.00,0.00,0.00,693
Client  78,0.440,0.004,0.00,0.00,0.444,0.00,0.00,0.00,0.112,0.00,277
Client  79,0.828,0.007,0.00,0.131,0.004,0.029,0.00,0.00,0.00,0.00,274
Client  80,0.00,0.201,0.003,0.661,0.00,0.013,0.003,0.119,0.00,0.00,319
Client  81,0.00,0.00,0.00,1.000,0.00,0.00,0.00,0.00,0.00,0.00,1152
Client  82,0.00,0.009,0.757,0.009,0.00,0.00,0.036,0.00,0.009,0.180,111
Client  83,0.005,0.00,0.00,0.00,0.003,0.228,0.00,0.750,0.00,0.013,372
Client  84,0.00,0.00,0.00,0.063,0.077,0.00,0.860,0.00,0.00,0.00,271
Client  85,0.00,0.244,0.555,0.00,0.00,0.00,0.00,0.088,0.113,0.00,238
Client  86,0.002,0.00,0.314,0.00,0.666,0.00,0.00,0.00,0.018,0.00,437
Client  87,0.00,0.022,0.022,0.011,0.022,0.043,0.269,0.258,0.032,0.323,93
Client  88,0.00,0.00,0.00,0.281,0.00,0.00,0.00,0.357,0.010,0.352,210
Client  89,0.086,0.029,0.642,0.011,0.103,0.019,0.00,0.00,0.00,0.109,523
Client  90,0.00,0.00,1.000,0.00,0.00,0.00,0.00,0.00,0.00,0.00,800
Client  91,0.00,0.00,0.377,0.003,0.00,0.002,0.00,0.619,0.00,0.00,1199
Client  92,0.065,0.00,0.015,0.915,0.00,0.005,0.00,0.00,0.00,0.00,400
Client  93,0.00,0.496,0.00,0.00,0.331,0.132,0.00,0.041,0.00,0.00,516
Client  94,0.00,0.00,0.00,0.00,0.008,0.032,0.944,0.00,0.016,0.00,125
Client  95,0.00,0.00,0.375,0.00,0.00,0.00,0.00,0.00,0.625,0.00,32
Client  96,0.00,0.073,0.011,0.00,0.915,0.00,0.00,0.00,0.00,0.00,1321
Client  97,0.00,0.00,0.00,0.00,0.00,0.00,0.925,0.00,0.050,0.025,40
Client  98,0.00,0.073,0.00,0.00,0.00,0.836,0.018,0.009,0.064,0.00,110
Client  99,0.173,0.003,0.003,0.799,0.003,0.003,0.003,0.003,0.006,0.006,359
Num_samples of Training set per client: [171, 52, 457, 56, 156, 479, 538, 110, 1059, 120, 513, 240, 48, 453, 354, 213, 830, 543, 526, 524, 37, 277, 718, 70, 1084, 484, 72, 269, 951, 845, 257, 211, 257, 546, 1363, 263, 979, 139, 239, 1283, 337, 502, 147, 372, 668, 122, 530, 689, 309, 695, 118, 762, 1078, 262, 323, 2015, 876, 655, 787, 131, 15, 711, 359, 582, 299, 356, 352, 543, 993, 584, 90, 159, 206, 117, 309, 2441, 2848, 693, 277, 274, 319, 1152, 111, 372, 271, 238, 437, 93, 210, 523, 800, 1199, 400, 516, 125, 32, 1321, 40, 110, 359]
Total_training_samples: 50000
Global test set: 10000
Finish Generating Samples, distribution saved
MobileNetV2(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layers): Sequential(
    (0): Block(
      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Block(
      (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (6): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (7): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (8): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (9): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (10): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (12): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (13): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (14): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (15): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (16): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv2): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear): Linear(in_features=1280, out_features=10, bias=True)
)
number of parameters: 2296922
clients initializting...
output size: 10
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:51,  1.94it/s]  2%|▏         | 2/100 [00:01<00:54,  1.81it/s]  3%|▎         | 3/100 [00:01<01:00,  1.61it/s]  4%|▍         | 4/100 [00:05<02:24,  1.51s/it]  5%|▌         | 5/100 [00:06<02:01,  1.27s/it]  6%|▌         | 6/100 [00:06<01:41,  1.08s/it]  7%|▋         | 7/100 [00:07<01:28,  1.05it/s]  8%|▊         | 8/100 [00:08<01:16,  1.21it/s]  9%|▉         | 9/100 [00:08<01:10,  1.30it/s] 10%|█         | 10/100 [00:09<01:07,  1.34it/s] 11%|█         | 11/100 [00:09<01:02,  1.42it/s] 12%|█▏        | 12/100 [00:10<01:03,  1.38it/s] 13%|█▎        | 13/100 [00:11<00:59,  1.46it/s] 14%|█▍        | 14/100 [00:11<00:57,  1.50it/s] 15%|█▌        | 15/100 [00:12<00:58,  1.46it/s] 16%|█▌        | 16/100 [00:13<00:54,  1.55it/s] 17%|█▋        | 17/100 [00:13<00:51,  1.63it/s] 18%|█▊        | 18/100 [00:14<00:51,  1.59it/s] 19%|█▉        | 19/100 [00:15<00:51,  1.57it/s] 20%|██        | 20/100 [00:15<00:52,  1.52it/s] 21%|██        | 21/100 [00:16<00:51,  1.54it/s] 22%|██▏       | 22/100 [00:17<00:50,  1.54it/s] 23%|██▎       | 23/100 [00:17<00:51,  1.51it/s] 24%|██▍       | 24/100 [00:18<00:50,  1.50it/s] 25%|██▌       | 25/100 [00:19<00:50,  1.50it/s] 26%|██▌       | 26/100 [00:19<00:50,  1.45it/s] 27%|██▋       | 27/100 [00:20<00:47,  1.54it/s] 28%|██▊       | 28/100 [00:21<00:47,  1.51it/s] 29%|██▉       | 29/100 [00:21<00:49,  1.42it/s] 30%|███       | 30/100 [00:22<00:45,  1.53it/s] 31%|███       | 31/100 [00:23<00:46,  1.48it/s] 32%|███▏      | 32/100 [00:23<00:47,  1.44it/s] 33%|███▎      | 33/100 [00:24<00:45,  1.47it/s] 34%|███▍      | 34/100 [00:25<00:46,  1.43it/s] 35%|███▌      | 35/100 [00:26<00:46,  1.41it/s] 36%|███▌      | 36/100 [00:29<01:42,  1.60s/it] 37%|███▋      | 37/100 [00:30<01:25,  1.36s/it] 38%|███▊      | 38/100 [00:31<01:12,  1.17s/it] 39%|███▉      | 39/100 [00:31<01:00,  1.01it/s] 40%|████      | 40/100 [00:32<00:52,  1.15it/s] 41%|████      | 41/100 [00:33<00:46,  1.26it/s] 42%|████▏     | 42/100 [00:33<00:41,  1.40it/s] 43%|████▎     | 43/100 [00:34<00:40,  1.42it/s] 44%|████▍     | 44/100 [00:35<00:40,  1.38it/s] 45%|████▌     | 45/100 [00:35<00:40,  1.37it/s] 46%|████▌     | 46/100 [00:36<00:38,  1.39it/s] 47%|████▋     | 47/100 [00:37<00:38,  1.38it/s] 48%|████▊     | 48/100 [00:37<00:34,  1.50it/s] 49%|████▉     | 49/100 [00:38<00:35,  1.43it/s] 50%|█████     | 50/100 [00:39<00:34,  1.45it/s] 51%|█████     | 51/100 [00:39<00:32,  1.51it/s] 52%|█████▏    | 52/100 [00:40<00:33,  1.43it/s] 53%|█████▎    | 53/100 [00:41<00:32,  1.46it/s] 54%|█████▍    | 54/100 [00:41<00:31,  1.48it/s] 55%|█████▌    | 55/100 [00:42<00:31,  1.42it/s] 56%|█████▌    | 56/100 [00:43<00:30,  1.43it/s] 57%|█████▋    | 57/100 [00:43<00:28,  1.51it/s] 58%|█████▊    | 58/100 [00:44<00:27,  1.51it/s] 59%|█████▉    | 59/100 [00:45<00:26,  1.57it/s] 60%|██████    | 60/100 [00:45<00:25,  1.59it/s] 61%|██████    | 61/100 [00:46<00:25,  1.52it/s] 62%|██████▏   | 62/100 [00:47<00:25,  1.49it/s] 63%|██████▎   | 63/100 [00:47<00:24,  1.52it/s] 64%|██████▍   | 64/100 [00:48<00:24,  1.49it/s] 65%|██████▌   | 65/100 [00:49<00:22,  1.53it/s] 66%|██████▌   | 66/100 [00:49<00:22,  1.53it/s] 67%|██████▋   | 67/100 [00:50<00:23,  1.43it/s] 68%|██████▊   | 68/100 [00:54<00:51,  1.60s/it] 69%|██████▉   | 69/100 [00:54<00:40,  1.30s/it] 70%|███████   | 70/100 [00:55<00:32,  1.10s/it] 71%|███████   | 71/100 [00:56<00:27,  1.05it/s] 72%|███████▏  | 72/100 [00:56<00:24,  1.17it/s] 73%|███████▎  | 73/100 [00:57<00:20,  1.30it/s] 74%|███████▍  | 74/100 [00:58<00:19,  1.32it/s] 75%|███████▌  | 75/100 [00:58<00:17,  1.40it/s] 76%|███████▌  | 76/100 [00:59<00:17,  1.38it/s] 77%|███████▋  | 77/100 [01:00<00:17,  1.35it/s] 78%|███████▊  | 78/100 [01:00<00:15,  1.45it/s] 79%|███████▉  | 79/100 [01:01<00:14,  1.47it/s] 80%|████████  | 80/100 [01:02<00:13,  1.47it/s] 81%|████████  | 81/100 [01:02<00:13,  1.44it/s] 82%|████████▏ | 82/100 [01:03<00:11,  1.53it/s] 83%|████████▎ | 83/100 [01:04<00:11,  1.48it/s] 84%|████████▍ | 84/100 [01:04<00:10,  1.47it/s] 85%|████████▌ | 85/100 [01:05<00:09,  1.52it/s] 86%|████████▌ | 86/100 [01:06<00:09,  1.52it/s] 87%|████████▋ | 87/100 [01:06<00:08,  1.52it/s] 88%|████████▊ | 88/100 [01:07<00:08,  1.40it/s] 89%|████████▉ | 89/100 [01:08<00:07,  1.44it/s] 90%|█████████ | 90/100 [01:08<00:07,  1.39it/s] 91%|█████████ | 91/100 [01:09<00:06,  1.50it/s] 92%|█████████▏| 92/100 [01:10<00:05,  1.48it/s] 93%|█████████▎| 93/100 [01:10<00:04,  1.50it/s] 94%|█████████▍| 94/100 [01:11<00:04,  1.50it/s] 95%|█████████▌| 95/100 [01:12<00:03,  1.52it/s] 96%|█████████▌| 96/100 [01:12<00:02,  1.60it/s] 97%|█████████▋| 97/100 [01:13<00:01,  1.59it/s] 98%|█████████▊| 98/100 [01:13<00:01,  1.61it/s] 99%|█████████▉| 99/100 [01:14<00:00,  1.56it/s]100%|██████████| 100/100 [01:15<00:00,  1.39it/s]100%|██████████| 100/100 [01:15<00:00,  1.32it/s]
Number of users per round / total users: 10  /  100
Finished creating FedAvg server.
=== FedAvg ===
-------------Round number:  0  -------------
loss: CE learning rate: 0.001
training loss: tensor(0.5284)
loss: CE learning rate: 0.001
training loss: tensor(0.5211)
loss: CE learning rate: 0.001
training loss: tensor(0.6040)
loss: CE learning rate: 0.001
training loss: tensor(0.0274)
loss: CE learning rate: 0.001
training loss: tensor(0.0921)
loss: CE learning rate: 0.001
training loss: tensor(0.5986)
loss: CE learning rate: 0.001
training loss: tensor(0.0395)
loss: CE learning rate: 0.001
training loss: tensor(0.6995)
loss: CE learning rate: 0.001
training loss: tensor(0.8337)
loss: CE learning rate: 0.001
training loss: tensor(0.5160)
         GM acc on global data: 0.1085 length of data: 10000
save a model
-------------Round number:  1  -------------
loss: CE learning rate: 0.000982
training loss: tensor(3.1342)
loss: CE learning rate: 0.000982
training loss: tensor(0.3575)
loss: CE learning rate: 0.000982
training loss: tensor(2.0570)
loss: CE learning rate: 0.000982
training loss: tensor(0.7355)
loss: CE learning rate: 0.000982
training loss: tensor(1.4886)
loss: CE learning rate: 0.000982
training loss: tensor(1.5511)
loss: CE learning rate: 0.000982
training loss: tensor(1.1755)
loss: CE learning rate: 0.000982
training loss: tensor(5.3897)
loss: CE learning rate: 0.000982
training loss: tensor(1.0755)
loss: CE learning rate: 0.000982
training loss: tensor(2.6652)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  2  -------------
loss: CE learning rate: 0.000964
training loss: tensor(0.6178)
loss: CE learning rate: 0.000964
training loss: tensor(2.5865)
loss: CE learning rate: 0.000964
training loss: tensor(0.3908)
loss: CE learning rate: 0.000964
training loss: tensor(0.2130)
loss: CE learning rate: 0.000964
training loss: tensor(0.0630)
loss: CE learning rate: 0.000964
training loss: tensor(4.5305)
loss: CE learning rate: 0.000964
training loss: tensor(0.6302)
loss: CE learning rate: 0.000964
training loss: tensor(2.5636)
loss: CE learning rate: 0.000964
training loss: tensor(0.5526)
loss: CE learning rate: 0.000964
training loss: tensor(1.0944)
         GM acc on global data: 0.1013 length of data: 10000
-------------Round number:  3  -------------
loss: CE learning rate: 0.000946
training loss: tensor(0.9713)
loss: CE learning rate: 0.000946
training loss: tensor(0.2618)
loss: CE learning rate: 0.000946
training loss: tensor(0.8616)
loss: CE learning rate: 0.000946
training loss: tensor(0.5386)
loss: CE learning rate: 0.000946
training loss: tensor(0.2291)
loss: CE learning rate: 0.000946
training loss: tensor(0.3520)
loss: CE learning rate: 0.000946
training loss: tensor(1.2058)
loss: CE learning rate: 0.000946
training loss: tensor(0.8073)
loss: CE learning rate: 0.000946
training loss: tensor(0.6697)
loss: CE learning rate: 0.000946
training loss: tensor(0.1571)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  4  -------------
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(6.6227)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(1.8218)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.1064)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5081)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(1.4045)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.2979)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(2.6515)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(7.5239)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.7246)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5197)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  5  -------------
loss: CE learning rate: 0.00091
training loss: tensor(0.6471)
loss: CE learning rate: 0.00091
training loss: tensor(1.5738)
loss: CE learning rate: 0.00091
training loss: tensor(0.5429)
loss: CE learning rate: 0.00091
training loss: tensor(0.6473)
loss: CE learning rate: 0.00091
training loss: tensor(1.6752)
loss: CE learning rate: 0.00091
training loss: tensor(2.2947)
loss: CE learning rate: 0.00091
training loss: tensor(0.6710)
loss: CE learning rate: 0.00091
training loss: tensor(3.4359)
loss: CE learning rate: 0.00091
training loss: tensor(0.2704)
loss: CE learning rate: 0.00091
training loss: tensor(0.4314)
         GM acc on global data: 0.1421 length of data: 10000
save a model
-------------Round number:  6  -------------
loss: CE learning rate: 0.000892
training loss: tensor(0.1122)
loss: CE learning rate: 0.000892
training loss: tensor(0.3301)
loss: CE learning rate: 0.000892
training loss: tensor(3.4847)
loss: CE learning rate: 0.000892
training loss: tensor(1.0083)
loss: CE learning rate: 0.000892
training loss: tensor(1.3395)
loss: CE learning rate: 0.000892
training loss: tensor(0.3947)
loss: CE learning rate: 0.000892
training loss: tensor(1.7683)
loss: CE learning rate: 0.000892
training loss: tensor(0.0129)
loss: CE learning rate: 0.000892
training loss: tensor(0.6521)
loss: CE learning rate: 0.000892
training loss: tensor(0.4911)
         GM acc on global data: 0.1723 length of data: 10000
save a model
-------------Round number:  7  -------------
loss: CE learning rate: 0.000874
training loss: tensor(1.6522)
loss: CE learning rate: 0.000874
training loss: tensor(6.0428)
loss: CE learning rate: 0.000874
training loss: tensor(0.3100)
loss: CE learning rate: 0.000874
training loss: tensor(0.3352)
loss: CE learning rate: 0.000874
training loss: tensor(0.5803)
loss: CE learning rate: 0.000874
training loss: tensor(0.1032)
loss: CE learning rate: 0.000874
training loss: tensor(0.6016)
loss: CE learning rate: 0.000874
training loss: tensor(1.7511)
loss: CE learning rate: 0.000874
training loss: tensor(0.3107)
loss: CE learning rate: 0.000874
training loss: tensor(1.4574)
         GM acc on global data: 0.1347 length of data: 10000
-------------Round number:  8  -------------
loss: CE learning rate: 0.000856
training loss: tensor(0.1515)
loss: CE learning rate: 0.000856
training loss: tensor(1.6088)
loss: CE learning rate: 0.000856
training loss: tensor(0.3894)
loss: CE learning rate: 0.000856
training loss: tensor(0.4624)
loss: CE learning rate: 0.000856
training loss: tensor(0.1132)
loss: CE learning rate: 0.000856
training loss: tensor(0.6395)
loss: CE learning rate: 0.000856
training loss: tensor(0.2887)
loss: CE learning rate: 0.000856
training loss: tensor(0.0599)
loss: CE learning rate: 0.000856
training loss: tensor(0.4657)
loss: CE learning rate: 0.000856
training loss: tensor(0.4216)
         GM acc on global data: 0.1165 length of data: 10000
-------------Round number:  9  -------------
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.2057)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.4595)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.3191)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.0993)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.6327)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.1857)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(1.6902)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.6263)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(3.6123)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(2.2679)
         GM acc on global data: 0.1007 length of data: 10000
-------------Round number:  10  -------------
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.8143)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3697)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(1.3226)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(1.6649)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.6743)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(3.0071)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.9620)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.5348)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(1.4324)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.1420)
         GM acc on global data: 0.1069 length of data: 10000
-------------Round number:  11  -------------
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3316)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.2139)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.5389)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.1047)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.0214)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.5891)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.2131)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.0768)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.6823)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.4992)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  12  -------------
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.3261)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.2975)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.4477)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.8045)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.2840)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.3660)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.6882)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(1.2118)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.8866)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(2.0898)
         GM acc on global data: 0.1023 length of data: 10000
-------------Round number:  13  -------------
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.6613)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.2983)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3260)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.4125)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3396)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.6052)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.4548)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3913)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3828)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.2362)
         GM acc on global data: 0.1699 length of data: 10000
-------------Round number:  14  -------------
loss: CE learning rate: 0.000748
training loss: tensor(0.0733)
loss: CE learning rate: 0.000748
training loss: tensor(0.1433)
loss: CE learning rate: 0.000748
training loss: tensor(0.3396)
loss: CE learning rate: 0.000748
training loss: tensor(0.2460)
loss: CE learning rate: 0.000748
training loss: tensor(0.0310)
loss: CE learning rate: 0.000748
training loss: tensor(0.7146)
loss: CE learning rate: 0.000748
training loss: tensor(1.5462)
loss: CE learning rate: 0.000748
training loss: tensor(0.3380)
loss: CE learning rate: 0.000748
training loss: tensor(0.2510)
loss: CE learning rate: 0.000748
training loss: tensor(0.6786)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  15  -------------
loss: CE learning rate: 0.00073
training loss: tensor(1.8455)
loss: CE learning rate: 0.00073
training loss: tensor(0.5459)
loss: CE learning rate: 0.00073
training loss: tensor(0.7284)
loss: CE learning rate: 0.00073
training loss: tensor(0.9618)
loss: CE learning rate: 0.00073
training loss: tensor(0.4421)
loss: CE learning rate: 0.00073
training loss: tensor(1.2285)
loss: CE learning rate: 0.00073
training loss: tensor(0.7687)
loss: CE learning rate: 0.00073
training loss: tensor(0.8632)
loss: CE learning rate: 0.00073
training loss: tensor(1.3482)
loss: CE learning rate: 0.00073
training loss: tensor(1.9851)
         GM acc on global data: 0.2763 length of data: 10000
save a model
-------------Round number:  16  -------------
loss: CE learning rate: 0.000712
training loss: tensor(0.6708)
loss: CE learning rate: 0.000712
training loss: tensor(0.2677)
loss: CE learning rate: 0.000712
training loss: tensor(0.3465)
loss: CE learning rate: 0.000712
training loss: tensor(0.3909)
loss: CE learning rate: 0.000712
training loss: tensor(0.0865)
loss: CE learning rate: 0.000712
training loss: tensor(0.0745)
loss: CE learning rate: 0.000712
training loss: tensor(0.4910)
loss: CE learning rate: 0.000712
training loss: tensor(0.6272)
loss: CE learning rate: 0.000712
training loss: tensor(1.5442)
loss: CE learning rate: 0.000712
training loss: tensor(0.8264)
         GM acc on global data: 0.2268 length of data: 10000
-------------Round number:  17  -------------
loss: CE learning rate: 0.000694
training loss: tensor(0.1434)
loss: CE learning rate: 0.000694
training loss: tensor(0.2329)
loss: CE learning rate: 0.000694
training loss: tensor(0.4385)
loss: CE learning rate: 0.000694
training loss: tensor(0.3084)
loss: CE learning rate: 0.000694
training loss: tensor(0.6979)
loss: CE learning rate: 0.000694
training loss: tensor(0.2093)
loss: CE learning rate: 0.000694
training loss: tensor(0.2178)
loss: CE learning rate: 0.000694
training loss: tensor(0.1894)
loss: CE learning rate: 0.000694
training loss: tensor(0.0356)
loss: CE learning rate: 0.000694
training loss: tensor(0.5204)
         GM acc on global data: 0.1523 length of data: 10000
-------------Round number:  18  -------------
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.4838)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.1297)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(1.0582)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.7406)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.4187)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.4222)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(2.5702)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.7784)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3372)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3579)
         GM acc on global data: 0.1885 length of data: 10000
-------------Round number:  19  -------------
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3686)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.0907)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.1290)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.4727)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.4023)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(1.1946)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.0585)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.9700)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.2801)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3282)
         GM acc on global data: 0.1321 length of data: 10000
-------------Round number:  20  -------------
loss: CE learning rate: 0.00064
training loss: tensor(0.9095)
loss: CE learning rate: 0.00064
training loss: tensor(0.5302)
loss: CE learning rate: 0.00064
training loss: tensor(0.1937)
loss: CE learning rate: 0.00064
training loss: tensor(0.2058)
loss: CE learning rate: 0.00064
training loss: tensor(0.2249)
loss: CE learning rate: 0.00064
training loss: tensor(0.3700)
loss: CE learning rate: 0.00064
training loss: tensor(0.9000)
loss: CE learning rate: 0.00064
training loss: tensor(0.1584)
loss: CE learning rate: 0.00064
training loss: tensor(1.0614)
loss: CE learning rate: 0.00064
training loss: tensor(0.2851)
         GM acc on global data: 0.2026 length of data: 10000
-------------Round number:  21  -------------
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(2.0972)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.0171)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.5639)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.1199)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.6700)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.3917)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.4271)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.4246)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.3587)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.7674)
         GM acc on global data: 0.1715 length of data: 10000
-------------Round number:  22  -------------
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.3157)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2129)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.9147)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.3936)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.5881)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.5595)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(1.3732)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.5985)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.3234)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2953)
         GM acc on global data: 0.2492 length of data: 10000
-------------Round number:  23  -------------
loss: CE learning rate: 0.000586
training loss: tensor(1.5801)
loss: CE learning rate: 0.000586
training loss: tensor(0.2133)
loss: CE learning rate: 0.000586
training loss: tensor(1.2769)
loss: CE learning rate: 0.000586
training loss: tensor(0.1680)
loss: CE learning rate: 0.000586
training loss: tensor(0.3085)
loss: CE learning rate: 0.000586
training loss: tensor(0.7593)
loss: CE learning rate: 0.000586
training loss: tensor(1.0854)
loss: CE learning rate: 0.000586
training loss: tensor(0.1101)
loss: CE learning rate: 0.000586
training loss: tensor(1.3622)
loss: CE learning rate: 0.000586
training loss: tensor(0.2057)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  24  -------------
loss: CE learning rate: 0.000568
training loss: tensor(2.1038)
loss: CE learning rate: 0.000568
training loss: tensor(0.8117)
loss: CE learning rate: 0.000568
training loss: tensor(0.7915)
loss: CE learning rate: 0.000568
training loss: tensor(0.5109)
loss: CE learning rate: 0.000568
training loss: tensor(0.2056)
loss: CE learning rate: 0.000568
training loss: tensor(0.7231)
loss: CE learning rate: 0.000568
training loss: tensor(0.9120)
loss: CE learning rate: 0.000568
training loss: tensor(0.9717)
loss: CE learning rate: 0.000568
training loss: tensor(1.1887)
loss: CE learning rate: 0.000568
training loss: tensor(0.4132)
         GM acc on global data: 0.1691 length of data: 10000
-------------Round number:  25  -------------
loss: CE learning rate: 0.00055
training loss: tensor(1.0156)
loss: CE learning rate: 0.00055
training loss: tensor(1.5995)
loss: CE learning rate: 0.00055
training loss: tensor(1.1914)
loss: CE learning rate: 0.00055
training loss: tensor(2.7501)
loss: CE learning rate: 0.00055
training loss: tensor(3.2553)
loss: CE learning rate: 0.00055
training loss: tensor(1.5384)
loss: CE learning rate: 0.00055
training loss: tensor(1.1175)
loss: CE learning rate: 0.00055
training loss: tensor(1.5453)
loss: CE learning rate: 0.00055
training loss: tensor(2.1245)
loss: CE learning rate: 0.00055
training loss: tensor(4.2609)
         GM acc on global data: 0.1537 length of data: 10000
-------------Round number:  26  -------------
loss: CE learning rate: 0.000532
training loss: tensor(0.5694)
loss: CE learning rate: 0.000532
training loss: tensor(0.4075)
loss: CE learning rate: 0.000532
training loss: tensor(0.5630)
loss: CE learning rate: 0.000532
training loss: tensor(0.0033)
loss: CE learning rate: 0.000532
training loss: tensor(0.7618)
loss: CE learning rate: 0.000532
training loss: tensor(0.2606)
loss: CE learning rate: 0.000532
training loss: tensor(0.7467)
loss: CE learning rate: 0.000532
training loss: tensor(4.7490)
loss: CE learning rate: 0.000532
training loss: tensor(0.2146)
loss: CE learning rate: 0.000532
training loss: tensor(1.2176)
         GM acc on global data: 0.2081 length of data: 10000
-------------Round number:  27  -------------
loss: CE learning rate: 0.000514
training loss: tensor(0.3496)
loss: CE learning rate: 0.000514
training loss: tensor(0.1977)
loss: CE learning rate: 0.000514
training loss: tensor(0.2736)
loss: CE learning rate: 0.000514
training loss: tensor(0.8066)
loss: CE learning rate: 0.000514
training loss: tensor(0.1873)
loss: CE learning rate: 0.000514
training loss: tensor(0.9751)
loss: CE learning rate: 0.000514
training loss: tensor(0.3348)
loss: CE learning rate: 0.000514
training loss: tensor(0.4264)
loss: CE learning rate: 0.000514
training loss: tensor(0.2407)
loss: CE learning rate: 0.000514
training loss: tensor(0.2138)
         GM acc on global data: 0.1824 length of data: 10000
-------------Round number:  28  -------------
loss: CE learning rate: 0.000496
training loss: tensor(0.1080)
loss: CE learning rate: 0.000496
training loss: tensor(0.2851)
loss: CE learning rate: 0.000496
training loss: tensor(0.0483)
loss: CE learning rate: 0.000496
training loss: tensor(0.1736)
loss: CE learning rate: 0.000496
training loss: tensor(0.4242)
loss: CE learning rate: 0.000496
training loss: tensor(0.2027)
loss: CE learning rate: 0.000496
training loss: tensor(0.7955)
loss: CE learning rate: 0.000496
training loss: tensor(0.1308)
loss: CE learning rate: 0.000496
training loss: tensor(0.1148)
loss: CE learning rate: 0.000496
training loss: tensor(0.4956)
         GM acc on global data: 0.1738 length of data: 10000
-------------Round number:  29  -------------
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.7785)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.1437)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2813)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.1433)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.3181)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2780)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2585)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2012)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.6405)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(1.0814)
         GM acc on global data: 0.149 length of data: 10000
-------------Round number:  30  -------------
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.4376)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2131)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.1191)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.1011)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.0733)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2534)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.3775)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.6622)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2542)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2464)
         GM acc on global data: 0.1838 length of data: 10000
-------------Round number:  31  -------------
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2458)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.3358)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.1470)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(1.9810)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.3091)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(1.5409)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.8059)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.0901)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.9372)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2966)
         GM acc on global data: 0.155 length of data: 10000
-------------Round number:  32  -------------
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.1844)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.2348)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(1.0771)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(4.7487)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.1931)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.3552)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.6657)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.8076)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.1467)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.1802)
         GM acc on global data: 0.1747 length of data: 10000
-------------Round number:  33  -------------
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.1960)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2747)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(2.5811)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(1.2448)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.4740)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.1493)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2886)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.1286)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.1942)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(1.1637)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  34  -------------
loss: CE learning rate: 0.000388
training loss: tensor(0.5096)
loss: CE learning rate: 0.000388
training loss: tensor(0.1015)
loss: CE learning rate: 0.000388
training loss: tensor(0.5890)
loss: CE learning rate: 0.000388
training loss: tensor(0.7707)
loss: CE learning rate: 0.000388
training loss: tensor(0.7789)
loss: CE learning rate: 0.000388
training loss: tensor(0.7712)
loss: CE learning rate: 0.000388
training loss: tensor(0.2556)
loss: CE learning rate: 0.000388
training loss: tensor(0.6090)
loss: CE learning rate: 0.000388
training loss: tensor(1.3653)
loss: CE learning rate: 0.000388
training loss: tensor(0.7646)
         GM acc on global data: 0.1629 length of data: 10000
-------------Round number:  35  -------------
loss: CE learning rate: 0.00037
training loss: tensor(0.5554)
loss: CE learning rate: 0.00037
training loss: tensor(0.9400)
loss: CE learning rate: 0.00037
training loss: tensor(1.8951)
loss: CE learning rate: 0.00037
training loss: tensor(1.1717)
loss: CE learning rate: 0.00037
training loss: tensor(1.1544)
loss: CE learning rate: 0.00037
training loss: tensor(0.6820)
loss: CE learning rate: 0.00037
training loss: tensor(0.4757)
loss: CE learning rate: 0.00037
training loss: tensor(0.0815)
loss: CE learning rate: 0.00037
training loss: tensor(0.5495)
loss: CE learning rate: 0.00037
training loss: tensor(0.8423)
         GM acc on global data: 0.208 length of data: 10000
-------------Round number:  36  -------------
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.3718)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.3076)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.3183)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.2209)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.2453)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.0592)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.0821)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.0635)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.0845)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.3295)
         GM acc on global data: 0.1037 length of data: 10000
-------------Round number:  37  -------------
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.1745)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2175)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2995)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.3325)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.3243)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.1237)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.1691)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.3689)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2082)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2109)
         GM acc on global data: 0.1536 length of data: 10000
-------------Round number:  38  -------------
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.2708)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.6329)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.0721)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.6795)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.2254)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.1406)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.2965)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.1048)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.1404)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.3482)
         GM acc on global data: 0.2522 length of data: 10000
-------------Round number:  39  -------------
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2587)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.5531)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.5562)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.1259)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.8626)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.0393)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.0338)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.3337)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.3327)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.0867)
         GM acc on global data: 0.1007 length of data: 10000
-------------Round number:  40  -------------
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.4837)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.1468)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.6503)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(1.0914)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(1.2293)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(1.1570)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.5662)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(1.8781)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.2245)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.8900)
         GM acc on global data: 0.1009 length of data: 10000
-------------Round number:  41  -------------
loss: CE learning rate: 0.000262
training loss: tensor(0.6317)
loss: CE learning rate: 0.000262
training loss: tensor(0.0501)
loss: CE learning rate: 0.000262
training loss: tensor(1.0052)
loss: CE learning rate: 0.000262
training loss: tensor(0.0440)
loss: CE learning rate: 0.000262
training loss: tensor(0.5508)
loss: CE learning rate: 0.000262
training loss: tensor(0.7318)
loss: CE learning rate: 0.000262
training loss: tensor(0.0880)
loss: CE learning rate: 0.000262
training loss: tensor(1.6623)
loss: CE learning rate: 0.000262
training loss: tensor(0.2972)
loss: CE learning rate: 0.000262
training loss: tensor(0.3725)
         GM acc on global data: 0.1706 length of data: 10000
-------------Round number:  42  -------------
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.1858)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.7577)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.7648)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(1.1493)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.2891)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.0702)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.3942)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.6543)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.3540)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(1.7107)
         GM acc on global data: 0.2117 length of data: 10000
-------------Round number:  43  -------------
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(1.2418)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.4649)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.9902)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2026)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2567)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.0869)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.4364)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.1857)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.9227)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.7608)
         GM acc on global data: 0.2078 length of data: 10000
-------------Round number:  44  -------------
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.5089)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.6261)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.3215)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.3626)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.1078)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.5000)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.8792)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.7309)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.0411)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.4722)
         GM acc on global data: 0.1401 length of data: 10000
-------------Round number:  45  -------------
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(1.7731)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.5335)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.9754)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.1569)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(1.8310)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(1.1127)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.8420)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(1.3304)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.5149)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(1.4185)
         GM acc on global data: 0.1816 length of data: 10000
-------------Round number:  46  -------------
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(2.0396)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.7328)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(2.2788)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.9851)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.5335)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.4088)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.2698)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(2.4413)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(1.0567)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(7.3385)
         GM acc on global data: 0.2286 length of data: 10000
-------------Round number:  47  -------------
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.1338)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.2291)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(1.8792)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.2771)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.4938)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.2919)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.8680)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.5132)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.3277)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.3485)
         GM acc on global data: 0.1556 length of data: 10000
-------------Round number:  48  -------------
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.3959)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2480)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.9224)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.4784)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.4106)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.3534)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2298)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.1712)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.1588)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2017)
         GM acc on global data: 0.2377 length of data: 10000
-------------Round number:  49  -------------
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.0790)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.2512)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.1526)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.1004)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.3845)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.1675)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.2232)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.2024)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.1022)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.7997)
         GM acc on global data: 0.1035 length of data: 10000
-------------Round number:  50  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.5487)
loss: CE learning rate: 0.0001
training loss: tensor(0.3291)
loss: CE learning rate: 0.0001
training loss: tensor(0.3437)
loss: CE learning rate: 0.0001
training loss: tensor(0.6446)
loss: CE learning rate: 0.0001
training loss: tensor(0.1442)
loss: CE learning rate: 0.0001
training loss: tensor(0.3923)
loss: CE learning rate: 0.0001
training loss: tensor(0.5068)
loss: CE learning rate: 0.0001
training loss: tensor(0.2523)
loss: CE learning rate: 0.0001
training loss: tensor(0.2169)
loss: CE learning rate: 0.0001
training loss: tensor(0.1987)
         GM acc on global data: 0.2683 length of data: 10000
-------------Round number:  51  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.5178)
loss: CE learning rate: 0.0001
training loss: tensor(0.4316)
loss: CE learning rate: 0.0001
training loss: tensor(0.7972)
loss: CE learning rate: 0.0001
training loss: tensor(0.1682)
loss: CE learning rate: 0.0001
training loss: tensor(0.4714)
loss: CE learning rate: 0.0001
training loss: tensor(0.5162)
loss: CE learning rate: 0.0001
training loss: tensor(0.2671)
loss: CE learning rate: 0.0001
training loss: tensor(0.6112)
loss: CE learning rate: 0.0001
training loss: tensor(0.2742)
loss: CE learning rate: 0.0001
training loss: tensor(0.0945)
         GM acc on global data: 0.1069 length of data: 10000
-------------Round number:  52  -------------
loss: CE learning rate: 0.0001
training loss: tensor(2.2936)
loss: CE learning rate: 0.0001
training loss: tensor(0.3190)
loss: CE learning rate: 0.0001
training loss: tensor(0.3576)
loss: CE learning rate: 0.0001
training loss: tensor(0.5973)
loss: CE learning rate: 0.0001
training loss: tensor(0.4935)
loss: CE learning rate: 0.0001
training loss: tensor(1.6051)
loss: CE learning rate: 0.0001
training loss: tensor(1.5986)
loss: CE learning rate: 0.0001
training loss: tensor(0.6749)
loss: CE learning rate: 0.0001
training loss: tensor(0.4795)
loss: CE learning rate: 0.0001
training loss: tensor(0.4298)
         GM acc on global data: 0.2426 length of data: 10000
-------------Round number:  53  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2410)
loss: CE learning rate: 0.0001
training loss: tensor(0.1803)
loss: CE learning rate: 0.0001
training loss: tensor(0.4884)
loss: CE learning rate: 0.0001
training loss: tensor(0.6250)
loss: CE learning rate: 0.0001
training loss: tensor(0.4799)
loss: CE learning rate: 0.0001
training loss: tensor(1.0255)
loss: CE learning rate: 0.0001
training loss: tensor(0.1584)
loss: CE learning rate: 0.0001
training loss: tensor(0.6159)
loss: CE learning rate: 0.0001
training loss: tensor(0.0716)
loss: CE learning rate: 0.0001
training loss: tensor(0.4166)
         GM acc on global data: 0.1009 length of data: 10000
-------------Round number:  54  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3322)
loss: CE learning rate: 0.0001
training loss: tensor(1.5236)
loss: CE learning rate: 0.0001
training loss: tensor(1.0186)
loss: CE learning rate: 0.0001
training loss: tensor(1.6119)
loss: CE learning rate: 0.0001
training loss: tensor(0.2460)
loss: CE learning rate: 0.0001
training loss: tensor(0.4640)
loss: CE learning rate: 0.0001
training loss: tensor(0.0326)
loss: CE learning rate: 0.0001
training loss: tensor(0.3901)
loss: CE learning rate: 0.0001
training loss: tensor(0.2076)
loss: CE learning rate: 0.0001
training loss: tensor(0.5428)
         GM acc on global data: 0.1071 length of data: 10000
-------------Round number:  55  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1619)
loss: CE learning rate: 0.0001
training loss: tensor(0.3060)
loss: CE learning rate: 0.0001
training loss: tensor(0.2213)
loss: CE learning rate: 0.0001
training loss: tensor(0.3285)
loss: CE learning rate: 0.0001
training loss: tensor(0.6913)
loss: CE learning rate: 0.0001
training loss: tensor(0.0336)
loss: CE learning rate: 0.0001
training loss: tensor(0.1357)
loss: CE learning rate: 0.0001
training loss: tensor(0.1020)
loss: CE learning rate: 0.0001
training loss: tensor(0.2652)
loss: CE learning rate: 0.0001
training loss: tensor(0.3209)
         GM acc on global data: 0.2178 length of data: 10000
-------------Round number:  56  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.5679)
loss: CE learning rate: 0.0001
training loss: tensor(0.6340)
loss: CE learning rate: 0.0001
training loss: tensor(1.8346)
loss: CE learning rate: 0.0001
training loss: tensor(0.2483)
loss: CE learning rate: 0.0001
training loss: tensor(0.2164)
loss: CE learning rate: 0.0001
training loss: tensor(0.4912)
loss: CE learning rate: 0.0001
training loss: tensor(0.5897)
loss: CE learning rate: 0.0001
training loss: tensor(1.1997)
loss: CE learning rate: 0.0001
training loss: tensor(1.2813)
loss: CE learning rate: 0.0001
training loss: tensor(0.1653)
         GM acc on global data: 0.1033 length of data: 10000
-------------Round number:  57  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0348)
loss: CE learning rate: 0.0001
training loss: tensor(0.4817)
loss: CE learning rate: 0.0001
training loss: tensor(0.8446)
loss: CE learning rate: 0.0001
training loss: tensor(4.1612)
loss: CE learning rate: 0.0001
training loss: tensor(0.3657)
loss: CE learning rate: 0.0001
training loss: tensor(1.9942)
loss: CE learning rate: 0.0001
training loss: tensor(0.2977)
loss: CE learning rate: 0.0001
training loss: tensor(1.7307)
loss: CE learning rate: 0.0001
training loss: tensor(0.7784)
loss: CE learning rate: 0.0001
training loss: tensor(4.0092)
         GM acc on global data: 0.1393 length of data: 10000
-------------Round number:  58  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.5093)
loss: CE learning rate: 0.0001
training loss: tensor(0.1601)
loss: CE learning rate: 0.0001
training loss: tensor(0.0466)
loss: CE learning rate: 0.0001
training loss: tensor(0.5334)
loss: CE learning rate: 0.0001
training loss: tensor(2.0851)
loss: CE learning rate: 0.0001
training loss: tensor(1.5700)
loss: CE learning rate: 0.0001
training loss: tensor(0.3427)
loss: CE learning rate: 0.0001
training loss: tensor(0.5216)
loss: CE learning rate: 0.0001
training loss: tensor(1.4783)
loss: CE learning rate: 0.0001
training loss: tensor(0.6388)
         GM acc on global data: 0.1436 length of data: 10000
-------------Round number:  59  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.7270)
loss: CE learning rate: 0.0001
training loss: tensor(0.4198)
loss: CE learning rate: 0.0001
training loss: tensor(0.2561)
loss: CE learning rate: 0.0001
training loss: tensor(1.7846)
loss: CE learning rate: 0.0001
training loss: tensor(1.3626)
loss: CE learning rate: 0.0001
training loss: tensor(0.3409)
loss: CE learning rate: 0.0001
training loss: tensor(0.1676)
loss: CE learning rate: 0.0001
training loss: tensor(0.1229)
loss: CE learning rate: 0.0001
training loss: tensor(0.1133)
loss: CE learning rate: 0.0001
training loss: tensor(0.6468)
         GM acc on global data: 0.1727 length of data: 10000
-------------Round number:  60  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.7687)
loss: CE learning rate: 0.0001
training loss: tensor(0.6304)
loss: CE learning rate: 0.0001
training loss: tensor(0.2881)
loss: CE learning rate: 0.0001
training loss: tensor(0.7105)
loss: CE learning rate: 0.0001
training loss: tensor(0.0951)
loss: CE learning rate: 0.0001
training loss: tensor(0.5662)
loss: CE learning rate: 0.0001
training loss: tensor(0.4625)
loss: CE learning rate: 0.0001
training loss: tensor(0.1841)
loss: CE learning rate: 0.0001
training loss: tensor(2.7326)
loss: CE learning rate: 0.0001
training loss: tensor(0.3248)
         GM acc on global data: 0.1734 length of data: 10000
-------------Round number:  61  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.5244)
loss: CE learning rate: 0.0001
training loss: tensor(1.4581)
loss: CE learning rate: 0.0001
training loss: tensor(0.0002)
loss: CE learning rate: 0.0001
training loss: tensor(1.7058)
loss: CE learning rate: 0.0001
training loss: tensor(0.9147)
loss: CE learning rate: 0.0001
training loss: tensor(0.1313)
loss: CE learning rate: 0.0001
training loss: tensor(0.1472)
loss: CE learning rate: 0.0001
training loss: tensor(0.2975)
loss: CE learning rate: 0.0001
training loss: tensor(0.4949)
loss: CE learning rate: 0.0001
training loss: tensor(0.4712)
         GM acc on global data: 0.1011 length of data: 10000
-------------Round number:  62  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1672)
loss: CE learning rate: 0.0001
training loss: tensor(0.8507)
loss: CE learning rate: 0.0001
training loss: tensor(0.0114)
loss: CE learning rate: 0.0001
training loss: tensor(0.9448)
loss: CE learning rate: 0.0001
training loss: tensor(0.1816)
loss: CE learning rate: 0.0001
training loss: tensor(0.9061)
loss: CE learning rate: 0.0001
training loss: tensor(1.5376)
loss: CE learning rate: 0.0001
training loss: tensor(0.5043)
loss: CE learning rate: 0.0001
training loss: tensor(0.2359)
loss: CE learning rate: 0.0001
training loss: tensor(0.0949)
         GM acc on global data: 0.126 length of data: 10000
-------------Round number:  63  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.5177)
loss: CE learning rate: 0.0001
training loss: tensor(1.3322)
loss: CE learning rate: 0.0001
training loss: tensor(0.4126)
loss: CE learning rate: 0.0001
training loss: tensor(0.2455)
loss: CE learning rate: 0.0001
training loss: tensor(0.4337)
loss: CE learning rate: 0.0001
training loss: tensor(1.6790)
loss: CE learning rate: 0.0001
training loss: tensor(0.1464)
loss: CE learning rate: 0.0001
training loss: tensor(0.5950)
loss: CE learning rate: 0.0001
training loss: tensor(0.4399)
loss: CE learning rate: 0.0001
training loss: tensor(0.2644)
         GM acc on global data: 0.2746 length of data: 10000
-------------Round number:  64  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1029)
loss: CE learning rate: 0.0001
training loss: tensor(0.0864)
loss: CE learning rate: 0.0001
training loss: tensor(0.5756)
loss: CE learning rate: 0.0001
training loss: tensor(0.2737)
loss: CE learning rate: 0.0001
training loss: tensor(0.8907)
loss: CE learning rate: 0.0001
training loss: tensor(0.2563)
loss: CE learning rate: 0.0001
training loss: tensor(0.1757)
loss: CE learning rate: 0.0001
training loss: tensor(0.7552)
loss: CE learning rate: 0.0001
training loss: tensor(0.2842)
loss: CE learning rate: 0.0001
training loss: tensor(0.0929)
         GM acc on global data: 0.2737 length of data: 10000
-------------Round number:  65  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.7224)
loss: CE learning rate: 0.0001
training loss: tensor(0.0420)
loss: CE learning rate: 0.0001
training loss: tensor(0.1271)
loss: CE learning rate: 0.0001
training loss: tensor(0.3552)
loss: CE learning rate: 0.0001
training loss: tensor(0.0311)
loss: CE learning rate: 0.0001
training loss: tensor(0.2326)
loss: CE learning rate: 0.0001
training loss: tensor(0.1551)
loss: CE learning rate: 0.0001
training loss: tensor(0.3977)
loss: CE learning rate: 0.0001
training loss: tensor(0.1487)
loss: CE learning rate: 0.0001
training loss: tensor(0.1588)
         GM acc on global data: 0.1099 length of data: 10000
-------------Round number:  66  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1323)
loss: CE learning rate: 0.0001
training loss: tensor(0.2839)
loss: CE learning rate: 0.0001
training loss: tensor(0.8082)
loss: CE learning rate: 0.0001
training loss: tensor(0.0524)
loss: CE learning rate: 0.0001
training loss: tensor(0.1450)
loss: CE learning rate: 0.0001
training loss: tensor(0.3848)
loss: CE learning rate: 0.0001
training loss: tensor(0.0043)
loss: CE learning rate: 0.0001
training loss: tensor(0.3586)
loss: CE learning rate: 0.0001
training loss: tensor(0.2880)
loss: CE learning rate: 0.0001
training loss: tensor(0.1649)
         GM acc on global data: 0.181 length of data: 10000
-------------Round number:  67  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1657)
loss: CE learning rate: 0.0001
training loss: tensor(0.1525)
loss: CE learning rate: 0.0001
training loss: tensor(0.6565)
loss: CE learning rate: 0.0001
training loss: tensor(0.2492)
loss: CE learning rate: 0.0001
training loss: tensor(3.5171)
loss: CE learning rate: 0.0001
training loss: tensor(0.0404)
loss: CE learning rate: 0.0001
training loss: tensor(2.0335)
loss: CE learning rate: 0.0001
training loss: tensor(2.2709)
loss: CE learning rate: 0.0001
training loss: tensor(0.1852)
loss: CE learning rate: 0.0001
training loss: tensor(0.5904)
         GM acc on global data: 0.1091 length of data: 10000
-------------Round number:  68  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0344)
loss: CE learning rate: 0.0001
training loss: tensor(1.0315)
loss: CE learning rate: 0.0001
training loss: tensor(0.2701)
loss: CE learning rate: 0.0001
training loss: tensor(0.5030)
loss: CE learning rate: 0.0001
training loss: tensor(0.8706)
loss: CE learning rate: 0.0001
training loss: tensor(0.2078)
loss: CE learning rate: 0.0001
training loss: tensor(0.1168)
loss: CE learning rate: 0.0001
training loss: tensor(2.6268)
loss: CE learning rate: 0.0001
training loss: tensor(0.7651)
loss: CE learning rate: 0.0001
training loss: tensor(2.6518)
         GM acc on global data: 0.1083 length of data: 10000
-------------Round number:  69  -------------
loss: CE learning rate: 0.0001
training loss: tensor(1.6140)
loss: CE learning rate: 0.0001
training loss: tensor(0.9118)
loss: CE learning rate: 0.0001
training loss: tensor(1.3543)
loss: CE learning rate: 0.0001
training loss: tensor(1.1715)
loss: CE learning rate: 0.0001
training loss: tensor(0.6082)
loss: CE learning rate: 0.0001
training loss: tensor(0.3788)
loss: CE learning rate: 0.0001
training loss: tensor(0.4546)
loss: CE learning rate: 0.0001
training loss: tensor(0.8218)
loss: CE learning rate: 0.0001
training loss: tensor(1.4214)
loss: CE learning rate: 0.0001
training loss: tensor(5.4174)
         GM acc on global data: 0.1793 length of data: 10000
-------------Round number:  70  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1878)
loss: CE learning rate: 0.0001
training loss: tensor(0.1470)
loss: CE learning rate: 0.0001
training loss: tensor(0.3858)
loss: CE learning rate: 0.0001
training loss: tensor(0.7196)
loss: CE learning rate: 0.0001
training loss: tensor(0.3716)
loss: CE learning rate: 0.0001
training loss: tensor(0.2052)
loss: CE learning rate: 0.0001
training loss: tensor(0.5286)
loss: CE learning rate: 0.0001
training loss: tensor(0.1014)
loss: CE learning rate: 0.0001
training loss: tensor(0.5496)
loss: CE learning rate: 0.0001
training loss: tensor(0.2427)
         GM acc on global data: 0.1039 length of data: 10000
-------------Round number:  71  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4186)
loss: CE learning rate: 0.0001
training loss: tensor(1.0172)
loss: CE learning rate: 0.0001
training loss: tensor(0.1703)
loss: CE learning rate: 0.0001
training loss: tensor(0.8668)
loss: CE learning rate: 0.0001
training loss: tensor(0.1457)
loss: CE learning rate: 0.0001
training loss: tensor(0.4404)
loss: CE learning rate: 0.0001
training loss: tensor(1.2817)
loss: CE learning rate: 0.0001
training loss: tensor(0.2704)
loss: CE learning rate: 0.0001
training loss: tensor(0.2335)
loss: CE learning rate: 0.0001
training loss: tensor(0.2698)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  72  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1043)
loss: CE learning rate: 0.0001
training loss: tensor(0.9606)
loss: CE learning rate: 0.0001
training loss: tensor(0.6577)
loss: CE learning rate: 0.0001
training loss: tensor(0.2102)
loss: CE learning rate: 0.0001
training loss: tensor(0.1032)
loss: CE learning rate: 0.0001
training loss: tensor(1.9891)
loss: CE learning rate: 0.0001
training loss: tensor(0.5706)
loss: CE learning rate: 0.0001
training loss: tensor(0.2017)
loss: CE learning rate: 0.0001
training loss: tensor(0.7745)
loss: CE learning rate: 0.0001
training loss: tensor(0.3709)
         GM acc on global data: 0.1126 length of data: 10000
-------------Round number:  73  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3220)
loss: CE learning rate: 0.0001
training loss: tensor(0.7167)
loss: CE learning rate: 0.0001
training loss: tensor(0.1932)
loss: CE learning rate: 0.0001
training loss: tensor(2.1150)
loss: CE learning rate: 0.0001
training loss: tensor(0.2527)
loss: CE learning rate: 0.0001
training loss: tensor(0.7918)
loss: CE learning rate: 0.0001
training loss: tensor(0.2119)
loss: CE learning rate: 0.0001
training loss: tensor(0.0228)
loss: CE learning rate: 0.0001
training loss: tensor(0.8684)
loss: CE learning rate: 0.0001
training loss: tensor(2.0375)
         GM acc on global data: 0.1116 length of data: 10000
-------------Round number:  74  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3650)
loss: CE learning rate: 0.0001
training loss: tensor(0.8859)
loss: CE learning rate: 0.0001
training loss: tensor(0.3024)
loss: CE learning rate: 0.0001
training loss: tensor(0.6838)
loss: CE learning rate: 0.0001
training loss: tensor(0.5565)
loss: CE learning rate: 0.0001
training loss: tensor(2.0555)
loss: CE learning rate: 0.0001
training loss: tensor(0.1282)
loss: CE learning rate: 0.0001
training loss: tensor(0.3067)
loss: CE learning rate: 0.0001
training loss: tensor(2.2865)
loss: CE learning rate: 0.0001
training loss: tensor(0.1553)
         GM acc on global data: 0.1783 length of data: 10000
-------------Round number:  75  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1187)
loss: CE learning rate: 0.0001
training loss: tensor(1.4133)
loss: CE learning rate: 0.0001
training loss: tensor(0.1550)
loss: CE learning rate: 0.0001
training loss: tensor(1.1636)
loss: CE learning rate: 0.0001
training loss: tensor(0.1632)
loss: CE learning rate: 0.0001
training loss: tensor(0.7241)
loss: CE learning rate: 0.0001
training loss: tensor(0.0701)
loss: CE learning rate: 0.0001
training loss: tensor(0.7001)
loss: CE learning rate: 0.0001
training loss: tensor(0.3007)
loss: CE learning rate: 0.0001
training loss: tensor(0.0223)
         GM acc on global data: 0.164 length of data: 10000
-------------Round number:  76  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.6244)
loss: CE learning rate: 0.0001
training loss: tensor(0.5287)
loss: CE learning rate: 0.0001
training loss: tensor(0.7766)
loss: CE learning rate: 0.0001
training loss: tensor(0.4614)
loss: CE learning rate: 0.0001
training loss: tensor(0.3409)
loss: CE learning rate: 0.0001
training loss: tensor(0.8497)
loss: CE learning rate: 0.0001
training loss: tensor(0.2181)
loss: CE learning rate: 0.0001
training loss: tensor(0.1971)
loss: CE learning rate: 0.0001
training loss: tensor(0.2479)
loss: CE learning rate: 0.0001
training loss: tensor(0.4880)
         GM acc on global data: 0.2436 length of data: 10000
-------------Round number:  77  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2634)
loss: CE learning rate: 0.0001
training loss: tensor(0.3491)
loss: CE learning rate: 0.0001
training loss: tensor(0.4367)
loss: CE learning rate: 0.0001
training loss: tensor(0.0596)
loss: CE learning rate: 0.0001
training loss: tensor(0.2760)
loss: CE learning rate: 0.0001
training loss: tensor(0.7792)
loss: CE learning rate: 0.0001
training loss: tensor(0.0398)
loss: CE learning rate: 0.0001
training loss: tensor(0.0841)
loss: CE learning rate: 0.0001
training loss: tensor(0.9271)
loss: CE learning rate: 0.0001
training loss: tensor(0.2135)
         GM acc on global data: 0.1026 length of data: 10000
-------------Round number:  78  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4881)
loss: CE learning rate: 0.0001
training loss: tensor(0.3082)
loss: CE learning rate: 0.0001
training loss: tensor(0.1070)
loss: CE learning rate: 0.0001
training loss: tensor(0.3052)
loss: CE learning rate: 0.0001
training loss: tensor(0.0537)
loss: CE learning rate: 0.0001
training loss: tensor(0.1810)
loss: CE learning rate: 0.0001
training loss: tensor(0.1999)
loss: CE learning rate: 0.0001
training loss: tensor(0.2675)
loss: CE learning rate: 0.0001
training loss: tensor(0.3321)
loss: CE learning rate: 0.0001
training loss: tensor(0.1516)
         GM acc on global data: 0.2045 length of data: 10000
-------------Round number:  79  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0554)
loss: CE learning rate: 0.0001
training loss: tensor(0.1938)
loss: CE learning rate: 0.0001
training loss: tensor(0.0789)
loss: CE learning rate: 0.0001
training loss: tensor(0.5209)
loss: CE learning rate: 0.0001
training loss: tensor(0.1465)
loss: CE learning rate: 0.0001
training loss: tensor(1.0418)
loss: CE learning rate: 0.0001
training loss: tensor(1.1180)
loss: CE learning rate: 0.0001
training loss: tensor(0.0998)
loss: CE learning rate: 0.0001
training loss: tensor(0.3608)
loss: CE learning rate: 0.0001
training loss: tensor(0.2855)
         GM acc on global data: 0.126 length of data: 10000
-------------Round number:  80  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.5754)
loss: CE learning rate: 0.0001
training loss: tensor(0.4894)
loss: CE learning rate: 0.0001
training loss: tensor(1.2429)
loss: CE learning rate: 0.0001
training loss: tensor(0.7206)
loss: CE learning rate: 0.0001
training loss: tensor(0.5457)
loss: CE learning rate: 0.0001
training loss: tensor(0.4995)
loss: CE learning rate: 0.0001
training loss: tensor(1.0823)
loss: CE learning rate: 0.0001
training loss: tensor(0.2522)
loss: CE learning rate: 0.0001
training loss: tensor(0.6576)
loss: CE learning rate: 0.0001
training loss: tensor(0.1500)
         GM acc on global data: 0.1316 length of data: 10000
-------------Round number:  81  -------------
loss: CE learning rate: 0.0001
training loss: tensor(1.4251)
loss: CE learning rate: 0.0001
training loss: tensor(0.4226)
loss: CE learning rate: 0.0001
training loss: tensor(0.4697)
loss: CE learning rate: 0.0001
training loss: tensor(0.4520)
loss: CE learning rate: 0.0001
training loss: tensor(0.5198)
loss: CE learning rate: 0.0001
training loss: tensor(0.5714)
loss: CE learning rate: 0.0001
training loss: tensor(0.7306)
loss: CE learning rate: 0.0001
training loss: tensor(0.4829)
loss: CE learning rate: 0.0001
training loss: tensor(0.0318)
loss: CE learning rate: 0.0001
training loss: tensor(0.6909)
         GM acc on global data: 0.1568 length of data: 10000
-------------Round number:  82  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.9907)
loss: CE learning rate: 0.0001
training loss: tensor(0.1719)
loss: CE learning rate: 0.0001
training loss: tensor(0.2174)
loss: CE learning rate: 0.0001
training loss: tensor(0.1459)
loss: CE learning rate: 0.0001
training loss: tensor(0.0860)
loss: CE learning rate: 0.0001
training loss: tensor(1.0112)
loss: CE learning rate: 0.0001
training loss: tensor(0.5513)
loss: CE learning rate: 0.0001
training loss: tensor(0.1537)
loss: CE learning rate: 0.0001
training loss: tensor(0.0934)
loss: CE learning rate: 0.0001
training loss: tensor(0.2190)
         GM acc on global data: 0.1355 length of data: 10000
-------------Round number:  83  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0552)
loss: CE learning rate: 0.0001
training loss: tensor(0.1608)
loss: CE learning rate: 0.0001
training loss: tensor(1.0948)
loss: CE learning rate: 0.0001
training loss: tensor(0.1392)
loss: CE learning rate: 0.0001
training loss: tensor(0.1567)
loss: CE learning rate: 0.0001
training loss: tensor(0.2190)
loss: CE learning rate: 0.0001
training loss: tensor(1.0527)
loss: CE learning rate: 0.0001
training loss: tensor(0.1125)
loss: CE learning rate: 0.0001
training loss: tensor(0.2698)
loss: CE learning rate: 0.0001
training loss: tensor(1.0261)
         GM acc on global data: 0.1983 length of data: 10000
-------------Round number:  84  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0781)
loss: CE learning rate: 0.0001
training loss: tensor(0.0440)
loss: CE learning rate: 0.0001
training loss: tensor(0.5120)
loss: CE learning rate: 0.0001
training loss: tensor(1.2592)
loss: CE learning rate: 0.0001
training loss: tensor(0.4054)
loss: CE learning rate: 0.0001
training loss: tensor(0.4535)
loss: CE learning rate: 0.0001
training loss: tensor(0.6018)
loss: CE learning rate: 0.0001
training loss: tensor(0.0535)
loss: CE learning rate: 0.0001
training loss: tensor(0.0112)
loss: CE learning rate: 0.0001
training loss: tensor(1.9301)
         GM acc on global data: 0.1005 length of data: 10000
-------------Round number:  85  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2815)
loss: CE learning rate: 0.0001
training loss: tensor(0.2574)
loss: CE learning rate: 0.0001
training loss: tensor(0.2254)
loss: CE learning rate: 0.0001
training loss: tensor(1.6006)
loss: CE learning rate: 0.0001
training loss: tensor(0.5058)
loss: CE learning rate: 0.0001
training loss: tensor(1.1158)
loss: CE learning rate: 0.0001
training loss: tensor(0.4032)
loss: CE learning rate: 0.0001
training loss: tensor(0.4363)
loss: CE learning rate: 0.0001
training loss: tensor(2.5940)
loss: CE learning rate: 0.0001
training loss: tensor(0.9523)
         GM acc on global data: 0.109 length of data: 10000
-------------Round number:  86  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1308)
loss: CE learning rate: 0.0001
training loss: tensor(0.1032)
loss: CE learning rate: 0.0001
training loss: tensor(0.0933)
loss: CE learning rate: 0.0001
training loss: tensor(0.0394)
loss: CE learning rate: 0.0001
training loss: tensor(0.5218)
loss: CE learning rate: 0.0001
training loss: tensor(1.0976)
loss: CE learning rate: 0.0001
training loss: tensor(0.6150)
loss: CE learning rate: 0.0001
training loss: tensor(0.5534)
loss: CE learning rate: 0.0001
training loss: tensor(0.0715)
loss: CE learning rate: 0.0001
training loss: tensor(1.2703)
         GM acc on global data: 0.1063 length of data: 10000
-------------Round number:  87  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3506)
loss: CE learning rate: 0.0001
training loss: tensor(0.1914)
loss: CE learning rate: 0.0001
training loss: tensor(0.2089)
loss: CE learning rate: 0.0001
training loss: tensor(1.2150)
loss: CE learning rate: 0.0001
training loss: tensor(0.2648)
loss: CE learning rate: 0.0001
training loss: tensor(0.1067)
loss: CE learning rate: 0.0001
training loss: tensor(2.6772)
loss: CE learning rate: 0.0001
training loss: tensor(0.2352)
loss: CE learning rate: 0.0001
training loss: tensor(0.1790)
loss: CE learning rate: 0.0001
training loss: tensor(1.6078)
         GM acc on global data: 0.1011 length of data: 10000
-------------Round number:  88  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.5839)
loss: CE learning rate: 0.0001
training loss: tensor(0.2974)
loss: CE learning rate: 0.0001
training loss: tensor(0.5467)
loss: CE learning rate: 0.0001
training loss: tensor(0.5554)
loss: CE learning rate: 0.0001
training loss: tensor(0.3259)
loss: CE learning rate: 0.0001
training loss: tensor(0.0858)
loss: CE learning rate: 0.0001
training loss: tensor(0.0654)
loss: CE learning rate: 0.0001
training loss: tensor(0.1625)
loss: CE learning rate: 0.0001
training loss: tensor(0.6600)
loss: CE learning rate: 0.0001
training loss: tensor(2.2441)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  89  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.8140)
loss: CE learning rate: 0.0001
training loss: tensor(0.8242)
loss: CE learning rate: 0.0001
training loss: tensor(2.0462)
loss: CE learning rate: 0.0001
training loss: tensor(0.5593)
loss: CE learning rate: 0.0001
training loss: tensor(0.4300)
loss: CE learning rate: 0.0001
training loss: tensor(0.8767)
loss: CE learning rate: 0.0001
training loss: tensor(0.4232)
loss: CE learning rate: 0.0001
training loss: tensor(1.8571)
loss: CE learning rate: 0.0001
training loss: tensor(1.3969)
loss: CE learning rate: 0.0001
training loss: tensor(0.6846)
         GM acc on global data: 0.2556 length of data: 10000
-------------Round number:  90  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1159)
loss: CE learning rate: 0.0001
training loss: tensor(0.3945)
loss: CE learning rate: 0.0001
training loss: tensor(0.0425)
loss: CE learning rate: 0.0001
training loss: tensor(0.3097)
loss: CE learning rate: 0.0001
training loss: tensor(0.1746)
loss: CE learning rate: 0.0001
training loss: tensor(0.2943)
loss: CE learning rate: 0.0001
training loss: tensor(0.1489)
loss: CE learning rate: 0.0001
training loss: tensor(0.4733)
loss: CE learning rate: 0.0001
training loss: tensor(0.0509)
loss: CE learning rate: 0.0001
training loss: tensor(0.1529)
         GM acc on global data: 0.1863 length of data: 10000
-------------Round number:  91  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0907)
loss: CE learning rate: 0.0001
training loss: tensor(0.0829)
loss: CE learning rate: 0.0001
training loss: tensor(0.3285)
loss: CE learning rate: 0.0001
training loss: tensor(0.1122)
loss: CE learning rate: 0.0001
training loss: tensor(0.2047)
loss: CE learning rate: 0.0001
training loss: tensor(0.2665)
loss: CE learning rate: 0.0001
training loss: tensor(0.8223)
loss: CE learning rate: 0.0001
training loss: tensor(1.3038)
loss: CE learning rate: 0.0001
training loss: tensor(0.1605)
loss: CE learning rate: 0.0001
training loss: tensor(2.2592)
         GM acc on global data: 0.1033 length of data: 10000
-------------Round number:  92  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3896)
loss: CE learning rate: 0.0001
training loss: tensor(0.7739)
loss: CE learning rate: 0.0001
training loss: tensor(1.6504)
loss: CE learning rate: 0.0001
training loss: tensor(0.1982)
loss: CE learning rate: 0.0001
training loss: tensor(0.6518)
loss: CE learning rate: 0.0001
training loss: tensor(0.2092)
loss: CE learning rate: 0.0001
training loss: tensor(1.0198)
loss: CE learning rate: 0.0001
training loss: tensor(0.9481)
loss: CE learning rate: 0.0001
training loss: tensor(0.3159)
loss: CE learning rate: 0.0001
training loss: tensor(0.0201)
         GM acc on global data: 0.1735 length of data: 10000
-------------Round number:  93  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0127)
loss: CE learning rate: 0.0001
training loss: tensor(1.0724)
loss: CE learning rate: 0.0001
training loss: tensor(0.2870)
loss: CE learning rate: 0.0001
training loss: tensor(0.8106)
loss: CE learning rate: 0.0001
training loss: tensor(0.7861)
loss: CE learning rate: 0.0001
training loss: tensor(0.7314)
loss: CE learning rate: 0.0001
training loss: tensor(1.3328)
loss: CE learning rate: 0.0001
training loss: tensor(0.1607)
loss: CE learning rate: 0.0001
training loss: tensor(0.4196)
loss: CE learning rate: 0.0001
training loss: tensor(0.1287)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  94  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.6306)
loss: CE learning rate: 0.0001
training loss: tensor(2.5822)
loss: CE learning rate: 0.0001
training loss: tensor(0.8056)
loss: CE learning rate: 0.0001
training loss: tensor(0.7069)
loss: CE learning rate: 0.0001
training loss: tensor(0.8035)
loss: CE learning rate: 0.0001
training loss: tensor(0.7832)
loss: CE learning rate: 0.0001
training loss: tensor(2.9440)
loss: CE learning rate: 0.0001
training loss: tensor(1.6896)
loss: CE learning rate: 0.0001
training loss: tensor(1.3106)
loss: CE learning rate: 0.0001
training loss: tensor(0.4896)
         GM acc on global data: 0.2114 length of data: 10000
-------------Round number:  95  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2989)
loss: CE learning rate: 0.0001
training loss: tensor(0.8231)
loss: CE learning rate: 0.0001
training loss: tensor(0.7869)
loss: CE learning rate: 0.0001
training loss: tensor(0.7162)
loss: CE learning rate: 0.0001
training loss: tensor(1.2968)
loss: CE learning rate: 0.0001
training loss: tensor(0.7557)
loss: CE learning rate: 0.0001
training loss: tensor(0.6662)
loss: CE learning rate: 0.0001
training loss: tensor(0.4448)
loss: CE learning rate: 0.0001
training loss: tensor(0.1219)
loss: CE learning rate: 0.0001
training loss: tensor(0.3982)
         GM acc on global data: 0.2504 length of data: 10000
-------------Round number:  96  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4413)
loss: CE learning rate: 0.0001
training loss: tensor(0.1751)
loss: CE learning rate: 0.0001
training loss: tensor(0.2309)
loss: CE learning rate: 0.0001
training loss: tensor(0.1870)
loss: CE learning rate: 0.0001
training loss: tensor(0.8441)
loss: CE learning rate: 0.0001
training loss: tensor(0.0855)
loss: CE learning rate: 0.0001
training loss: tensor(0.0282)
loss: CE learning rate: 0.0001
training loss: tensor(0.5565)
loss: CE learning rate: 0.0001
training loss: tensor(0.3601)
loss: CE learning rate: 0.0001
training loss: tensor(0.1698)
         GM acc on global data: 0.1098 length of data: 10000
-------------Round number:  97  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.5656)
loss: CE learning rate: 0.0001
training loss: tensor(0.6350)
loss: CE learning rate: 0.0001
training loss: tensor(0.4876)
loss: CE learning rate: 0.0001
training loss: tensor(0.2157)
loss: CE learning rate: 0.0001
training loss: tensor(0.5535)
loss: CE learning rate: 0.0001
training loss: tensor(0.3254)
loss: CE learning rate: 0.0001
training loss: tensor(0.0915)
loss: CE learning rate: 0.0001
training loss: tensor(0.5380)
loss: CE learning rate: 0.0001
training loss: tensor(0.1145)
loss: CE learning rate: 0.0001
training loss: tensor(1.8221)
         GM acc on global data: 0.1783 length of data: 10000
-------------Round number:  98  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.5725)
loss: CE learning rate: 0.0001
training loss: tensor(0.2288)
loss: CE learning rate: 0.0001
training loss: tensor(0.9757)
loss: CE learning rate: 0.0001
training loss: tensor(0.3372)
loss: CE learning rate: 0.0001
training loss: tensor(0.3665)
loss: CE learning rate: 0.0001
training loss: tensor(0.0363)
loss: CE learning rate: 0.0001
training loss: tensor(0.1806)
loss: CE learning rate: 0.0001
training loss: tensor(0.2536)
loss: CE learning rate: 0.0001
training loss: tensor(0.2922)
loss: CE learning rate: 0.0001
training loss: tensor(0.0499)
         GM acc on global data: 0.106 length of data: 10000
-------------Round number:  99  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3605)
loss: CE learning rate: 0.0001
training loss: tensor(0.0446)
loss: CE learning rate: 0.0001
training loss: tensor(0.9868)
loss: CE learning rate: 0.0001
training loss: tensor(0.2283)
loss: CE learning rate: 0.0001
training loss: tensor(0.0945)
loss: CE learning rate: 0.0001
training loss: tensor(0.4505)
loss: CE learning rate: 0.0001
training loss: tensor(0.1330)
loss: CE learning rate: 0.0001
training loss: tensor(0.8917)
loss: CE learning rate: 0.0001
training loss: tensor(0.4711)
loss: CE learning rate: 0.0001
training loss: tensor(0.4536)
         GM acc on global data: 0.1652 length of data: 10000
