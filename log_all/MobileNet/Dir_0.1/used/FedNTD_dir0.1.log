nohup: ignoring input
================================================================================
Summary of training process:
Dataset:                 Cifar10
Batch size:              64
Learing rate :           0.001
personal learning rate : 0.001
Number of total clients: 100
Split parameter        : 0.1
Clients per round      : 10
Number of global rounds: 100
Number of local rounds : 10
Feature from layer     : 11
Feature reduction      : 64
Local training loss    : NT_CE
Loss of beta           : 1.0
Algorithm              : FedAvg
Modelname              : MOBNET
Mode                   : training
================================================================================
Files already downloaded and verified
Files already downloaded and verified
Here
Done
Class frequencies:
client,class0,class1,class2,class3,class4,class5,class6,class7,class8,class9,Amount
Client   0,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,1.000,171
Client   1,0.231,0.038,0.00,0.00,0.00,0.077,0.00,0.654,0.00,0.00,52
Client   2,0.374,0.015,0.00,0.00,0.519,0.011,0.007,0.015,0.00,0.059,457
Client   3,0.00,0.00,0.00,0.232,0.768,0.00,0.00,0.00,0.00,0.00,56
Client   4,0.660,0.071,0.032,0.122,0.00,0.00,0.083,0.00,0.00,0.032,156
Client   5,0.00,0.00,0.00,0.00,0.029,0.00,0.00,0.00,0.121,0.850,479
Client   6,0.00,0.080,0.00,0.00,0.009,0.017,0.894,0.00,0.00,0.00,538
Client   7,0.00,0.00,0.00,0.00,0.991,0.00,0.00,0.009,0.00,0.00,110
Client   8,0.131,0.116,0.00,0.00,0.00,0.00,0.00,0.00,0.753,0.00,1059
Client   9,0.00,0.00,0.175,0.00,0.467,0.017,0.192,0.150,0.00,0.00,120
Client  10,0.00,0.331,0.540,0.00,0.00,0.00,0.129,0.00,0.00,0.00,513
Client  11,0.00,0.00,0.250,0.00,0.004,0.358,0.254,0.008,0.121,0.004,240
Client  12,0.00,0.00,0.00,0.00,0.00,0.562,0.00,0.00,0.396,0.042,48
Client  13,0.00,0.00,0.00,0.371,0.185,0.00,0.444,0.00,0.00,0.00,453
Client  14,0.068,0.00,0.003,0.011,0.00,0.042,0.641,0.234,0.00,0.00,354
Client  15,0.00,0.005,0.00,0.00,0.995,0.00,0.00,0.00,0.00,0.00,213
Client  16,1.000,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,830
Client  17,0.335,0.00,0.004,0.00,0.007,0.00,0.00,0.654,0.00,0.00,543
Client  18,0.00,0.743,0.00,0.127,0.053,0.076,0.00,0.00,0.00,0.00,526
Client  19,0.008,0.00,0.057,0.676,0.185,0.074,0.00,0.00,0.00,0.00,524
Client  20,0.00,0.00,0.243,0.108,0.00,0.00,0.00,0.216,0.432,0.00,37
Client  21,0.00,0.00,0.00,0.141,0.848,0.00,0.004,0.007,0.00,0.00,277
Client  22,0.00,0.192,0.00,0.00,0.00,0.010,0.00,0.028,0.063,0.708,718
Client  23,0.00,0.100,0.529,0.00,0.00,0.214,0.129,0.00,0.00,0.029,70
Client  24,0.014,0.00,0.002,0.002,0.00,0.00,0.00,0.982,0.00,0.00,1084
Client  25,0.00,0.00,0.006,0.105,0.00,0.169,0.010,0.025,0.684,0.00,484
Client  26,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.194,0.00,0.806,72
Client  27,0.00,0.00,0.00,0.00,0.011,0.086,0.004,0.00,0.019,0.881,269
Client  28,0.237,0.00,0.00,0.001,0.038,0.167,0.001,0.011,0.546,0.00,951
Client  29,0.00,1.000,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,845
Client  30,0.004,0.00,0.218,0.743,0.00,0.00,0.00,0.023,0.008,0.004,257
Client  31,0.014,0.730,0.00,0.005,0.194,0.005,0.052,0.00,0.00,0.00,211
Client  32,0.00,0.00,0.00,0.00,0.004,0.00,0.743,0.222,0.031,0.00,257
Client  33,0.559,0.084,0.002,0.00,0.013,0.00,0.00,0.234,0.108,0.00,546
Client  34,0.00,0.010,0.093,0.00,0.202,0.010,0.00,0.00,0.685,0.00,1363
Client  35,0.913,0.008,0.00,0.011,0.00,0.065,0.00,0.00,0.00,0.004,263
Client  36,0.029,0.00,0.001,0.00,0.139,0.006,0.001,0.003,0.821,0.00,979
Client  37,0.00,0.00,0.259,0.194,0.237,0.245,0.029,0.00,0.00,0.036,139
Client  38,0.075,0.00,0.00,0.00,0.00,0.925,0.00,0.00,0.00,0.00,239
Client  39,0.00,0.334,0.00,0.00,0.00,0.00,0.00,0.666,0.00,0.00,1283
Client  40,0.00,0.00,0.00,0.00,0.00,0.353,0.00,0.211,0.436,0.00,337
Client  41,1.000,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,502
Client  42,0.218,0.00,0.020,0.259,0.00,0.041,0.00,0.463,0.00,0.00,147
Client  43,0.073,0.008,0.024,0.062,0.073,0.00,0.00,0.00,0.003,0.758,372
Client  44,0.003,0.00,0.00,0.064,0.269,0.00,0.114,0.247,0.302,0.00,668
Client  45,0.00,0.025,0.00,0.066,0.689,0.00,0.213,0.00,0.008,0.00,122
Client  46,0.002,0.258,0.019,0.00,0.042,0.274,0.00,0.406,0.00,0.00,530
Client  47,1.000,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,689
Client  48,0.023,0.00,0.003,0.036,0.00,0.00,0.227,0.006,0.605,0.100,309
Client  49,0.102,0.377,0.001,0.519,0.00,0.00,0.00,0.00,0.00,0.00,695
Client  50,0.00,0.00,0.00,0.136,0.00,0.00,0.00,0.017,0.00,0.847,118
Client  51,0.331,0.035,0.00,0.052,0.00,0.083,0.034,0.00,0.012,0.453,762
Client  52,0.007,0.276,0.717,0.00,0.00,0.00,0.00,0.00,0.00,0.00,1078
Client  53,0.00,0.073,0.153,0.00,0.00,0.00,0.271,0.00,0.504,0.00,262
Client  54,0.003,0.003,0.127,0.173,0.663,0.025,0.00,0.006,0.00,0.00,323
Client  55,0.000,0.00,0.000,0.00,0.00,0.999,0.00,0.00,0.00,0.00,2015
Client  56,0.00,0.00,0.014,0.986,0.00,0.00,0.00,0.00,0.00,0.00,876
Client  57,0.424,0.299,0.002,0.275,0.00,0.00,0.00,0.00,0.00,0.00,655
Client  58,0.003,0.997,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,787
Client  59,0.00,0.00,0.908,0.00,0.00,0.00,0.031,0.00,0.061,0.00,131
Client  60,0.067,0.00,0.00,0.00,0.067,0.200,0.00,0.200,0.333,0.133,15
Client  61,0.00,0.027,0.001,0.00,0.00,0.00,0.430,0.177,0.364,0.00,711
Client  62,0.00,0.00,0.00,0.00,0.006,0.983,0.00,0.00,0.011,0.00,359
Client  63,0.00,0.00,0.519,0.002,0.005,0.229,0.246,0.00,0.00,0.00,582
Client  64,0.00,0.00,0.00,0.00,0.00,0.064,0.207,0.729,0.00,0.00,299
Client  65,0.295,0.003,0.00,0.00,0.00,0.00,0.00,0.011,0.691,0.00,356
Client  66,0.375,0.267,0.003,0.00,0.182,0.020,0.00,0.003,0.131,0.020,352
Client  67,0.053,0.00,0.037,0.00,0.910,0.00,0.00,0.00,0.00,0.00,543
Client  68,0.00,0.001,0.999,0.00,0.00,0.00,0.00,0.00,0.00,0.00,993
Client  69,0.00,0.185,0.00,0.00,0.228,0.587,0.00,0.00,0.00,0.00,584
Client  70,0.00,0.00,0.011,0.00,0.122,0.00,0.867,0.00,0.00,0.00,90
Client  71,0.00,0.931,0.00,0.00,0.038,0.013,0.00,0.00,0.019,0.00,159
Client  72,0.00,0.00,0.034,0.00,0.00,0.00,0.00,0.966,0.00,0.00,206
Client  73,0.00,0.103,0.00,0.068,0.017,0.222,0.00,0.00,0.111,0.479,117
Client  74,0.00,0.00,0.003,0.861,0.00,0.00,0.00,0.136,0.00,0.00,309
Client  75,0.00,0.00,0.00,0.001,0.009,0.00,0.991,0.00,0.00,0.00,2441
Client  76,0.026,0.00,0.00,0.00,0.071,0.00,0.00,0.00,0.002,0.900,2848
Client  77,0.00,0.00,0.026,0.00,0.00,0.974,0.00,0.00,0.00,0.00,693
Client  78,0.440,0.004,0.00,0.00,0.444,0.00,0.00,0.00,0.112,0.00,277
Client  79,0.828,0.007,0.00,0.131,0.004,0.029,0.00,0.00,0.00,0.00,274
Client  80,0.00,0.201,0.003,0.661,0.00,0.013,0.003,0.119,0.00,0.00,319
Client  81,0.00,0.00,0.00,1.000,0.00,0.00,0.00,0.00,0.00,0.00,1152
Client  82,0.00,0.009,0.757,0.009,0.00,0.00,0.036,0.00,0.009,0.180,111
Client  83,0.005,0.00,0.00,0.00,0.003,0.228,0.00,0.750,0.00,0.013,372
Client  84,0.00,0.00,0.00,0.063,0.077,0.00,0.860,0.00,0.00,0.00,271
Client  85,0.00,0.244,0.555,0.00,0.00,0.00,0.00,0.088,0.113,0.00,238
Client  86,0.002,0.00,0.314,0.00,0.666,0.00,0.00,0.00,0.018,0.00,437
Client  87,0.00,0.022,0.022,0.011,0.022,0.043,0.269,0.258,0.032,0.323,93
Client  88,0.00,0.00,0.00,0.281,0.00,0.00,0.00,0.357,0.010,0.352,210
Client  89,0.086,0.029,0.642,0.011,0.103,0.019,0.00,0.00,0.00,0.109,523
Client  90,0.00,0.00,1.000,0.00,0.00,0.00,0.00,0.00,0.00,0.00,800
Client  91,0.00,0.00,0.377,0.003,0.00,0.002,0.00,0.619,0.00,0.00,1199
Client  92,0.065,0.00,0.015,0.915,0.00,0.005,0.00,0.00,0.00,0.00,400
Client  93,0.00,0.496,0.00,0.00,0.331,0.132,0.00,0.041,0.00,0.00,516
Client  94,0.00,0.00,0.00,0.00,0.008,0.032,0.944,0.00,0.016,0.00,125
Client  95,0.00,0.00,0.375,0.00,0.00,0.00,0.00,0.00,0.625,0.00,32
Client  96,0.00,0.073,0.011,0.00,0.915,0.00,0.00,0.00,0.00,0.00,1321
Client  97,0.00,0.00,0.00,0.00,0.00,0.00,0.925,0.00,0.050,0.025,40
Client  98,0.00,0.073,0.00,0.00,0.00,0.836,0.018,0.009,0.064,0.00,110
Client  99,0.173,0.003,0.003,0.799,0.003,0.003,0.003,0.003,0.006,0.006,359
Num_samples of Training set per client: [171, 52, 457, 56, 156, 479, 538, 110, 1059, 120, 513, 240, 48, 453, 354, 213, 830, 543, 526, 524, 37, 277, 718, 70, 1084, 484, 72, 269, 951, 845, 257, 211, 257, 546, 1363, 263, 979, 139, 239, 1283, 337, 502, 147, 372, 668, 122, 530, 689, 309, 695, 118, 762, 1078, 262, 323, 2015, 876, 655, 787, 131, 15, 711, 359, 582, 299, 356, 352, 543, 993, 584, 90, 159, 206, 117, 309, 2441, 2848, 693, 277, 274, 319, 1152, 111, 372, 271, 238, 437, 93, 210, 523, 800, 1199, 400, 516, 125, 32, 1321, 40, 110, 359]
Total_training_samples: 50000
Global test set: 10000
Finish Generating Samples, distribution saved
MobileNetV2(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layers): Sequential(
    (0): Block(
      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Block(
      (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (6): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (7): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (8): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (9): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (10): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (12): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (13): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (14): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (15): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (16): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv2): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear): Linear(in_features=1280, out_features=10, bias=True)
)
number of parameters: 2296922
clients initializting...
output size: 10
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:51,  1.93it/s]  2%|▏         | 2/100 [00:01<00:54,  1.78it/s]  3%|▎         | 3/100 [00:01<01:01,  1.58it/s]  4%|▍         | 4/100 [00:05<02:28,  1.54s/it]  5%|▌         | 5/100 [00:06<02:04,  1.31s/it]  6%|▌         | 6/100 [00:07<01:43,  1.11s/it]  7%|▋         | 7/100 [00:07<01:30,  1.02it/s]  8%|▊         | 8/100 [00:08<01:18,  1.18it/s]  9%|▉         | 9/100 [00:08<01:11,  1.27it/s] 10%|█         | 10/100 [00:09<01:08,  1.32it/s] 11%|█         | 11/100 [00:10<01:04,  1.39it/s] 12%|█▏        | 12/100 [00:11<01:05,  1.35it/s] 13%|█▎        | 13/100 [00:11<01:01,  1.42it/s] 14%|█▍        | 14/100 [00:12<00:58,  1.46it/s] 15%|█▌        | 15/100 [00:13<00:59,  1.42it/s] 16%|█▌        | 16/100 [00:13<00:55,  1.51it/s] 17%|█▋        | 17/100 [00:14<00:51,  1.60it/s] 18%|█▊        | 18/100 [00:14<00:52,  1.57it/s] 19%|█▉        | 19/100 [00:15<00:52,  1.55it/s] 20%|██        | 20/100 [00:16<00:53,  1.50it/s] 21%|██        | 21/100 [00:16<00:52,  1.50it/s] 22%|██▏       | 22/100 [00:17<00:51,  1.50it/s] 23%|██▎       | 23/100 [00:18<00:52,  1.47it/s] 24%|██▍       | 24/100 [00:18<00:51,  1.46it/s] 25%|██▌       | 25/100 [00:19<00:51,  1.46it/s] 26%|██▌       | 26/100 [00:20<00:52,  1.42it/s] 27%|██▋       | 27/100 [00:20<00:48,  1.50it/s] 28%|██▊       | 28/100 [00:21<00:48,  1.48it/s] 29%|██▉       | 29/100 [00:22<00:51,  1.39it/s] 30%|███       | 30/100 [00:23<00:47,  1.47it/s] 31%|███       | 31/100 [00:23<00:48,  1.44it/s] 32%|███▏      | 32/100 [00:24<00:48,  1.41it/s] 33%|███▎      | 33/100 [00:25<00:46,  1.44it/s] 34%|███▍      | 34/100 [00:25<00:47,  1.40it/s] 35%|███▌      | 35/100 [00:26<00:47,  1.37it/s] 36%|███▌      | 36/100 [00:30<01:46,  1.67s/it] 37%|███▋      | 37/100 [00:31<01:29,  1.41s/it] 38%|███▊      | 38/100 [00:32<01:15,  1.21s/it] 39%|███▉      | 39/100 [00:32<01:02,  1.02s/it] 40%|████      | 40/100 [00:33<00:53,  1.11it/s] 41%|████      | 41/100 [00:33<00:48,  1.23it/s] 42%|████▏     | 42/100 [00:34<00:42,  1.37it/s] 43%|████▎     | 43/100 [00:35<00:41,  1.39it/s] 44%|████▍     | 44/100 [00:35<00:41,  1.35it/s] 45%|████▌     | 45/100 [00:36<00:41,  1.34it/s] 46%|████▌     | 46/100 [00:37<00:39,  1.36it/s] 47%|████▋     | 47/100 [00:38<00:39,  1.35it/s] 48%|████▊     | 48/100 [00:38<00:35,  1.46it/s] 49%|████▉     | 49/100 [00:39<00:36,  1.39it/s] 50%|█████     | 50/100 [00:40<00:35,  1.41it/s] 51%|█████     | 51/100 [00:40<00:33,  1.46it/s] 52%|█████▏    | 52/100 [00:41<00:34,  1.38it/s] 53%|█████▎    | 53/100 [00:42<00:33,  1.41it/s] 54%|█████▍    | 54/100 [00:42<00:32,  1.44it/s] 55%|█████▌    | 55/100 [00:43<00:32,  1.38it/s] 56%|█████▌    | 56/100 [00:44<00:31,  1.38it/s] 57%|█████▋    | 57/100 [00:45<00:29,  1.46it/s] 58%|█████▊    | 58/100 [00:45<00:28,  1.46it/s] 59%|█████▉    | 59/100 [00:46<00:27,  1.51it/s] 60%|██████    | 60/100 [00:47<00:26,  1.53it/s] 61%|██████    | 61/100 [00:47<00:26,  1.47it/s] 62%|██████▏   | 62/100 [00:48<00:26,  1.44it/s] 63%|██████▎   | 63/100 [00:49<00:25,  1.48it/s] 64%|██████▍   | 64/100 [00:49<00:24,  1.45it/s] 65%|██████▌   | 65/100 [00:50<00:23,  1.50it/s] 66%|██████▌   | 66/100 [00:51<00:22,  1.49it/s] 67%|██████▋   | 67/100 [00:51<00:23,  1.39it/s] 68%|██████▊   | 68/100 [00:55<00:52,  1.63s/it] 69%|██████▉   | 69/100 [00:56<00:40,  1.32s/it] 70%|███████   | 70/100 [00:56<00:33,  1.12s/it] 71%|███████   | 71/100 [00:57<00:28,  1.03it/s] 72%|███████▏  | 72/100 [00:58<00:24,  1.15it/s] 73%|███████▎  | 73/100 [00:58<00:21,  1.29it/s] 74%|███████▍  | 74/100 [00:59<00:19,  1.31it/s] 75%|███████▌  | 75/100 [01:00<00:18,  1.39it/s] 76%|███████▌  | 76/100 [01:00<00:17,  1.40it/s] 77%|███████▋  | 77/100 [01:01<00:17,  1.35it/s] 78%|███████▊  | 78/100 [01:02<00:15,  1.44it/s] 79%|███████▉  | 79/100 [01:02<00:14,  1.46it/s] 80%|████████  | 80/100 [01:03<00:13,  1.45it/s] 81%|████████  | 81/100 [01:04<00:13,  1.42it/s] 82%|████████▏ | 82/100 [01:04<00:11,  1.51it/s] 83%|████████▎ | 83/100 [01:05<00:11,  1.46it/s] 84%|████████▍ | 84/100 [01:06<00:11,  1.45it/s] 85%|████████▌ | 85/100 [01:06<00:10,  1.49it/s] 86%|████████▌ | 86/100 [01:07<00:09,  1.50it/s] 87%|████████▋ | 87/100 [01:08<00:08,  1.49it/s] 88%|████████▊ | 88/100 [01:09<00:08,  1.38it/s] 89%|████████▉ | 89/100 [01:09<00:07,  1.42it/s] 90%|█████████ | 90/100 [01:10<00:07,  1.37it/s] 91%|█████████ | 91/100 [01:11<00:06,  1.49it/s] 92%|█████████▏| 92/100 [01:11<00:05,  1.46it/s] 93%|█████████▎| 93/100 [01:12<00:04,  1.47it/s] 94%|█████████▍| 94/100 [01:13<00:04,  1.47it/s] 95%|█████████▌| 95/100 [01:13<00:03,  1.49it/s] 96%|█████████▌| 96/100 [01:14<00:02,  1.57it/s] 97%|█████████▋| 97/100 [01:15<00:01,  1.56it/s] 98%|█████████▊| 98/100 [01:15<00:01,  1.59it/s] 99%|█████████▉| 99/100 [01:16<00:00,  1.54it/s]100%|██████████| 100/100 [01:17<00:00,  1.37it/s]100%|██████████| 100/100 [01:17<00:00,  1.29it/s]
Number of users per round / total users: 10  /  100
Finished creating FedAvg server.
=== FedAvg ===
-------------Round number:  0  -------------
loss: NT_CE learning rate: 0.001
training loss: tensor(0.9521) KL loss: tensor(0.2343)
loss: NT_CE learning rate: 0.001
training loss: tensor(0.8224) KL loss: tensor(0.2017)
loss: NT_CE learning rate: 0.001
training loss: tensor(0.8936) KL loss: tensor(0.1979)
loss: NT_CE learning rate: 0.001
training loss: tensor(0.0537) KL loss: tensor(0.0221)
loss: NT_CE learning rate: 0.001
training loss: tensor(0.1332) KL loss: tensor(0.0542)
loss: NT_CE learning rate: 0.001
training loss: tensor(0.9740) KL loss: tensor(0.2129)
loss: NT_CE learning rate: 0.001
training loss: tensor(0.0623) KL loss: tensor(0.0227)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.1405) KL loss: tensor(0.2348)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.1729) KL loss: tensor(0.2510)
loss: NT_CE learning rate: 0.001
training loss: tensor(0.6884) KL loss: tensor(0.1312)
         GM acc on global data: 0.1 length of data: 10000
save a model
-------------Round number:  1  -------------
loss: NT_CE learning rate: 0.000982
training loss: tensor(4.5588) KL loss: tensor(1.3874)
loss: NT_CE learning rate: 0.000982
training loss: tensor(4.1715) KL loss: tensor(1.8671)
loss: NT_CE learning rate: 0.000982
training loss: tensor(2.6652) KL loss: tensor(0.7752)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.5485) KL loss: tensor(0.5355)
loss: NT_CE learning rate: 0.000982
training loss: tensor(3.2958) KL loss: tensor(1.1440)
loss: NT_CE learning rate: 0.000982
training loss: tensor(2.8389) KL loss: tensor(0.9599)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.6923) KL loss: tensor(0.5118)
loss: NT_CE learning rate: 0.000982
training loss: tensor(10.0345) KL loss: tensor(3.5231)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.8285) KL loss: tensor(0.6229)
loss: NT_CE learning rate: 0.000982
training loss: tensor(4.4496) KL loss: tensor(1.4654)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  2  -------------
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.8045) KL loss: tensor(0.1398)
loss: NT_CE learning rate: 0.000964
training loss: tensor(1.0981) KL loss: tensor(0.2281)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.0803) KL loss: tensor(0.0212)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.3109) KL loss: tensor(0.1117)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.2226) KL loss: tensor(0.0845)
loss: NT_CE learning rate: 0.000964
training loss: tensor(2.6517) KL loss: tensor(0.2843)
loss: NT_CE learning rate: 0.000964
training loss: tensor(1.1568) KL loss: tensor(0.2331)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.9282) KL loss: tensor(0.1487)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.8929) KL loss: tensor(0.1786)
loss: NT_CE learning rate: 0.000964
training loss: tensor(1.2016) KL loss: tensor(0.2593)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  3  -------------
loss: NT_CE learning rate: 0.000946
training loss: tensor(1.0029) KL loss: tensor(0.2376)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.4377) KL loss: tensor(0.0885)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.5773) KL loss: tensor(0.1068)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.7152) KL loss: tensor(0.1519)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.3403) KL loss: tensor(0.0892)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.4817) KL loss: tensor(0.1102)
loss: NT_CE learning rate: 0.000946
training loss: tensor(1.3090) KL loss: tensor(0.1728)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.7234) KL loss: tensor(0.1369)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.6555) KL loss: tensor(0.1180)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.4970) KL loss: tensor(0.0645)
         GM acc on global data: 0.1059 length of data: 10000
save a model
-------------Round number:  4  -------------
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(1.5545) KL loss: tensor(0.1678)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.9943) KL loss: tensor(0.2179)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.1567) KL loss: tensor(0.0562)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.1139) KL loss: tensor(0.0382)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.9265) KL loss: tensor(0.2253)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.1334) KL loss: tensor(0.0486)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.9947) KL loss: tensor(0.1981)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(1.5834) KL loss: tensor(0.1110)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.1685) KL loss: tensor(0.0425)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.1261) KL loss: tensor(0.0369)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  5  -------------
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.9113) KL loss: tensor(0.1477)
loss: NT_CE learning rate: 0.00091
training loss: tensor(1.6464) KL loss: tensor(0.2685)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.8326) KL loss: tensor(0.1751)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.6661) KL loss: tensor(0.1431)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.5473) KL loss: tensor(0.0829)
loss: NT_CE learning rate: 0.00091
training loss: tensor(2.5649) KL loss: tensor(0.2064)
loss: NT_CE learning rate: 0.00091
training loss: tensor(1.2068) KL loss: tensor(0.1953)
loss: NT_CE learning rate: 0.00091
training loss: tensor(2.2765) KL loss: tensor(0.2077)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.2543) KL loss: tensor(0.0680)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.3347) KL loss: tensor(0.0644)
         GM acc on global data: 0.1001 length of data: 10000
-------------Round number:  6  -------------
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.1877) KL loss: tensor(0.0672)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.4987) KL loss: tensor(0.1162)
loss: NT_CE learning rate: 0.000892
training loss: tensor(1.0477) KL loss: tensor(0.1592)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.6679) KL loss: tensor(0.1070)
loss: NT_CE learning rate: 0.000892
training loss: tensor(1.1125) KL loss: tensor(0.1432)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.4321) KL loss: tensor(0.1061)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.9939) KL loss: tensor(0.1540)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.0514) KL loss: tensor(0.0349)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.7435) KL loss: tensor(0.1543)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.0856) KL loss: tensor(0.0368)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  7  -------------
loss: NT_CE learning rate: 0.000874
training loss: tensor(1.1210) KL loss: tensor(0.1336)
loss: NT_CE learning rate: 0.000874
training loss: tensor(2.0173) KL loss: tensor(0.1764)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.3702) KL loss: tensor(0.1036)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.4528) KL loss: tensor(0.1246)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.3530) KL loss: tensor(0.0945)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.2518) KL loss: tensor(0.0283)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.7954) KL loss: tensor(0.1614)
loss: NT_CE learning rate: 0.000874
training loss: tensor(1.0105) KL loss: tensor(0.0907)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.4322) KL loss: tensor(0.0886)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.7663) KL loss: tensor(0.1533)
         GM acc on global data: 0.1089 length of data: 10000
save a model
-------------Round number:  8  -------------
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.2269) KL loss: tensor(0.0899)
loss: NT_CE learning rate: 0.000856
training loss: tensor(1.5181) KL loss: tensor(0.1868)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.5537) KL loss: tensor(0.1291)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.5343) KL loss: tensor(0.1323)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.1136) KL loss: tensor(0.0286)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.2979) KL loss: tensor(0.0907)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.3379) KL loss: tensor(0.0937)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.1164) KL loss: tensor(0.0329)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.6340) KL loss: tensor(0.0700)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.4807) KL loss: tensor(0.1431)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  9  -------------
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.2752) KL loss: tensor(0.0587)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(1.4695) KL loss: tensor(0.1947)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.5947) KL loss: tensor(0.1282)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.3045) KL loss: tensor(0.0450)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(2.3033) KL loss: tensor(0.1381)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.4611) KL loss: tensor(0.1285)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(2.9051) KL loss: tensor(0.2515)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(2.6254) KL loss: tensor(0.1535)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(3.0204) KL loss: tensor(0.2264)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(2.2775) KL loss: tensor(0.2151)
         GM acc on global data: 0.1211 length of data: 10000
save a model
-------------Round number:  10  -------------
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.7654) KL loss: tensor(0.1782)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.5574) KL loss: tensor(0.1298)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(1.2511) KL loss: tensor(0.1270)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(1.4585) KL loss: tensor(0.2564)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(1.0473) KL loss: tensor(0.1832)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(2.0713) KL loss: tensor(0.3083)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.9393) KL loss: tensor(0.1772)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.6285) KL loss: tensor(0.1690)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.5184) KL loss: tensor(0.1596)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.2361) KL loss: tensor(0.1050)
         GM acc on global data: 0.1 length of data: 10000
-------------Round number:  11  -------------
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.5563) KL loss: tensor(0.1305)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.3568) KL loss: tensor(0.0874)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.5518) KL loss: tensor(0.1056)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.2853) KL loss: tensor(0.0684)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.2255) KL loss: tensor(0.0824)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.6735) KL loss: tensor(0.0708)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.2294) KL loss: tensor(0.0474)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.1808) KL loss: tensor(0.0349)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(1.5109) KL loss: tensor(0.6252)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.6922) KL loss: tensor(0.1277)
         GM acc on global data: 0.1023 length of data: 10000
-------------Round number:  12  -------------
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.3073) KL loss: tensor(0.0799)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.1627) KL loss: tensor(0.0432)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.3442) KL loss: tensor(0.0780)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.6058) KL loss: tensor(0.1208)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.1015) KL loss: tensor(0.0321)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.2757) KL loss: tensor(0.0691)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.5621) KL loss: tensor(0.1348)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.4491) KL loss: tensor(0.0709)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.5235) KL loss: tensor(0.1023)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(1.7420) KL loss: tensor(0.2327)
         GM acc on global data: 0.1086 length of data: 10000
-------------Round number:  13  -------------
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.8802) KL loss: tensor(0.1616)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.3783) KL loss: tensor(0.0631)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.4797) KL loss: tensor(0.0844)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.5041) KL loss: tensor(0.0892)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.5046) KL loss: tensor(0.0736)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.8564) KL loss: tensor(0.1239)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.6734) KL loss: tensor(0.1556)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.3935) KL loss: tensor(0.0701)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.4773) KL loss: tensor(0.1352)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.3897) KL loss: tensor(0.1193)
         GM acc on global data: 0.2129 length of data: 10000
save a model
-------------Round number:  14  -------------
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.1077) KL loss: tensor(0.0485)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.2839) KL loss: tensor(0.1304)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.3347) KL loss: tensor(0.1308)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.3517) KL loss: tensor(0.1314)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.0504) KL loss: tensor(0.0340)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.5902) KL loss: tensor(0.1057)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.4808) KL loss: tensor(0.1301)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.3951) KL loss: tensor(0.1311)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.2801) KL loss: tensor(0.1070)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.4318) KL loss: tensor(0.1020)
         GM acc on global data: 0.1075 length of data: 10000
-------------Round number:  15  -------------
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.8198) KL loss: tensor(0.1322)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.3527) KL loss: tensor(0.0868)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.4378) KL loss: tensor(0.1232)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.6058) KL loss: tensor(0.1417)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.3303) KL loss: tensor(0.0924)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.6979) KL loss: tensor(0.1312)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.5230) KL loss: tensor(0.1450)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.5324) KL loss: tensor(0.0944)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.6549) KL loss: tensor(0.1306)
loss: NT_CE learning rate: 0.00073
training loss: tensor(1.5158) KL loss: tensor(0.4718)
         GM acc on global data: 0.2786 length of data: 10000
save a model
-------------Round number:  16  -------------
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.5359) KL loss: tensor(0.1146)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.3077) KL loss: tensor(0.1062)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.2842) KL loss: tensor(0.0697)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.4781) KL loss: tensor(0.1476)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.1039) KL loss: tensor(0.0447)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.0817) KL loss: tensor(0.0370)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.5283) KL loss: tensor(0.2030)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.5797) KL loss: tensor(0.1661)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.7919) KL loss: tensor(0.1053)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.7080) KL loss: tensor(0.1717)
         GM acc on global data: 0.2539 length of data: 10000
-------------Round number:  17  -------------
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.1137) KL loss: tensor(0.0456)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.3206) KL loss: tensor(0.1012)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.7284) KL loss: tensor(0.1420)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.4514) KL loss: tensor(0.1095)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.7827) KL loss: tensor(0.1174)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.2670) KL loss: tensor(0.0918)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.7168) KL loss: tensor(0.1058)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.3143) KL loss: tensor(0.0843)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.0699) KL loss: tensor(0.0343)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.7734) KL loss: tensor(0.1736)
         GM acc on global data: 0.2322 length of data: 10000
-------------Round number:  18  -------------
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.4431) KL loss: tensor(0.1589)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.0845) KL loss: tensor(0.0525)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.5341) KL loss: tensor(0.1246)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.7066) KL loss: tensor(0.2244)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.3625) KL loss: tensor(0.1287)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.4670) KL loss: tensor(0.1245)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.8689) KL loss: tensor(0.1169)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.7480) KL loss: tensor(0.1584)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.3862) KL loss: tensor(0.1457)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.3274) KL loss: tensor(0.1307)
         GM acc on global data: 0.2404 length of data: 10000
-------------Round number:  19  -------------
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.4169) KL loss: tensor(0.1219)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.2448) KL loss: tensor(0.0768)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.0655) KL loss: tensor(0.0504)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.3276) KL loss: tensor(0.1121)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.3718) KL loss: tensor(0.1398)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.4434) KL loss: tensor(0.0912)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.1406) KL loss: tensor(0.0519)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(1.2647) KL loss: tensor(0.4589)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.3392) KL loss: tensor(0.0990)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.3498) KL loss: tensor(0.1454)
         GM acc on global data: 0.1872 length of data: 10000
-------------Round number:  20  -------------
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.7784) KL loss: tensor(0.0956)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.3969) KL loss: tensor(0.0658)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.3044) KL loss: tensor(0.1021)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.2754) KL loss: tensor(0.1074)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.1735) KL loss: tensor(0.0506)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.3939) KL loss: tensor(0.1013)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.7448) KL loss: tensor(0.0923)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.1370) KL loss: tensor(0.0535)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.6694) KL loss: tensor(0.0679)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.2354) KL loss: tensor(0.0779)
         GM acc on global data: 0.259 length of data: 10000
-------------Round number:  21  -------------
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(1.4011) KL loss: tensor(0.3320)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.0518) KL loss: tensor(0.0390)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.3898) KL loss: tensor(0.1111)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.1564) KL loss: tensor(0.0784)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.3634) KL loss: tensor(0.0764)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.4224) KL loss: tensor(0.1383)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.4662) KL loss: tensor(0.1350)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.3974) KL loss: tensor(0.1024)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.3972) KL loss: tensor(0.1528)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.5814) KL loss: tensor(0.1373)
         GM acc on global data: 0.2064 length of data: 10000
-------------Round number:  22  -------------
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.3851) KL loss: tensor(0.1267)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.1728) KL loss: tensor(0.0797)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.9274) KL loss: tensor(0.1181)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.3975) KL loss: tensor(0.0903)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.7294) KL loss: tensor(0.1557)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.8174) KL loss: tensor(0.1045)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(1.1620) KL loss: tensor(0.1429)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.5537) KL loss: tensor(0.1327)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.2020) KL loss: tensor(0.0880)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.4249) KL loss: tensor(0.1036)
         GM acc on global data: 0.261 length of data: 10000
-------------Round number:  23  -------------
loss: NT_CE learning rate: 0.000586
training loss: tensor(1.3629) KL loss: tensor(0.2345)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.2585) KL loss: tensor(0.1025)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.8127) KL loss: tensor(0.1450)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.1560) KL loss: tensor(0.0651)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.2414) KL loss: tensor(0.0741)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.6555) KL loss: tensor(0.1776)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.9395) KL loss: tensor(0.2029)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.0907) KL loss: tensor(0.0405)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.8144) KL loss: tensor(0.2033)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.2884) KL loss: tensor(0.1075)
         GM acc on global data: 0.1181 length of data: 10000
-------------Round number:  24  -------------
loss: NT_CE learning rate: 0.000568
training loss: tensor(3.7709) KL loss: tensor(0.2202)
loss: NT_CE learning rate: 0.000568
training loss: tensor(1.3922) KL loss: tensor(0.1182)
loss: NT_CE learning rate: 0.000568
training loss: tensor(1.0689) KL loss: tensor(0.1210)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.6778) KL loss: tensor(0.0816)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.3009) KL loss: tensor(0.0686)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.9713) KL loss: tensor(0.0912)
loss: NT_CE learning rate: 0.000568
training loss: tensor(1.3080) KL loss: tensor(0.1106)
loss: NT_CE learning rate: 0.000568
training loss: tensor(1.1529) KL loss: tensor(0.1286)
loss: NT_CE learning rate: 0.000568
training loss: tensor(1.6283) KL loss: tensor(0.1341)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.6378) KL loss: tensor(0.0697)
         GM acc on global data: 0.2341 length of data: 10000
-------------Round number:  25  -------------
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.5128) KL loss: tensor(0.1298)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.2507) KL loss: tensor(0.0475)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.6471) KL loss: tensor(0.1876)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.7713) KL loss: tensor(0.1685)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.8530) KL loss: tensor(0.1879)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.6511) KL loss: tensor(0.1553)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.5139) KL loss: tensor(0.1636)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.7231) KL loss: tensor(0.2342)
loss: NT_CE learning rate: 0.00055
training loss: tensor(1.1979) KL loss: tensor(0.2110)
loss: NT_CE learning rate: 0.00055
training loss: tensor(1.0627) KL loss: tensor(0.2543)
         GM acc on global data: 0.2781 length of data: 10000
-------------Round number:  26  -------------
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.2651) KL loss: tensor(0.0986)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.3390) KL loss: tensor(0.1492)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.5390) KL loss: tensor(0.1369)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.1169) KL loss: tensor(0.0533)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.3130) KL loss: tensor(0.0999)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.1912) KL loss: tensor(0.0763)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.6135) KL loss: tensor(0.2158)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.9993) KL loss: tensor(0.1444)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.2411) KL loss: tensor(0.0973)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.9601) KL loss: tensor(0.1676)
         GM acc on global data: 0.3322 length of data: 10000
save a model
-------------Round number:  27  -------------
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.3814) KL loss: tensor(0.1426)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.1515) KL loss: tensor(0.0658)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.2654) KL loss: tensor(0.0954)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.4810) KL loss: tensor(0.1071)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.2911) KL loss: tensor(0.1093)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.5633) KL loss: tensor(0.1202)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.2827) KL loss: tensor(0.1119)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.3507) KL loss: tensor(0.1108)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.2696) KL loss: tensor(0.1016)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.3307) KL loss: tensor(0.1293)
         GM acc on global data: 0.3092 length of data: 10000
-------------Round number:  28  -------------
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.2102) KL loss: tensor(0.0724)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.4069) KL loss: tensor(0.1050)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.2267) KL loss: tensor(0.0711)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.2426) KL loss: tensor(0.0821)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.6172) KL loss: tensor(0.1881)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.2753) KL loss: tensor(0.0966)
loss: NT_CE learning rate: 0.000496
training loss: tensor(1.8331) KL loss: tensor(0.9380)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.1454) KL loss: tensor(0.0681)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.1392) KL loss: tensor(0.0730)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.4041) KL loss: tensor(0.1448)
         GM acc on global data: 0.3035 length of data: 10000
-------------Round number:  29  -------------
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.5665) KL loss: tensor(0.1138)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.1616) KL loss: tensor(0.0768)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.8565) KL loss: tensor(0.4148)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.3493) KL loss: tensor(0.0832)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.4348) KL loss: tensor(0.1335)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.3618) KL loss: tensor(0.0964)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.2894) KL loss: tensor(0.0815)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.2351) KL loss: tensor(0.0915)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.4869) KL loss: tensor(0.0808)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.7814) KL loss: tensor(0.1168)
         GM acc on global data: 0.3065 length of data: 10000
-------------Round number:  30  -------------
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.4239) KL loss: tensor(0.1258)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.2440) KL loss: tensor(0.1001)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.1411) KL loss: tensor(0.0807)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.1243) KL loss: tensor(0.0761)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.0855) KL loss: tensor(0.0571)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3349) KL loss: tensor(0.1069)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3587) KL loss: tensor(0.1163)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.5662) KL loss: tensor(0.1432)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3340) KL loss: tensor(0.1111)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.2917) KL loss: tensor(0.1317)
         GM acc on global data: 0.2957 length of data: 10000
-------------Round number:  31  -------------
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.2865) KL loss: tensor(0.0980)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.3051) KL loss: tensor(0.1197)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.1111) KL loss: tensor(0.0688)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.9635) KL loss: tensor(0.2009)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.1613) KL loss: tensor(0.0673)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.7153) KL loss: tensor(0.1224)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.6737) KL loss: tensor(0.2035)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.0905) KL loss: tensor(0.0620)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.6931) KL loss: tensor(0.1946)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.2211) KL loss: tensor(0.0855)
         GM acc on global data: 0.2965 length of data: 10000
-------------Round number:  32  -------------
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.1878) KL loss: tensor(0.0741)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.2126) KL loss: tensor(0.1028)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.3979) KL loss: tensor(0.1121)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(2.4544) KL loss: tensor(0.1979)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.1406) KL loss: tensor(0.0569)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.2844) KL loss: tensor(0.1004)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.3801) KL loss: tensor(0.1028)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.7102) KL loss: tensor(0.2071)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.1898) KL loss: tensor(0.0797)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.1579) KL loss: tensor(0.0766)
         GM acc on global data: 0.3449 length of data: 10000
save a model
-------------Round number:  33  -------------
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.1699) KL loss: tensor(0.0618)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.1718) KL loss: tensor(0.0534)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.9292) KL loss: tensor(0.1276)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.6519) KL loss: tensor(0.1024)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.4462) KL loss: tensor(0.1326)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.4261) KL loss: tensor(0.0972)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.3933) KL loss: tensor(0.0880)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.1143) KL loss: tensor(0.0423)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.3114) KL loss: tensor(0.1312)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(1.2119) KL loss: tensor(0.1251)
         GM acc on global data: 0.1409 length of data: 10000
-------------Round number:  34  -------------
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.6376) KL loss: tensor(0.1069)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.1160) KL loss: tensor(0.0437)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.4405) KL loss: tensor(0.0589)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.9350) KL loss: tensor(0.1107)
loss: NT_CE learning rate: 0.000388
training loss: tensor(2.5743) KL loss: tensor(0.3441)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.6496) KL loss: tensor(0.0973)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.1952) KL loss: tensor(0.0577)
loss: NT_CE learning rate: 0.000388
training loss: tensor(1.0460) KL loss: tensor(0.2886)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.8074) KL loss: tensor(0.0964)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.7018) KL loss: tensor(0.1513)
         GM acc on global data: 0.2439 length of data: 10000
-------------Round number:  35  -------------
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.4171) KL loss: tensor(0.1217)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.7532) KL loss: tensor(0.1257)
loss: NT_CE learning rate: 0.00037
training loss: tensor(1.0496) KL loss: tensor(0.1599)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.5678) KL loss: tensor(0.1311)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.8356) KL loss: tensor(0.1304)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.5403) KL loss: tensor(0.1156)
loss: NT_CE learning rate: 0.00037
training loss: tensor(1.3901) KL loss: tensor(0.4168)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.0942) KL loss: tensor(0.0357)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.5005) KL loss: tensor(0.1510)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.7056) KL loss: tensor(0.1350)
         GM acc on global data: 0.3205 length of data: 10000
-------------Round number:  36  -------------
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.8374) KL loss: tensor(0.3554)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.3906) KL loss: tensor(0.1236)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.3836) KL loss: tensor(0.1236)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.3352) KL loss: tensor(0.0913)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.3433) KL loss: tensor(0.1244)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.1149) KL loss: tensor(0.0512)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.1500) KL loss: tensor(0.0472)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.1330) KL loss: tensor(0.0584)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.1351) KL loss: tensor(0.0698)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.2018) KL loss: tensor(0.0903)
         GM acc on global data: 0.3217 length of data: 10000
-------------Round number:  37  -------------
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.2708) KL loss: tensor(0.1322)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.2280) KL loss: tensor(0.0863)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.4028) KL loss: tensor(0.1103)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.5430) KL loss: tensor(0.1262)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.4076) KL loss: tensor(0.0947)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.1137) KL loss: tensor(0.0685)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.1207) KL loss: tensor(0.0522)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.4367) KL loss: tensor(0.1292)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.3398) KL loss: tensor(0.1304)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.2993) KL loss: tensor(0.0912)
         GM acc on global data: 0.3285 length of data: 10000
-------------Round number:  38  -------------
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.3627) KL loss: tensor(0.0956)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.7786) KL loss: tensor(0.0918)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.1248) KL loss: tensor(0.0617)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.7101) KL loss: tensor(0.1021)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.3459) KL loss: tensor(0.0786)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.2143) KL loss: tensor(0.0517)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.4225) KL loss: tensor(0.1188)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.1964) KL loss: tensor(0.0675)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.2708) KL loss: tensor(0.0616)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.5223) KL loss: tensor(0.1168)
         GM acc on global data: 0.3684 length of data: 10000
save a model
-------------Round number:  39  -------------
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.3003) KL loss: tensor(0.0982)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.6577) KL loss: tensor(0.2384)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.5361) KL loss: tensor(0.1384)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.1744) KL loss: tensor(0.0860)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.7116) KL loss: tensor(0.1320)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.1528) KL loss: tensor(0.0565)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.1447) KL loss: tensor(0.0619)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.5141) KL loss: tensor(0.1214)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.3329) KL loss: tensor(0.0917)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2281) KL loss: tensor(0.0848)
         GM acc on global data: 0.3009 length of data: 10000
-------------Round number:  40  -------------
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.3515) KL loss: tensor(0.0952)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.1098) KL loss: tensor(0.0540)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.6761) KL loss: tensor(0.2170)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.7991) KL loss: tensor(0.1670)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.9639) KL loss: tensor(0.1795)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.9353) KL loss: tensor(0.1958)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.4783) KL loss: tensor(0.1388)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(1.2233) KL loss: tensor(0.2146)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.1971) KL loss: tensor(0.1145)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.7628) KL loss: tensor(0.1634)
         GM acc on global data: 0.3176 length of data: 10000
-------------Round number:  41  -------------
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.7685) KL loss: tensor(0.1511)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.1143) KL loss: tensor(0.0494)
loss: NT_CE learning rate: 0.000262
training loss: tensor(1.1821) KL loss: tensor(0.2082)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.0951) KL loss: tensor(0.0527)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.5600) KL loss: tensor(0.1345)
loss: NT_CE learning rate: 0.000262
training loss: tensor(1.1931) KL loss: tensor(0.5488)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.1538) KL loss: tensor(0.0497)
loss: NT_CE learning rate: 0.000262
training loss: tensor(1.3500) KL loss: tensor(0.1188)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.3657) KL loss: tensor(0.1125)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.4553) KL loss: tensor(0.1581)
         GM acc on global data: 0.337 length of data: 10000
-------------Round number:  42  -------------
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.3229) KL loss: tensor(0.0816)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.5883) KL loss: tensor(0.1870)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.3546) KL loss: tensor(0.0818)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.7999) KL loss: tensor(0.1063)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.3231) KL loss: tensor(0.1052)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.1121) KL loss: tensor(0.0843)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.3160) KL loss: tensor(0.0857)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.4331) KL loss: tensor(0.0944)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.6535) KL loss: tensor(0.3783)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(1.0148) KL loss: tensor(0.1019)
         GM acc on global data: 0.3246 length of data: 10000
-------------Round number:  43  -------------
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.8849) KL loss: tensor(0.0979)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.4177) KL loss: tensor(0.1230)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.7724) KL loss: tensor(0.1252)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.2236) KL loss: tensor(0.0828)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.2374) KL loss: tensor(0.0928)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.1134) KL loss: tensor(0.0613)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.3636) KL loss: tensor(0.1119)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.1472) KL loss: tensor(0.0462)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.7649) KL loss: tensor(0.1402)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.5296) KL loss: tensor(0.1223)
         GM acc on global data: 0.3496 length of data: 10000
-------------Round number:  44  -------------
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.5695) KL loss: tensor(0.1141)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(1.2747) KL loss: tensor(0.3373)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.6083) KL loss: tensor(0.1261)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.4174) KL loss: tensor(0.1648)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.2089) KL loss: tensor(0.1155)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.6871) KL loss: tensor(0.1588)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(1.2337) KL loss: tensor(0.1237)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(1.3769) KL loss: tensor(0.2283)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.0745) KL loss: tensor(0.0350)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.4236) KL loss: tensor(0.0946)
         GM acc on global data: 0.2627 length of data: 10000
-------------Round number:  45  -------------
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(1.2387) KL loss: tensor(0.1184)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.2142) KL loss: tensor(0.0493)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.3550) KL loss: tensor(0.0541)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.1340) KL loss: tensor(0.0538)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(1.6493) KL loss: tensor(0.2137)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.9115) KL loss: tensor(0.1213)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.7793) KL loss: tensor(0.1297)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.4728) KL loss: tensor(0.0587)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.4925) KL loss: tensor(0.1076)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.9994) KL loss: tensor(0.1352)
         GM acc on global data: 0.2841 length of data: 10000
-------------Round number:  46  -------------
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(1.3233) KL loss: tensor(0.1142)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.4755) KL loss: tensor(0.1163)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(2.1299) KL loss: tensor(0.3498)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(1.6150) KL loss: tensor(0.4380)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.5071) KL loss: tensor(0.1432)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.3783) KL loss: tensor(0.1084)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.1831) KL loss: tensor(0.0732)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(1.6825) KL loss: tensor(0.2346)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.7882) KL loss: tensor(0.1247)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(4.1150) KL loss: tensor(0.2885)
         GM acc on global data: 0.3431 length of data: 10000
-------------Round number:  47  -------------
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.1634) KL loss: tensor(0.0735)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.4302) KL loss: tensor(0.0827)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(2.3131) KL loss: tensor(0.2423)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.5511) KL loss: tensor(0.1210)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.8704) KL loss: tensor(0.2358)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.2087) KL loss: tensor(0.0757)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.8448) KL loss: tensor(0.1169)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.6618) KL loss: tensor(0.1353)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.3151) KL loss: tensor(0.0984)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.3569) KL loss: tensor(0.0841)
         GM acc on global data: 0.3694 length of data: 10000
save a model
-------------Round number:  48  -------------
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.4500) KL loss: tensor(0.1333)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.2531) KL loss: tensor(0.0927)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(1.2679) KL loss: tensor(0.2085)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.8783) KL loss: tensor(0.1194)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.6116) KL loss: tensor(0.1315)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.4592) KL loss: tensor(0.1213)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.3434) KL loss: tensor(0.1123)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.2010) KL loss: tensor(0.0837)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(1.4101) KL loss: tensor(0.5761)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.5385) KL loss: tensor(0.2172)
         GM acc on global data: 0.4112 length of data: 10000
save a model
-------------Round number:  49  -------------
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.0755) KL loss: tensor(0.0492)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.3253) KL loss: tensor(0.0857)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2209) KL loss: tensor(0.0870)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.1909) KL loss: tensor(0.0801)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.4075) KL loss: tensor(0.0895)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2147) KL loss: tensor(0.0673)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2683) KL loss: tensor(0.0775)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2070) KL loss: tensor(0.0959)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2438) KL loss: tensor(0.0614)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.7181) KL loss: tensor(0.1314)
         GM acc on global data: 0.4148 length of data: 10000
save a model
-------------Round number:  50  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8033) KL loss: tensor(0.1224)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4222) KL loss: tensor(0.1170)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3858) KL loss: tensor(0.0930)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.9830) KL loss: tensor(0.1867)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1869) KL loss: tensor(0.0777)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3728) KL loss: tensor(0.0996)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5046) KL loss: tensor(0.1414)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3027) KL loss: tensor(0.0904)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2652) KL loss: tensor(0.0809)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5029) KL loss: tensor(0.0844)
         GM acc on global data: 0.4003 length of data: 10000
-------------Round number:  51  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6297) KL loss: tensor(0.1267)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4452) KL loss: tensor(0.0988)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7577) KL loss: tensor(0.1777)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5076) KL loss: tensor(0.1123)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4666) KL loss: tensor(0.1121)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8662) KL loss: tensor(0.2011)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4731) KL loss: tensor(0.1032)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6866) KL loss: tensor(0.1200)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3196) KL loss: tensor(0.1003)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1545) KL loss: tensor(0.0743)
         GM acc on global data: 0.3331 length of data: 10000
-------------Round number:  52  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.6733) KL loss: tensor(0.1336)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2951) KL loss: tensor(0.1024)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3013) KL loss: tensor(0.1010)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7291) KL loss: tensor(0.1502)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4723) KL loss: tensor(0.1317)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.6634) KL loss: tensor(0.1482)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.4697) KL loss: tensor(0.2297)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7249) KL loss: tensor(0.1541)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5287) KL loss: tensor(0.0965)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5509) KL loss: tensor(0.0988)
         GM acc on global data: 0.3928 length of data: 10000
-------------Round number:  53  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5383) KL loss: tensor(0.1560)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3048) KL loss: tensor(0.0895)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6524) KL loss: tensor(0.1416)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.0106) KL loss: tensor(0.1562)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8194) KL loss: tensor(0.1224)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.3629) KL loss: tensor(0.9394)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2851) KL loss: tensor(0.0768)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7566) KL loss: tensor(0.1301)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2287) KL loss: tensor(0.0707)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6327) KL loss: tensor(0.1585)
         GM acc on global data: 0.3502 length of data: 10000
-------------Round number:  54  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2912) KL loss: tensor(0.1136)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7165) KL loss: tensor(0.1087)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7590) KL loss: tensor(0.1721)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6166) KL loss: tensor(0.1037)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1839) KL loss: tensor(0.0425)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5215) KL loss: tensor(0.1205)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1320) KL loss: tensor(0.0585)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4142) KL loss: tensor(0.0956)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2023) KL loss: tensor(0.0996)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5505) KL loss: tensor(0.1487)
         GM acc on global data: 0.3571 length of data: 10000
-------------Round number:  55  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2104) KL loss: tensor(0.0812)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3556) KL loss: tensor(0.1069)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5411) KL loss: tensor(0.1930)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4406) KL loss: tensor(0.0807)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6765) KL loss: tensor(0.2071)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0927) KL loss: tensor(0.0633)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3632) KL loss: tensor(0.1272)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1331) KL loss: tensor(0.0472)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3638) KL loss: tensor(0.0822)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4602) KL loss: tensor(0.1105)
         GM acc on global data: 0.3748 length of data: 10000
-------------Round number:  56  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3867) KL loss: tensor(0.1335)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4341) KL loss: tensor(0.0948)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.7044) KL loss: tensor(0.4496)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2970) KL loss: tensor(0.0891)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2336) KL loss: tensor(0.0822)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4923) KL loss: tensor(0.1319)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4530) KL loss: tensor(0.0787)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.5819) KL loss: tensor(0.2761)
loss: NT_CE learning rate: 0.0001
training loss: tensor(2.0053) KL loss: tensor(0.6973)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3223) KL loss: tensor(0.1045)
         GM acc on global data: 0.2928 length of data: 10000
-------------Round number:  57  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0754) KL loss: tensor(0.0546)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4640) KL loss: tensor(0.0669)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5132) KL loss: tensor(0.0648)
loss: NT_CE learning rate: 0.0001
training loss: tensor(2.0116) KL loss: tensor(0.1043)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2078) KL loss: tensor(0.0831)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.1509) KL loss: tensor(0.0979)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8510) KL loss: tensor(0.5334)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7978) KL loss: tensor(0.0608)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4636) KL loss: tensor(0.0714)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.8168) KL loss: tensor(0.1245)
         GM acc on global data: 0.288 length of data: 10000
-------------Round number:  58  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5494) KL loss: tensor(0.1099)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3128) KL loss: tensor(0.0769)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0968) KL loss: tensor(0.0544)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6961) KL loss: tensor(0.1111)
loss: NT_CE learning rate: 0.0001
training loss: tensor(2.7015) KL loss: tensor(0.2039)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.7620) KL loss: tensor(0.1876)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3999) KL loss: tensor(0.1171)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2788) KL loss: tensor(0.0705)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.4999) KL loss: tensor(0.1506)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7225) KL loss: tensor(0.1062)
         GM acc on global data: 0.3127 length of data: 10000
-------------Round number:  59  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8912) KL loss: tensor(0.1652)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6990) KL loss: tensor(0.1684)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4334) KL loss: tensor(0.0887)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.7407) KL loss: tensor(0.1700)
loss: NT_CE learning rate: 0.0001
training loss: tensor(2.4404) KL loss: tensor(1.2169)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3282) KL loss: tensor(0.1057)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1967) KL loss: tensor(0.0547)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2595) KL loss: tensor(0.0831)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2333) KL loss: tensor(0.0811)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5826) KL loss: tensor(0.1304)
         GM acc on global data: 0.3397 length of data: 10000
-------------Round number:  60  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8982) KL loss: tensor(0.1618)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6587) KL loss: tensor(0.1206)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2894) KL loss: tensor(0.0992)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8092) KL loss: tensor(0.1016)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1403) KL loss: tensor(0.0493)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7735) KL loss: tensor(0.0822)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4605) KL loss: tensor(0.1272)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2361) KL loss: tensor(0.0921)
loss: NT_CE learning rate: 0.0001
training loss: tensor(2.6424) KL loss: tensor(0.1478)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4227) KL loss: tensor(0.1031)
         GM acc on global data: 0.2924 length of data: 10000
-------------Round number:  61  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5669) KL loss: tensor(0.1220)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.2557) KL loss: tensor(0.0935)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0461) KL loss: tensor(0.0409)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.6898) KL loss: tensor(0.2018)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5679) KL loss: tensor(0.1231)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1817) KL loss: tensor(0.0954)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1949) KL loss: tensor(0.0627)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2330) KL loss: tensor(0.0788)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5764) KL loss: tensor(0.1373)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5890) KL loss: tensor(0.1835)
         GM acc on global data: 0.2482 length of data: 10000
-------------Round number:  62  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1675) KL loss: tensor(0.0485)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.0464) KL loss: tensor(0.1457)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0743) KL loss: tensor(0.0550)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7828) KL loss: tensor(0.1138)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1938) KL loss: tensor(0.0600)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7089) KL loss: tensor(0.0922)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8782) KL loss: tensor(0.1329)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5860) KL loss: tensor(0.1435)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2630) KL loss: tensor(0.0672)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1347) KL loss: tensor(0.0909)
         GM acc on global data: 0.3162 length of data: 10000
-------------Round number:  63  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7935) KL loss: tensor(0.1290)
loss: NT_CE learning rate: 0.0001
training loss: tensor(2.2577) KL loss: tensor(0.5614)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5789) KL loss: tensor(0.1648)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3030) KL loss: tensor(0.1016)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5363) KL loss: tensor(0.1019)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.3104) KL loss: tensor(0.0907)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2091) KL loss: tensor(0.0830)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7233) KL loss: tensor(0.1043)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7660) KL loss: tensor(0.1464)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3862) KL loss: tensor(0.1120)
         GM acc on global data: 0.3702 length of data: 10000
-------------Round number:  64  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1798) KL loss: tensor(0.0680)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1799) KL loss: tensor(0.0937)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3948) KL loss: tensor(0.1253)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4832) KL loss: tensor(0.1158)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.3833) KL loss: tensor(0.3397)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4382) KL loss: tensor(0.1399)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3242) KL loss: tensor(0.1138)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.4788) KL loss: tensor(0.1580)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3141) KL loss: tensor(0.1091)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1664) KL loss: tensor(0.0650)
         GM acc on global data: 0.3819 length of data: 10000
-------------Round number:  65  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.9365) KL loss: tensor(0.2549)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1116) KL loss: tensor(0.0520)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2300) KL loss: tensor(0.1441)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4817) KL loss: tensor(0.1683)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0727) KL loss: tensor(0.0526)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3316) KL loss: tensor(0.0890)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3373) KL loss: tensor(0.1091)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5887) KL loss: tensor(0.1619)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2247) KL loss: tensor(0.1018)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2330) KL loss: tensor(0.1285)
         GM acc on global data: 0.371 length of data: 10000
-------------Round number:  66  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1863) KL loss: tensor(0.0792)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3771) KL loss: tensor(0.1229)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7846) KL loss: tensor(0.1287)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1569) KL loss: tensor(0.0715)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1872) KL loss: tensor(0.0631)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4483) KL loss: tensor(0.1002)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1875) KL loss: tensor(0.0679)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4540) KL loss: tensor(0.0909)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3347) KL loss: tensor(0.1006)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2738) KL loss: tensor(0.0787)
         GM acc on global data: 0.3839 length of data: 10000
-------------Round number:  67  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1840) KL loss: tensor(0.0666)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1665) KL loss: tensor(0.0626)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6506) KL loss: tensor(0.1411)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3676) KL loss: tensor(0.1425)
loss: NT_CE learning rate: 0.0001
training loss: tensor(3.2107) KL loss: tensor(0.1903)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1561) KL loss: tensor(0.0531)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.7497) KL loss: tensor(0.1916)
loss: NT_CE learning rate: 0.0001
training loss: tensor(2.0807) KL loss: tensor(0.2315)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2982) KL loss: tensor(0.1073)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6744) KL loss: tensor(0.1624)
         GM acc on global data: 0.331 length of data: 10000
-------------Round number:  68  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1166) KL loss: tensor(0.0568)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.7375) KL loss: tensor(0.6822)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2920) KL loss: tensor(0.0797)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2869) KL loss: tensor(0.0762)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8528) KL loss: tensor(0.1049)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3732) KL loss: tensor(0.1322)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0710) KL loss: tensor(0.0412)
loss: NT_CE learning rate: 0.0001
training loss: tensor(2.0535) KL loss: tensor(0.2140)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3826) KL loss: tensor(0.0946)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.3751) KL loss: tensor(0.1673)
         GM acc on global data: 0.3025 length of data: 10000
-------------Round number:  69  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5873) KL loss: tensor(0.1897)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4957) KL loss: tensor(0.0789)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.2261) KL loss: tensor(0.2822)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6630) KL loss: tensor(0.1633)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3429) KL loss: tensor(0.1081)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2341) KL loss: tensor(0.0646)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4768) KL loss: tensor(0.1905)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5352) KL loss: tensor(0.1668)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6621) KL loss: tensor(0.1711)
loss: NT_CE learning rate: 0.0001
training loss: tensor(4.3654) KL loss: tensor(0.1998)
         GM acc on global data: 0.3733 length of data: 10000
-------------Round number:  70  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1876) KL loss: tensor(0.0650)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2352) KL loss: tensor(0.0820)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4315) KL loss: tensor(0.1304)
loss: NT_CE learning rate: 0.0001
training loss: tensor(3.1589) KL loss: tensor(0.8565)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3828) KL loss: tensor(0.0866)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3879) KL loss: tensor(0.1031)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6446) KL loss: tensor(0.0983)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1813) KL loss: tensor(0.0617)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5304) KL loss: tensor(0.1372)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2820) KL loss: tensor(0.1022)
         GM acc on global data: 0.3854 length of data: 10000
-------------Round number:  71  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3409) KL loss: tensor(0.0956)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7635) KL loss: tensor(0.1256)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2345) KL loss: tensor(0.0564)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4485) KL loss: tensor(0.1163)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1694) KL loss: tensor(0.0655)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3231) KL loss: tensor(0.0994)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4429) KL loss: tensor(0.1594)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3326) KL loss: tensor(0.1173)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1767) KL loss: tensor(0.0622)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1837) KL loss: tensor(0.0676)
         GM acc on global data: 0.3816 length of data: 10000
-------------Round number:  72  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1014) KL loss: tensor(0.0529)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.0328) KL loss: tensor(0.1353)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5227) KL loss: tensor(0.1186)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2749) KL loss: tensor(0.0907)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2087) KL loss: tensor(0.0806)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.4436) KL loss: tensor(0.1974)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6226) KL loss: tensor(0.1366)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1809) KL loss: tensor(0.0737)
loss: NT_CE learning rate: 0.0001
training loss: tensor(2.6358) KL loss: tensor(1.3943)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3245) KL loss: tensor(0.1224)
         GM acc on global data: 0.3411 length of data: 10000
-------------Round number:  73  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2327) KL loss: tensor(0.0781)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3129) KL loss: tensor(0.1248)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2100) KL loss: tensor(0.0777)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6345) KL loss: tensor(0.1612)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3265) KL loss: tensor(0.0897)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5636) KL loss: tensor(0.1537)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2787) KL loss: tensor(0.0833)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0401) KL loss: tensor(0.0381)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4674) KL loss: tensor(0.1061)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.3583) KL loss: tensor(0.2087)
         GM acc on global data: 0.3065 length of data: 10000
-------------Round number:  74  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4940) KL loss: tensor(0.1018)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5960) KL loss: tensor(0.1442)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3950) KL loss: tensor(0.1849)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5310) KL loss: tensor(0.1115)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2701) KL loss: tensor(0.0989)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.3244) KL loss: tensor(0.0957)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1663) KL loss: tensor(0.0743)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2599) KL loss: tensor(0.0778)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.9016) KL loss: tensor(0.2165)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1703) KL loss: tensor(0.0616)
         GM acc on global data: 0.391 length of data: 10000
-------------Round number:  75  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1831) KL loss: tensor(0.0780)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.4475) KL loss: tensor(0.1567)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1783) KL loss: tensor(0.0811)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.1217) KL loss: tensor(0.1679)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2470) KL loss: tensor(0.0724)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.0657) KL loss: tensor(0.2601)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1500) KL loss: tensor(0.0945)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.9205) KL loss: tensor(0.2027)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3964) KL loss: tensor(0.1416)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1463) KL loss: tensor(0.0666)
         GM acc on global data: 0.3951 length of data: 10000
-------------Round number:  76  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8267) KL loss: tensor(0.0937)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8368) KL loss: tensor(0.2282)
loss: NT_CE learning rate: 0.0001
training loss: tensor(2.7843) KL loss: tensor(1.2694)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5068) KL loss: tensor(0.1242)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.0158) KL loss: tensor(0.4707)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4077) KL loss: tensor(0.1055)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3489) KL loss: tensor(0.1216)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2412) KL loss: tensor(0.0956)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3489) KL loss: tensor(0.0963)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8730) KL loss: tensor(0.3219)
         GM acc on global data: 0.3996 length of data: 10000
-------------Round number:  77  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3028) KL loss: tensor(0.1283)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4596) KL loss: tensor(0.1961)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4513) KL loss: tensor(0.1057)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1437) KL loss: tensor(0.0647)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3039) KL loss: tensor(0.0907)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7138) KL loss: tensor(0.1428)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1337) KL loss: tensor(0.0561)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1207) KL loss: tensor(0.0697)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7578) KL loss: tensor(0.1664)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3411) KL loss: tensor(0.0950)
         GM acc on global data: 0.3885 length of data: 10000
-------------Round number:  78  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5519) KL loss: tensor(0.1120)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5162) KL loss: tensor(0.1386)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5071) KL loss: tensor(0.1041)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3291) KL loss: tensor(0.1567)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3361) KL loss: tensor(0.0949)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2111) KL loss: tensor(0.0973)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2876) KL loss: tensor(0.0825)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3472) KL loss: tensor(0.0989)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2892) KL loss: tensor(0.0951)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1536) KL loss: tensor(0.0683)
         GM acc on global data: 0.3759 length of data: 10000
-------------Round number:  79  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1348) KL loss: tensor(0.0575)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2778) KL loss: tensor(0.0981)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2112) KL loss: tensor(0.0907)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5128) KL loss: tensor(0.1148)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1516) KL loss: tensor(0.0692)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.3122) KL loss: tensor(0.5186)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.5554) KL loss: tensor(0.7182)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1716) KL loss: tensor(0.0551)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5269) KL loss: tensor(0.1237)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2320) KL loss: tensor(0.0912)
         GM acc on global data: 0.3469 length of data: 10000
-------------Round number:  80  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6270) KL loss: tensor(0.1637)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5274) KL loss: tensor(0.1499)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7015) KL loss: tensor(0.1437)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5777) KL loss: tensor(0.1815)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6048) KL loss: tensor(0.1509)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.9224) KL loss: tensor(0.5483)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.2001) KL loss: tensor(0.1456)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2803) KL loss: tensor(0.0804)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5583) KL loss: tensor(0.0952)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1523) KL loss: tensor(0.0718)
         GM acc on global data: 0.332 length of data: 10000
-------------Round number:  81  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.3423) KL loss: tensor(0.1343)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4021) KL loss: tensor(0.1578)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3234) KL loss: tensor(0.0658)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3657) KL loss: tensor(0.1095)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5058) KL loss: tensor(0.1350)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4974) KL loss: tensor(0.1357)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.0746) KL loss: tensor(0.7114)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4099) KL loss: tensor(0.1328)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0661) KL loss: tensor(0.0557)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4906) KL loss: tensor(0.0955)
         GM acc on global data: 0.3399 length of data: 10000
-------------Round number:  82  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7371) KL loss: tensor(0.1606)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2075) KL loss: tensor(0.0865)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2419) KL loss: tensor(0.0654)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1955) KL loss: tensor(0.0992)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1621) KL loss: tensor(0.0598)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.9236) KL loss: tensor(0.1888)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7317) KL loss: tensor(0.2660)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1693) KL loss: tensor(0.0769)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1289) KL loss: tensor(0.0749)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2777) KL loss: tensor(0.1015)
         GM acc on global data: 0.379 length of data: 10000
-------------Round number:  83  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1474) KL loss: tensor(0.0725)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3062) KL loss: tensor(0.1366)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7564) KL loss: tensor(0.1764)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3198) KL loss: tensor(0.1152)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2537) KL loss: tensor(0.1224)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3425) KL loss: tensor(0.0832)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.3754) KL loss: tensor(0.8831)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1751) KL loss: tensor(0.1076)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4603) KL loss: tensor(0.1379)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.1324) KL loss: tensor(0.1204)
         GM acc on global data: 0.3968 length of data: 10000
-------------Round number:  84  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2115) KL loss: tensor(0.0994)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0848) KL loss: tensor(0.0516)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4659) KL loss: tensor(0.1109)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.1956) KL loss: tensor(0.1350)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2808) KL loss: tensor(0.0804)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4018) KL loss: tensor(0.1177)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.9901) KL loss: tensor(0.5732)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1052) KL loss: tensor(0.0787)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1092) KL loss: tensor(0.0707)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.1973) KL loss: tensor(0.1219)
         GM acc on global data: 0.3083 length of data: 10000
-------------Round number:  85  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2230) KL loss: tensor(0.0438)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2989) KL loss: tensor(0.1072)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2590) KL loss: tensor(0.1135)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.4154) KL loss: tensor(0.1477)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3660) KL loss: tensor(0.0831)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.1743) KL loss: tensor(0.3082)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.0754) KL loss: tensor(0.3140)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4996) KL loss: tensor(0.1313)
loss: NT_CE learning rate: 0.0001
training loss: tensor(2.7561) KL loss: tensor(0.2262)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.0753) KL loss: tensor(0.3231)
         GM acc on global data: 0.3252 length of data: 10000
-------------Round number:  86  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1987) KL loss: tensor(0.0957)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2505) KL loss: tensor(0.0690)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1154) KL loss: tensor(0.0660)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1545) KL loss: tensor(0.0502)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6296) KL loss: tensor(0.1202)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.7887) KL loss: tensor(0.1481)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.5126) KL loss: tensor(0.5177)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.0114) KL loss: tensor(0.1573)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0949) KL loss: tensor(0.0637)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.3718) KL loss: tensor(0.1018)
         GM acc on global data: 0.3539 length of data: 10000
-------------Round number:  87  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1886) KL loss: tensor(0.0558)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0962) KL loss: tensor(0.0526)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2653) KL loss: tensor(0.0773)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.8338) KL loss: tensor(0.5548)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2699) KL loss: tensor(0.1200)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1542) KL loss: tensor(0.0742)
loss: NT_CE learning rate: 0.0001
training loss: tensor(2.5806) KL loss: tensor(0.2702)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2282) KL loss: tensor(0.0637)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2084) KL loss: tensor(0.0880)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.2679) KL loss: tensor(0.1501)
         GM acc on global data: 0.3324 length of data: 10000
-------------Round number:  88  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4456) KL loss: tensor(0.0628)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4330) KL loss: tensor(0.1419)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6850) KL loss: tensor(0.0869)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5984) KL loss: tensor(0.1205)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3443) KL loss: tensor(0.1019)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1730) KL loss: tensor(0.0853)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0743) KL loss: tensor(0.0355)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3078) KL loss: tensor(0.0755)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.9001) KL loss: tensor(0.2617)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.8857) KL loss: tensor(0.1172)
         GM acc on global data: 0.2315 length of data: 10000
-------------Round number:  89  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4114) KL loss: tensor(0.1115)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7335) KL loss: tensor(0.0961)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.4786) KL loss: tensor(0.1387)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3651) KL loss: tensor(0.0533)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3982) KL loss: tensor(0.1107)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3491) KL loss: tensor(0.0536)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3208) KL loss: tensor(0.0933)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8400) KL loss: tensor(0.1242)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8062) KL loss: tensor(0.0737)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6084) KL loss: tensor(0.0925)
         GM acc on global data: 0.3355 length of data: 10000
-------------Round number:  90  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1903) KL loss: tensor(0.0848)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4491) KL loss: tensor(0.0800)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1382) KL loss: tensor(0.0517)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3808) KL loss: tensor(0.1097)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2821) KL loss: tensor(0.1036)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6711) KL loss: tensor(0.0826)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3322) KL loss: tensor(0.0769)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5905) KL loss: tensor(0.1924)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1120) KL loss: tensor(0.0615)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2222) KL loss: tensor(0.0657)
         GM acc on global data: 0.3759 length of data: 10000
-------------Round number:  91  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1328) KL loss: tensor(0.0683)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1665) KL loss: tensor(0.0795)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5623) KL loss: tensor(0.1056)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1743) KL loss: tensor(0.0652)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2775) KL loss: tensor(0.0998)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6142) KL loss: tensor(0.2500)
loss: NT_CE learning rate: 0.0001
training loss: tensor(3.4944) KL loss: tensor(1.4216)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.6810) KL loss: tensor(0.2769)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2108) KL loss: tensor(0.0993)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.6439) KL loss: tensor(0.1394)
         GM acc on global data: 0.343 length of data: 10000
-------------Round number:  92  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3335) KL loss: tensor(0.1150)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5845) KL loss: tensor(0.1093)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.4318) KL loss: tensor(0.1772)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2027) KL loss: tensor(0.1096)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5959) KL loss: tensor(0.1583)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1658) KL loss: tensor(0.0835)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.1485) KL loss: tensor(0.3126)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6816) KL loss: tensor(0.1305)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2631) KL loss: tensor(0.0773)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0698) KL loss: tensor(0.0552)
         GM acc on global data: 0.3744 length of data: 10000
-------------Round number:  93  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0605) KL loss: tensor(0.0532)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.9715) KL loss: tensor(0.0926)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2472) KL loss: tensor(0.0649)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8455) KL loss: tensor(0.1439)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8511) KL loss: tensor(0.1576)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6457) KL loss: tensor(0.1357)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.1069) KL loss: tensor(0.1077)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2890) KL loss: tensor(0.1067)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4045) KL loss: tensor(0.1118)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1530) KL loss: tensor(0.0960)
         GM acc on global data: 0.2274 length of data: 10000
-------------Round number:  94  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6088) KL loss: tensor(0.1873)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.9243) KL loss: tensor(0.1063)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6338) KL loss: tensor(0.0589)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5762) KL loss: tensor(0.0889)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6315) KL loss: tensor(0.0717)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5152) KL loss: tensor(0.1010)
loss: NT_CE learning rate: 0.0001
training loss: tensor(2.0660) KL loss: tensor(0.3301)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.3435) KL loss: tensor(0.1477)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.0050) KL loss: tensor(0.1336)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4230) KL loss: tensor(0.0711)
         GM acc on global data: 0.3428 length of data: 10000
-------------Round number:  95  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5407) KL loss: tensor(0.1122)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.9777) KL loss: tensor(0.1310)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8831) KL loss: tensor(0.0926)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6114) KL loss: tensor(0.1335)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.0929) KL loss: tensor(0.1049)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.2679) KL loss: tensor(0.1658)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7714) KL loss: tensor(0.1251)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6250) KL loss: tensor(0.0879)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1790) KL loss: tensor(0.0908)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5180) KL loss: tensor(0.0972)
         GM acc on global data: 0.3877 length of data: 10000
-------------Round number:  96  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3753) KL loss: tensor(0.1076)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2331) KL loss: tensor(0.1118)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2193) KL loss: tensor(0.0674)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2884) KL loss: tensor(0.0799)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6006) KL loss: tensor(0.1104)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1427) KL loss: tensor(0.0615)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0706) KL loss: tensor(0.0579)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5889) KL loss: tensor(0.1179)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3553) KL loss: tensor(0.0985)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2027) KL loss: tensor(0.0819)
         GM acc on global data: 0.3108 length of data: 10000
-------------Round number:  97  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5268) KL loss: tensor(0.0805)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5060) KL loss: tensor(0.0809)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4667) KL loss: tensor(0.0955)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2165) KL loss: tensor(0.0427)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5780) KL loss: tensor(0.0717)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2250) KL loss: tensor(0.0704)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2194) KL loss: tensor(0.1142)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3993) KL loss: tensor(0.0721)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1258) KL loss: tensor(0.0614)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.4657) KL loss: tensor(0.1024)
         GM acc on global data: 0.3777 length of data: 10000
-------------Round number:  98  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7550) KL loss: tensor(0.1404)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2588) KL loss: tensor(0.0938)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3921) KL loss: tensor(0.1418)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3186) KL loss: tensor(0.1205)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2625) KL loss: tensor(0.0982)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0842) KL loss: tensor(0.0562)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2003) KL loss: tensor(0.0692)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2379) KL loss: tensor(0.0743)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2362) KL loss: tensor(0.0883)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1012) KL loss: tensor(0.0497)
         GM acc on global data: 0.3756 length of data: 10000
-------------Round number:  99  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3282) KL loss: tensor(0.1200)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0758) KL loss: tensor(0.0534)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.4807) KL loss: tensor(0.5008)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2756) KL loss: tensor(0.0923)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1393) KL loss: tensor(0.0541)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5667) KL loss: tensor(0.1176)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1513) KL loss: tensor(0.0624)
loss: NT_CE learning rate: 0.0001
training loss: tensor(1.1010) KL loss: tensor(0.0850)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8696) KL loss: tensor(0.3160)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4381) KL loss: tensor(0.0813)
         GM acc on global data: 0.3923 length of data: 10000
