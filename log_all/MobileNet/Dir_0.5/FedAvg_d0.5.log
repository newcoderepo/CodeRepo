nohup: ignoring input
================================================================================
Summary of training process:
Dataset                : Cifar10
Batch size             : 64
Learing rate           : 0.001
Number of total clients: 100
Split method           : distribution
Split parameter        : 0.5
Clients per round      : 10
Number of global rounds: 100
Number of local rounds : 10
Feature from layer     : 11
Feature percentage     : 0.1
Local training loss    : CE
Loss of beta           : 1.0
Algorithm              : FedAvg
Modelname              : MOBNET
Mode                   : training
Seed                   : 0
================================================================================
Files already downloaded and verified
Files already downloaded and verified
Here
Done
Class frequencies:
client,class0,class1,class2,class3,class4,class5,class6,class7,class8,class9,Amount
Client   0,0.007,0.034,0.041,0.104,0.007,0.108,0.00,0.698,0.00,0.00,729
Client   1,0.146,0.027,0.017,0.00,0.124,0.407,0.080,0.129,0.029,0.039,410
Client   2,0.037,0.163,0.019,0.023,0.326,0.028,0.023,0.302,0.023,0.056,215
Client   3,0.175,0.006,0.004,0.004,0.006,0.044,0.519,0.138,0.105,0.00,543
Client   4,0.002,0.079,0.002,0.002,0.068,0.075,0.066,0.599,0.042,0.066,456
Client   5,0.330,0.028,0.075,0.511,0.057,0.00,0.00,0.00,0.00,0.00,509
Client   6,0.002,0.317,0.081,0.006,0.106,0.092,0.017,0.224,0.156,0.00,545
Client   7,0.059,0.061,0.107,0.311,0.046,0.162,0.007,0.099,0.149,0.00,544
Client   8,0.055,0.003,0.227,0.428,0.095,0.006,0.00,0.014,0.034,0.138,348
Client   9,0.063,0.058,0.150,0.225,0.124,0.043,0.075,0.017,0.184,0.061,347
Client  10,0.007,0.078,0.129,0.035,0.001,0.136,0.003,0.036,0.149,0.424,689
Client  11,0.051,0.004,0.004,0.022,0.125,0.022,0.060,0.00,0.575,0.136,449
Client  12,0.060,0.107,0.324,0.013,0.00,0.130,0.003,0.157,0.104,0.100,299
Client  13,0.00,0.003,0.152,0.200,0.031,0.00,0.005,0.120,0.00,0.489,585
Client  14,0.003,0.243,0.119,0.228,0.044,0.095,0.00,0.00,0.268,0.00,639
Client  15,0.030,0.234,0.017,0.019,0.177,0.00,0.298,0.226,0.00,0.00,531
Client  16,0.028,0.00,0.135,0.403,0.015,0.220,0.163,0.037,0.00,0.00,400
Client  17,0.099,0.005,0.056,0.025,0.064,0.285,0.016,0.451,0.00,0.00,628
Client  18,0.157,0.133,0.215,0.013,0.045,0.011,0.077,0.096,0.202,0.051,376
Client  19,0.385,0.093,0.056,0.096,0.00,0.119,0.093,0.152,0.00,0.007,270
Client  20,0.136,0.00,0.056,0.032,0.136,0.096,0.144,0.160,0.128,0.112,125
Client  21,0.013,0.051,0.115,0.194,0.013,0.120,0.071,0.231,0.192,0.00,468
Client  22,0.246,0.004,0.011,0.011,0.022,0.138,0.062,0.098,0.199,0.210,276
Client  23,0.019,0.087,0.049,0.107,0.132,0.371,0.060,0.175,0.00,0.00,531
Client  24,0.006,0.099,0.227,0.193,0.066,0.068,0.163,0.139,0.040,0.00,503
Client  25,0.073,0.034,0.015,0.017,0.228,0.002,0.007,0.545,0.041,0.039,413
Client  26,0.049,0.081,0.104,0.099,0.003,0.003,0.265,0.200,0.010,0.187,385
Client  27,0.024,0.409,0.001,0.00,0.566,0.00,0.00,0.00,0.00,0.00,1007
Client  28,0.025,0.013,0.134,0.293,0.108,0.006,0.089,0.00,0.070,0.261,314
Client  29,0.00,0.580,0.063,0.357,0.00,0.00,0.00,0.00,0.00,0.00,607
Client  30,0.011,0.006,0.00,0.057,0.001,0.00,0.016,0.093,0.815,0.00,795
Client  31,0.218,0.244,0.171,0.367,0.00,0.00,0.00,0.00,0.00,0.00,532
Client  32,0.001,0.00,0.310,0.00,0.069,0.057,0.048,0.007,0.016,0.492,706
Client  33,0.163,0.002,0.261,0.149,0.021,0.00,0.274,0.130,0.00,0.00,529
Client  34,0.030,0.003,0.090,0.276,0.047,0.017,0.196,0.047,0.083,0.213,301
Client  35,0.021,0.213,0.002,0.034,0.247,0.484,0.00,0.00,0.00,0.00,616
Client  36,0.020,0.190,0.018,0.108,0.029,0.119,0.143,0.192,0.033,0.148,453
Client  37,0.355,0.040,0.064,0.012,0.171,0.202,0.004,0.008,0.083,0.062,519
Client  38,0.00,0.023,0.438,0.355,0.011,0.109,0.004,0.019,0.042,0.00,265
Client  39,0.175,0.025,0.196,0.153,0.004,0.015,0.167,0.00,0.004,0.262,275
Client  40,0.482,0.324,0.033,0.004,0.158,0.00,0.00,0.00,0.00,0.00,546
Client  41,0.170,0.021,0.00,0.503,0.107,0.010,0.016,0.079,0.018,0.076,382
Client  42,0.004,0.002,0.503,0.143,0.111,0.238,0.00,0.00,0.00,0.00,505
Client  43,0.013,0.294,0.243,0.013,0.141,0.00,0.192,0.00,0.105,0.00,313
Client  44,0.158,0.047,0.011,0.158,0.007,0.007,0.452,0.079,0.029,0.054,279
Client  45,0.00,0.285,0.226,0.159,0.114,0.059,0.114,0.043,0.00,0.00,509
Client  46,0.525,0.191,0.284,0.00,0.00,0.00,0.00,0.00,0.00,0.00,598
Client  47,0.019,0.201,0.002,0.011,0.062,0.148,0.113,0.024,0.420,0.00,628
Client  48,0.028,0.109,0.019,0.057,0.057,0.066,0.209,0.090,0.289,0.076,211
Client  49,0.049,0.106,0.036,0.515,0.155,0.140,0.00,0.00,0.00,0.00,530
Client  50,0.029,0.044,0.323,0.223,0.006,0.019,0.356,0.00,0.00,0.00,685
Client  51,0.072,0.003,0.00,0.038,0.107,0.433,0.012,0.060,0.275,0.00,582
Client  52,0.001,0.011,0.050,0.030,0.026,0.00,0.178,0.008,0.154,0.543,741
Client  53,0.025,0.035,0.003,0.203,0.00,0.008,0.013,0.062,0.007,0.643,597
Client  54,0.027,0.263,0.015,0.042,0.066,0.355,0.00,0.216,0.004,0.012,259
Client  55,0.008,0.093,0.097,0.00,0.068,0.545,0.004,0.027,0.129,0.027,473
Client  56,0.077,0.009,0.009,0.014,0.009,0.066,0.229,0.318,0.267,0.00,647
Client  57,0.142,0.453,0.015,0.088,0.301,0.00,0.00,0.00,0.00,0.00,534
Client  58,0.161,0.074,0.107,0.004,0.009,0.035,0.090,0.057,0.464,0.00,690
Client  59,0.101,0.149,0.073,0.261,0.054,0.363,0.00,0.00,0.00,0.00,537
Client  60,0.066,0.002,0.191,0.026,0.033,0.193,0.060,0.002,0.153,0.274,580
Client  61,0.00,0.048,0.00,0.213,0.002,0.046,0.065,0.120,0.179,0.326,475
Client  62,0.030,0.001,0.001,0.006,0.074,0.008,0.009,0.258,0.613,0.00,979
Client  63,0.019,0.017,0.008,0.053,0.212,0.006,0.168,0.452,0.065,0.00,524
Client  64,0.005,0.010,0.007,0.010,0.421,0.300,0.00,0.058,0.002,0.188,416
Client  65,0.002,0.037,0.202,0.019,0.301,0.041,0.287,0.031,0.080,0.00,515
Client  66,0.009,0.00,0.251,0.136,0.002,0.319,0.049,0.049,0.178,0.007,427
Client  67,0.266,0.074,0.039,0.238,0.004,0.006,0.271,0.027,0.074,0.00,512
Client  68,0.532,0.037,0.005,0.053,0.005,0.037,0.163,0.142,0.016,0.011,190
Client  69,0.324,0.054,0.058,0.002,0.00,0.135,0.002,0.137,0.258,0.029,481
Client  70,0.048,0.006,0.075,0.025,0.847,0.00,0.00,0.00,0.00,0.00,881
Client  71,0.124,0.00,0.002,0.020,0.005,0.005,0.845,0.00,0.00,0.00,611
Client  72,0.023,0.105,0.041,0.395,0.013,0.020,0.005,0.212,0.00,0.186,392
Client  73,0.002,0.135,0.211,0.052,0.023,0.002,0.478,0.002,0.095,0.00,517
Client  74,0.024,0.128,0.104,0.002,0.177,0.234,0.051,0.113,0.077,0.091,453
Client  75,0.302,0.004,0.028,0.004,0.317,0.002,0.043,0.265,0.028,0.006,464
Client  76,0.015,0.097,0.005,0.077,0.005,0.116,0.192,0.035,0.018,0.441,662
Client  77,0.105,0.003,0.022,0.301,0.056,0.121,0.003,0.263,0.070,0.056,372
Client  78,0.278,0.045,0.106,0.036,0.005,0.025,0.323,0.009,0.097,0.077,443
Client  79,0.239,0.00,0.190,0.104,0.019,0.321,0.019,0.011,0.004,0.093,268
Client  80,0.002,0.011,0.257,0.097,0.037,0.037,0.054,0.166,0.006,0.333,463
Client  81,0.039,0.068,0.093,0.148,0.005,0.015,0.017,0.005,0.011,0.600,648
Client  82,0.253,0.387,0.048,0.089,0.007,0.00,0.010,0.051,0.144,0.010,292
Client  83,0.176,0.223,0.144,0.036,0.005,0.106,0.310,0.00,0.00,0.00,632
Client  84,0.035,0.009,0.200,0.016,0.167,0.019,0.00,0.100,0.012,0.442,430
Client  85,0.390,0.292,0.00,0.00,0.003,0.019,0.066,0.027,0.077,0.127,377
Client  86,0.182,0.364,0.016,0.022,0.057,0.149,0.00,0.030,0.041,0.139,368
Client  87,0.048,0.402,0.088,0.108,0.088,0.253,0.008,0.00,0.004,0.00,249
Client  88,0.120,0.032,0.162,0.056,0.00,0.241,0.00,0.074,0.134,0.181,216
Client  89,0.219,0.119,0.144,0.039,0.054,0.027,0.399,0.00,0.00,0.00,672
Client  90,0.417,0.008,0.00,0.035,0.009,0.00,0.121,0.020,0.390,0.00,782
Client  91,0.017,0.00,0.247,0.023,0.008,0.00,0.064,0.099,0.012,0.530,922
Client  92,0.367,0.004,0.074,0.020,0.026,0.00,0.028,0.032,0.072,0.377,501
Client  93,0.087,0.049,0.042,0.069,0.286,0.016,0.016,0.435,0.00,0.00,612
Client  94,0.015,0.092,0.007,0.033,0.261,0.592,0.00,0.00,0.00,0.00,671
Client  95,0.136,0.003,0.413,0.043,0.00,0.050,0.355,0.00,0.00,0.00,698
Client  96,0.046,0.034,0.114,0.009,0.462,0.052,0.009,0.00,0.135,0.138,325
Client  97,0.247,0.006,0.071,0.010,0.066,0.164,0.048,0.301,0.087,0.00,518
Client  98,0.011,0.019,0.162,0.151,0.207,0.323,0.127,0.00,0.00,0.00,569
Client  99,0.122,0.619,0.006,0.012,0.107,0.132,0.00,0.002,0.00,0.00,515
Num_samples of Training set per client: [729, 410, 215, 543, 456, 509, 545, 544, 348, 347, 689, 449, 299, 585, 639, 531, 400, 628, 376, 270, 125, 468, 276, 531, 503, 413, 385, 1007, 314, 607, 795, 532, 706, 529, 301, 616, 453, 519, 265, 275, 546, 382, 505, 313, 279, 509, 598, 628, 211, 530, 685, 582, 741, 597, 259, 473, 647, 534, 690, 537, 580, 475, 979, 524, 416, 515, 427, 512, 190, 481, 881, 611, 392, 517, 453, 464, 662, 372, 443, 268, 463, 648, 292, 632, 430, 377, 368, 249, 216, 672, 782, 922, 501, 612, 671, 698, 325, 518, 569, 515]
Total_training_samples: 50000
Global test set: 10000
Finish Generating Samples, distribution saved
MobileNetV2(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layers): Sequential(
    (0): Block(
      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Block(
      (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (6): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (7): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (8): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (9): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (10): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (12): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (13): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (14): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (15): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (16): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv2): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear): Linear(in_features=1280, out_features=10, bias=True)
)
number of parameters: 2296922
clients initializting...
output size: 10
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<01:22,  1.20it/s]  2%|▏         | 2/100 [00:01<01:23,  1.17it/s]  3%|▎         | 3/100 [00:02<01:25,  1.14it/s]  4%|▍         | 4/100 [00:03<01:25,  1.13it/s]  5%|▌         | 5/100 [00:04<01:26,  1.10it/s]  6%|▌         | 6/100 [00:05<01:21,  1.15it/s]  7%|▋         | 7/100 [00:06<01:23,  1.12it/s]  8%|▊         | 8/100 [00:07<01:23,  1.10it/s]  9%|▉         | 9/100 [00:08<01:23,  1.09it/s] 10%|█         | 10/100 [00:09<01:23,  1.07it/s] 11%|█         | 11/100 [00:10<01:24,  1.05it/s] 12%|█▏        | 12/100 [00:11<01:23,  1.05it/s] 13%|█▎        | 13/100 [00:12<01:23,  1.05it/s] 14%|█▍        | 14/100 [00:12<01:18,  1.10it/s] 15%|█▌        | 15/100 [00:13<01:14,  1.14it/s] 16%|█▌        | 16/100 [00:14<01:10,  1.19it/s] 17%|█▋        | 17/100 [00:15<01:09,  1.20it/s] 18%|█▊        | 18/100 [00:16<01:09,  1.18it/s] 19%|█▉        | 19/100 [00:17<01:11,  1.14it/s] 20%|██        | 20/100 [00:17<01:10,  1.14it/s] 21%|██        | 21/100 [00:18<01:10,  1.13it/s] 22%|██▏       | 22/100 [00:19<01:10,  1.10it/s] 23%|██▎       | 23/100 [00:20<01:11,  1.08it/s] 24%|██▍       | 24/100 [00:21<01:09,  1.10it/s] 25%|██▌       | 25/100 [00:22<01:08,  1.10it/s] 26%|██▌       | 26/100 [00:26<02:21,  1.91s/it] 27%|██▋       | 27/100 [00:27<01:57,  1.61s/it] 28%|██▊       | 28/100 [00:28<01:36,  1.34s/it] 29%|██▉       | 29/100 [00:29<01:25,  1.21s/it] 30%|███       | 30/100 [00:29<01:13,  1.05s/it] 31%|███       | 31/100 [00:30<01:08,  1.01it/s] 32%|███▏      | 32/100 [00:31<01:06,  1.02it/s] 33%|███▎      | 33/100 [00:32<01:03,  1.05it/s] 34%|███▍      | 34/100 [00:33<01:00,  1.09it/s] 35%|███▌      | 35/100 [00:34<01:00,  1.08it/s] 36%|███▌      | 36/100 [00:35<00:56,  1.13it/s] 37%|███▋      | 37/100 [00:36<00:56,  1.11it/s] 38%|███▊      | 38/100 [00:37<00:57,  1.08it/s] 39%|███▉      | 39/100 [00:38<00:55,  1.10it/s] 40%|████      | 40/100 [00:38<00:54,  1.11it/s] 41%|████      | 41/100 [00:39<00:49,  1.18it/s] 42%|████▏     | 42/100 [00:40<00:49,  1.16it/s] 43%|████▎     | 43/100 [00:41<00:47,  1.20it/s] 44%|████▍     | 44/100 [00:42<00:46,  1.21it/s] 45%|████▌     | 45/100 [00:43<00:47,  1.16it/s] 46%|████▌     | 46/100 [00:43<00:45,  1.18it/s] 47%|████▋     | 47/100 [00:44<00:41,  1.27it/s] 48%|████▊     | 48/100 [00:45<00:42,  1.21it/s] 49%|████▉     | 49/100 [00:46<00:43,  1.17it/s] 50%|█████     | 50/100 [00:47<00:41,  1.21it/s] 51%|█████     | 51/100 [00:47<00:40,  1.21it/s] 52%|█████▏    | 52/100 [00:48<00:40,  1.20it/s] 53%|█████▎    | 53/100 [00:49<00:40,  1.17it/s] 54%|█████▍    | 54/100 [00:54<01:27,  1.89s/it] 55%|█████▌    | 55/100 [00:54<01:12,  1.61s/it] 56%|█████▌    | 56/100 [00:55<01:02,  1.43s/it] 57%|█████▋    | 57/100 [00:56<00:54,  1.28s/it] 58%|█████▊    | 58/100 [00:57<00:46,  1.12s/it] 59%|█████▉    | 59/100 [00:58<00:43,  1.05s/it] 60%|██████    | 60/100 [00:59<00:39,  1.02it/s] 61%|██████    | 61/100 [01:00<00:37,  1.03it/s] 62%|██████▏   | 62/100 [01:01<00:35,  1.08it/s] 63%|██████▎   | 63/100 [01:02<00:34,  1.09it/s] 64%|██████▍   | 64/100 [01:02<00:32,  1.09it/s] 65%|██████▌   | 65/100 [01:03<00:33,  1.06it/s] 66%|██████▌   | 66/100 [01:04<00:31,  1.07it/s] 67%|██████▋   | 67/100 [01:05<00:30,  1.07it/s] 68%|██████▊   | 68/100 [01:06<00:29,  1.08it/s] 69%|██████▉   | 69/100 [01:07<00:28,  1.08it/s] 70%|███████   | 70/100 [01:08<00:27,  1.09it/s] 71%|███████   | 71/100 [01:09<00:25,  1.14it/s] 72%|███████▏  | 72/100 [01:10<00:24,  1.16it/s] 73%|███████▎  | 73/100 [01:11<00:23,  1.15it/s] 74%|███████▍  | 74/100 [01:11<00:23,  1.13it/s] 75%|███████▌  | 75/100 [01:12<00:22,  1.10it/s] 76%|███████▌  | 76/100 [01:13<00:21,  1.09it/s] 77%|███████▋  | 77/100 [01:14<00:21,  1.07it/s] 78%|███████▊  | 78/100 [01:15<00:20,  1.07it/s] 79%|███████▉  | 79/100 [01:16<00:19,  1.06it/s] 80%|████████  | 80/100 [01:21<00:40,  2.01s/it] 81%|████████  | 81/100 [01:22<00:32,  1.71s/it] 82%|████████▏ | 82/100 [01:23<00:26,  1.50s/it] 83%|████████▎ | 83/100 [01:24<00:22,  1.33s/it] 84%|████████▍ | 84/100 [01:25<00:19,  1.20s/it] 85%|████████▌ | 85/100 [01:26<00:17,  1.17s/it] 86%|████████▌ | 86/100 [01:27<00:15,  1.09s/it] 87%|████████▋ | 87/100 [01:27<00:13,  1.04s/it] 88%|████████▊ | 88/100 [01:28<00:11,  1.01it/s] 89%|████████▉ | 89/100 [01:29<00:10,  1.05it/s] 90%|█████████ | 90/100 [01:30<00:09,  1.09it/s] 91%|█████████ | 91/100 [01:31<00:08,  1.12it/s] 92%|█████████▏| 92/100 [01:32<00:07,  1.12it/s] 93%|█████████▎| 93/100 [01:33<00:06,  1.13it/s] 94%|█████████▍| 94/100 [01:34<00:05,  1.13it/s] 95%|█████████▌| 95/100 [01:34<00:04,  1.17it/s] 96%|█████████▌| 96/100 [01:35<00:03,  1.20it/s] 97%|█████████▋| 97/100 [01:36<00:02,  1.17it/s] 98%|█████████▊| 98/100 [01:37<00:01,  1.14it/s] 99%|█████████▉| 99/100 [01:38<00:00,  1.12it/s]100%|██████████| 100/100 [01:39<00:00,  1.11it/s]100%|██████████| 100/100 [01:39<00:00,  1.01it/s]
Number of users per round / total users: 10  /  100
Finished creating FL server.
=== Training starts: algorithm FedAvg ===
-------------Round number:  0  -------------
loss: CE learning rate: 0.001
training loss: tensor(1.1630)
loss: CE learning rate: 0.001
training loss: tensor(0.9830)
loss: CE learning rate: 0.001
training loss: tensor(0.9160)
loss: CE learning rate: 0.001
training loss: tensor(0.8859)
loss: CE learning rate: 0.001
training loss: tensor(0.9783)
loss: CE learning rate: 0.001
training loss: tensor(0.9293)
loss: CE learning rate: 0.001
training loss: tensor(0.9931)
loss: CE learning rate: 0.001
training loss: tensor(0.9644)
loss: CE learning rate: 0.001
training loss: tensor(1.0952)
loss: CE learning rate: 0.001
training loss: tensor(0.8032)
Global Model Acc on global data: 0.1 length of data: 10000
save a model
-------------Round number:  1  -------------
loss: CE learning rate: 0.000982
training loss: tensor(1.1149)
loss: CE learning rate: 0.000982
training loss: tensor(1.0587)
loss: CE learning rate: 0.000982
training loss: tensor(0.7868)
loss: CE learning rate: 0.000982
training loss: tensor(1.2286)
loss: CE learning rate: 0.000982
training loss: tensor(0.7465)
loss: CE learning rate: 0.000982
training loss: tensor(0.8299)
loss: CE learning rate: 0.000982
training loss: tensor(1.1996)
loss: CE learning rate: 0.000982
training loss: tensor(0.9759)
loss: CE learning rate: 0.000982
training loss: tensor(1.0247)
loss: CE learning rate: 0.000982
training loss: tensor(0.9398)
Global Model Acc on global data: 0.1007 length of data: 10000
save a model
-------------Round number:  2  -------------
loss: CE learning rate: 0.000964
training loss: tensor(0.6778)
loss: CE learning rate: 0.000964
training loss: tensor(0.7126)
loss: CE learning rate: 0.000964
training loss: tensor(0.8265)
loss: CE learning rate: 0.000964
training loss: tensor(0.7663)
loss: CE learning rate: 0.000964
training loss: tensor(0.7567)
loss: CE learning rate: 0.000964
training loss: tensor(0.7729)
loss: CE learning rate: 0.000964
training loss: tensor(0.6630)
loss: CE learning rate: 0.000964
training loss: tensor(0.2804)
loss: CE learning rate: 0.000964
training loss: tensor(0.7706)
loss: CE learning rate: 0.000964
training loss: tensor(0.7339)
Global Model Acc on global data: 0.1138 length of data: 10000
save a model
-------------Round number:  3  -------------
loss: CE learning rate: 0.000946
training loss: tensor(0.4606)
loss: CE learning rate: 0.000946
training loss: tensor(0.4491)
loss: CE learning rate: 0.000946
training loss: tensor(0.6480)
loss: CE learning rate: 0.000946
training loss: tensor(0.6835)
loss: CE learning rate: 0.000946
training loss: tensor(0.8395)
loss: CE learning rate: 0.000946
training loss: tensor(0.5294)
loss: CE learning rate: 0.000946
training loss: tensor(0.8046)
loss: CE learning rate: 0.000946
training loss: tensor(0.6529)
loss: CE learning rate: 0.000946
training loss: tensor(0.4841)
loss: CE learning rate: 0.000946
training loss: tensor(0.4904)
Global Model Acc on global data: 0.1594 length of data: 10000
save a model
-------------Round number:  4  -------------
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.9640)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.8934)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.8433)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5809)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5817)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5267)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.9558)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.6675)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.5507)
loss: CE learning rate: 0.0009280000000000001
training loss: tensor(0.6193)
Global Model Acc on global data: 0.2957 length of data: 10000
save a model
-------------Round number:  5  -------------
loss: CE learning rate: 0.00091
training loss: tensor(0.5062)
loss: CE learning rate: 0.00091
training loss: tensor(0.6478)
loss: CE learning rate: 0.00091
training loss: tensor(0.4810)
loss: CE learning rate: 0.00091
training loss: tensor(0.6078)
loss: CE learning rate: 0.00091
training loss: tensor(0.7485)
loss: CE learning rate: 0.00091
training loss: tensor(0.5196)
loss: CE learning rate: 0.00091
training loss: tensor(0.5274)
loss: CE learning rate: 0.00091
training loss: tensor(0.5048)
loss: CE learning rate: 0.00091
training loss: tensor(0.4223)
loss: CE learning rate: 0.00091
training loss: tensor(0.4000)
Global Model Acc on global data: 0.2754 length of data: 10000
-------------Round number:  6  -------------
loss: CE learning rate: 0.000892
training loss: tensor(0.3735)
loss: CE learning rate: 0.000892
training loss: tensor(0.7108)
loss: CE learning rate: 0.000892
training loss: tensor(0.4246)
loss: CE learning rate: 0.000892
training loss: tensor(0.4983)
loss: CE learning rate: 0.000892
training loss: tensor(0.7174)
loss: CE learning rate: 0.000892
training loss: tensor(0.5467)
loss: CE learning rate: 0.000892
training loss: tensor(0.6159)
loss: CE learning rate: 0.000892
training loss: tensor(0.4766)
loss: CE learning rate: 0.000892
training loss: tensor(0.8689)
loss: CE learning rate: 0.000892
training loss: tensor(0.6291)
Global Model Acc on global data: 0.3386 length of data: 10000
save a model
-------------Round number:  7  -------------
loss: CE learning rate: 0.000874
training loss: tensor(0.6338)
loss: CE learning rate: 0.000874
training loss: tensor(1.1110)
loss: CE learning rate: 0.000874
training loss: tensor(0.5626)
loss: CE learning rate: 0.000874
training loss: tensor(0.4176)
loss: CE learning rate: 0.000874
training loss: tensor(0.4611)
loss: CE learning rate: 0.000874
training loss: tensor(0.4888)
loss: CE learning rate: 0.000874
training loss: tensor(0.4321)
loss: CE learning rate: 0.000874
training loss: tensor(0.5167)
loss: CE learning rate: 0.000874
training loss: tensor(0.4125)
loss: CE learning rate: 0.000874
training loss: tensor(0.6950)
Global Model Acc on global data: 0.2915 length of data: 10000
-------------Round number:  8  -------------
loss: CE learning rate: 0.000856
training loss: tensor(0.2946)
loss: CE learning rate: 0.000856
training loss: tensor(0.4733)
loss: CE learning rate: 0.000856
training loss: tensor(0.5125)
loss: CE learning rate: 0.000856
training loss: tensor(0.3754)
loss: CE learning rate: 0.000856
training loss: tensor(0.4833)
loss: CE learning rate: 0.000856
training loss: tensor(0.5220)
loss: CE learning rate: 0.000856
training loss: tensor(0.3617)
loss: CE learning rate: 0.000856
training loss: tensor(0.2458)
loss: CE learning rate: 0.000856
training loss: tensor(0.3949)
loss: CE learning rate: 0.000856
training loss: tensor(0.3665)
Global Model Acc on global data: 0.3216 length of data: 10000
-------------Round number:  9  -------------
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.3343)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.4690)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.2804)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.3560)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.4710)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.6427)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.2950)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.4124)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.2686)
loss: CE learning rate: 0.0008380000000000001
training loss: tensor(0.5738)
Global Model Acc on global data: 0.2737 length of data: 10000
-------------Round number:  10  -------------
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.4299)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.4077)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3581)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.4830)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.4389)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3392)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.5400)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.5268)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.5014)
loss: CE learning rate: 0.0008200000000000001
training loss: tensor(0.3709)
Global Model Acc on global data: 0.374 length of data: 10000
save a model
-------------Round number:  11  -------------
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.2111)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3492)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.4520)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3719)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.2882)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.4287)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.2364)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.2321)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3103)
loss: CE learning rate: 0.0008020000000000001
training loss: tensor(0.3545)
Global Model Acc on global data: 0.3456 length of data: 10000
-------------Round number:  12  -------------
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.2685)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.2911)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.5231)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.2243)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.2783)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.3289)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.3790)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.3792)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.3771)
loss: CE learning rate: 0.0007840000000000001
training loss: tensor(0.6259)
Global Model Acc on global data: 0.3623 length of data: 10000
-------------Round number:  13  -------------
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3551)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.1813)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3246)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.1757)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.2867)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.4280)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.4561)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.2151)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.3566)
loss: CE learning rate: 0.0007660000000000001
training loss: tensor(0.2744)
Global Model Acc on global data: 0.3716 length of data: 10000
-------------Round number:  14  -------------
loss: CE learning rate: 0.000748
training loss: tensor(0.4437)
loss: CE learning rate: 0.000748
training loss: tensor(0.5591)
loss: CE learning rate: 0.000748
training loss: tensor(0.3045)
loss: CE learning rate: 0.000748
training loss: tensor(0.5057)
loss: CE learning rate: 0.000748
training loss: tensor(0.3753)
loss: CE learning rate: 0.000748
training loss: tensor(0.2258)
loss: CE learning rate: 0.000748
training loss: tensor(0.3089)
loss: CE learning rate: 0.000748
training loss: tensor(0.2166)
loss: CE learning rate: 0.000748
training loss: tensor(0.4080)
loss: CE learning rate: 0.000748
training loss: tensor(0.6136)
Global Model Acc on global data: 0.3441 length of data: 10000
-------------Round number:  15  -------------
loss: CE learning rate: 0.00073
training loss: tensor(0.3325)
loss: CE learning rate: 0.00073
training loss: tensor(0.6287)
loss: CE learning rate: 0.00073
training loss: tensor(0.1752)
loss: CE learning rate: 0.00073
training loss: tensor(0.4271)
loss: CE learning rate: 0.00073
training loss: tensor(0.1653)
loss: CE learning rate: 0.00073
training loss: tensor(0.3484)
loss: CE learning rate: 0.00073
training loss: tensor(0.3101)
loss: CE learning rate: 0.00073
training loss: tensor(0.1699)
loss: CE learning rate: 0.00073
training loss: tensor(0.6867)
loss: CE learning rate: 0.00073
training loss: tensor(0.4130)
Global Model Acc on global data: 0.3638 length of data: 10000
-------------Round number:  16  -------------
loss: CE learning rate: 0.000712
training loss: tensor(0.3818)
loss: CE learning rate: 0.000712
training loss: tensor(0.3819)
loss: CE learning rate: 0.000712
training loss: tensor(0.5864)
loss: CE learning rate: 0.000712
training loss: tensor(0.3307)
loss: CE learning rate: 0.000712
training loss: tensor(0.3046)
loss: CE learning rate: 0.000712
training loss: tensor(0.1547)
loss: CE learning rate: 0.000712
training loss: tensor(0.4059)
loss: CE learning rate: 0.000712
training loss: tensor(0.4289)
loss: CE learning rate: 0.000712
training loss: tensor(0.4555)
loss: CE learning rate: 0.000712
training loss: tensor(0.3506)
Global Model Acc on global data: 0.4047 length of data: 10000
save a model
-------------Round number:  17  -------------
loss: CE learning rate: 0.000694
training loss: tensor(0.2680)
loss: CE learning rate: 0.000694
training loss: tensor(0.4524)
loss: CE learning rate: 0.000694
training loss: tensor(0.2727)
loss: CE learning rate: 0.000694
training loss: tensor(0.4346)
loss: CE learning rate: 0.000694
training loss: tensor(0.3933)
loss: CE learning rate: 0.000694
training loss: tensor(0.4560)
loss: CE learning rate: 0.000694
training loss: tensor(0.2982)
loss: CE learning rate: 0.000694
training loss: tensor(0.4110)
loss: CE learning rate: 0.000694
training loss: tensor(0.1980)
loss: CE learning rate: 0.000694
training loss: tensor(0.2636)
Global Model Acc on global data: 0.3795 length of data: 10000
-------------Round number:  18  -------------
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3953)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.1295)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.2572)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.7887)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3221)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3763)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3825)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.3931)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.2733)
loss: CE learning rate: 0.0006760000000000001
training loss: tensor(0.2278)
Global Model Acc on global data: 0.4471 length of data: 10000
save a model
-------------Round number:  19  -------------
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.4368)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3312)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.4973)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3995)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.4095)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.5268)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3620)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.2631)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.2606)
loss: CE learning rate: 0.0006580000000000001
training loss: tensor(0.3106)
Global Model Acc on global data: 0.4039 length of data: 10000
-------------Round number:  20  -------------
loss: CE learning rate: 0.00064
training loss: tensor(0.4353)
loss: CE learning rate: 0.00064
training loss: tensor(0.5377)
loss: CE learning rate: 0.00064
training loss: tensor(0.5214)
loss: CE learning rate: 0.00064
training loss: tensor(0.2429)
loss: CE learning rate: 0.00064
training loss: tensor(0.2933)
loss: CE learning rate: 0.00064
training loss: tensor(0.3329)
loss: CE learning rate: 0.00064
training loss: tensor(0.3161)
loss: CE learning rate: 0.00064
training loss: tensor(0.4354)
loss: CE learning rate: 0.00064
training loss: tensor(0.1922)
loss: CE learning rate: 0.00064
training loss: tensor(0.3927)
Global Model Acc on global data: 0.4246 length of data: 10000
-------------Round number:  21  -------------
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.2672)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.4625)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.3368)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.3958)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.1366)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.2082)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.3157)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.2333)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.1937)
loss: CE learning rate: 0.0006220000000000002
training loss: tensor(0.6179)
Global Model Acc on global data: 0.4472 length of data: 10000
save a model
-------------Round number:  22  -------------
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2667)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2366)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2557)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.1622)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2285)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.1911)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.3322)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2274)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.3551)
loss: CE learning rate: 0.0006040000000000002
training loss: tensor(0.2066)
Global Model Acc on global data: 0.3778 length of data: 10000
-------------Round number:  23  -------------
loss: CE learning rate: 0.000586
training loss: tensor(1.2370)
loss: CE learning rate: 0.000586
training loss: tensor(0.4579)
loss: CE learning rate: 0.000586
training loss: tensor(0.2025)
loss: CE learning rate: 0.000586
training loss: tensor(0.4136)
loss: CE learning rate: 0.000586
training loss: tensor(0.5011)
loss: CE learning rate: 0.000586
training loss: tensor(0.2245)
loss: CE learning rate: 0.000586
training loss: tensor(0.4392)
loss: CE learning rate: 0.000586
training loss: tensor(0.2861)
loss: CE learning rate: 0.000586
training loss: tensor(0.6942)
loss: CE learning rate: 0.000586
training loss: tensor(0.3444)
Global Model Acc on global data: 0.4785 length of data: 10000
save a model
-------------Round number:  24  -------------
loss: CE learning rate: 0.000568
training loss: tensor(0.7292)
loss: CE learning rate: 0.000568
training loss: tensor(0.2125)
loss: CE learning rate: 0.000568
training loss: tensor(0.1169)
loss: CE learning rate: 0.000568
training loss: tensor(0.2912)
loss: CE learning rate: 0.000568
training loss: tensor(0.3973)
loss: CE learning rate: 0.000568
training loss: tensor(0.2027)
loss: CE learning rate: 0.000568
training loss: tensor(0.3660)
loss: CE learning rate: 0.000568
training loss: tensor(0.3964)
loss: CE learning rate: 0.000568
training loss: tensor(0.3955)
loss: CE learning rate: 0.000568
training loss: tensor(0.3021)
Global Model Acc on global data: 0.4671 length of data: 10000
-------------Round number:  25  -------------
loss: CE learning rate: 0.00055
training loss: tensor(0.2849)
loss: CE learning rate: 0.00055
training loss: tensor(0.1220)
loss: CE learning rate: 0.00055
training loss: tensor(0.2785)
loss: CE learning rate: 0.00055
training loss: tensor(0.6098)
loss: CE learning rate: 0.00055
training loss: tensor(0.3835)
loss: CE learning rate: 0.00055
training loss: tensor(0.3014)
loss: CE learning rate: 0.00055
training loss: tensor(0.1922)
loss: CE learning rate: 0.00055
training loss: tensor(0.2355)
loss: CE learning rate: 0.00055
training loss: tensor(0.2404)
loss: CE learning rate: 0.00055
training loss: tensor(0.4548)
Global Model Acc on global data: 0.4103 length of data: 10000
-------------Round number:  26  -------------
loss: CE learning rate: 0.000532
training loss: tensor(0.4839)
loss: CE learning rate: 0.000532
training loss: tensor(0.3775)
loss: CE learning rate: 0.000532
training loss: tensor(0.4828)
loss: CE learning rate: 0.000532
training loss: tensor(0.2529)
loss: CE learning rate: 0.000532
training loss: tensor(0.4381)
loss: CE learning rate: 0.000532
training loss: tensor(0.2910)
loss: CE learning rate: 0.000532
training loss: tensor(0.3416)
loss: CE learning rate: 0.000532
training loss: tensor(0.2920)
loss: CE learning rate: 0.000532
training loss: tensor(0.1931)
loss: CE learning rate: 0.000532
training loss: tensor(0.1653)
Global Model Acc on global data: 0.4146 length of data: 10000
-------------Round number:  27  -------------
loss: CE learning rate: 0.000514
training loss: tensor(0.1735)
loss: CE learning rate: 0.000514
training loss: tensor(0.2297)
loss: CE learning rate: 0.000514
training loss: tensor(0.2862)
loss: CE learning rate: 0.000514
training loss: tensor(0.2799)
loss: CE learning rate: 0.000514
training loss: tensor(0.1830)
loss: CE learning rate: 0.000514
training loss: tensor(0.1885)
loss: CE learning rate: 0.000514
training loss: tensor(0.1219)
loss: CE learning rate: 0.000514
training loss: tensor(0.2907)
loss: CE learning rate: 0.000514
training loss: tensor(0.5103)
loss: CE learning rate: 0.000514
training loss: tensor(0.2802)
Global Model Acc on global data: 0.3762 length of data: 10000
-------------Round number:  28  -------------
loss: CE learning rate: 0.000496
training loss: tensor(0.1980)
loss: CE learning rate: 0.000496
training loss: tensor(0.1950)
loss: CE learning rate: 0.000496
training loss: tensor(0.5128)
loss: CE learning rate: 0.000496
training loss: tensor(0.3220)
loss: CE learning rate: 0.000496
training loss: tensor(0.5621)
loss: CE learning rate: 0.000496
training loss: tensor(0.4283)
loss: CE learning rate: 0.000496
training loss: tensor(0.4575)
loss: CE learning rate: 0.000496
training loss: tensor(0.4954)
loss: CE learning rate: 0.000496
training loss: tensor(0.5440)
loss: CE learning rate: 0.000496
training loss: tensor(0.1307)
Global Model Acc on global data: 0.4689 length of data: 10000
-------------Round number:  29  -------------
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2855)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.3064)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.3227)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.3320)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.4096)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2357)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2375)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.3248)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.5928)
loss: CE learning rate: 0.0004780000000000001
training loss: tensor(0.2136)
Global Model Acc on global data: 0.4842 length of data: 10000
save a model
-------------Round number:  30  -------------
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.3159)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.1173)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2722)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2021)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2237)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.4487)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.3125)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.3347)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.2440)
loss: CE learning rate: 0.00046000000000000007
training loss: tensor(0.1093)
Global Model Acc on global data: 0.4556 length of data: 10000
-------------Round number:  31  -------------
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.3615)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2080)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2445)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2441)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.3062)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2642)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2278)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2443)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.3802)
loss: CE learning rate: 0.00044200000000000006
training loss: tensor(0.2053)
Global Model Acc on global data: 0.48 length of data: 10000
-------------Round number:  32  -------------
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.3202)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.3946)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.3259)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.4897)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.3560)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.1434)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.1911)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.3108)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.1924)
loss: CE learning rate: 0.00042400000000000006
training loss: tensor(0.7742)
Global Model Acc on global data: 0.4677 length of data: 10000
-------------Round number:  33  -------------
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.1560)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.3007)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.1299)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2714)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.3622)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2595)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.1982)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.3149)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.2253)
loss: CE learning rate: 0.00040600000000000006
training loss: tensor(0.1745)
Global Model Acc on global data: 0.4695 length of data: 10000
-------------Round number:  34  -------------
loss: CE learning rate: 0.000388
training loss: tensor(0.1076)
loss: CE learning rate: 0.000388
training loss: tensor(0.2514)
loss: CE learning rate: 0.000388
training loss: tensor(0.2507)
loss: CE learning rate: 0.000388
training loss: tensor(0.3575)
loss: CE learning rate: 0.000388
training loss: tensor(0.4238)
loss: CE learning rate: 0.000388
training loss: tensor(0.4306)
loss: CE learning rate: 0.000388
training loss: tensor(0.0951)
loss: CE learning rate: 0.000388
training loss: tensor(0.2307)
loss: CE learning rate: 0.000388
training loss: tensor(0.2084)
loss: CE learning rate: 0.000388
training loss: tensor(0.3399)
Global Model Acc on global data: 0.4841 length of data: 10000
-------------Round number:  35  -------------
loss: CE learning rate: 0.00037
training loss: tensor(0.2946)
loss: CE learning rate: 0.00037
training loss: tensor(0.2276)
loss: CE learning rate: 0.00037
training loss: tensor(0.4502)
loss: CE learning rate: 0.00037
training loss: tensor(0.0971)
loss: CE learning rate: 0.00037
training loss: tensor(0.2625)
loss: CE learning rate: 0.00037
training loss: tensor(0.1716)
loss: CE learning rate: 0.00037
training loss: tensor(0.0910)
loss: CE learning rate: 0.00037
training loss: tensor(0.3414)
loss: CE learning rate: 0.00037
training loss: tensor(0.4633)
loss: CE learning rate: 0.00037
training loss: tensor(0.3284)
Global Model Acc on global data: 0.3795 length of data: 10000
-------------Round number:  36  -------------
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.2618)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.2905)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.2791)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.0088)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.5461)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.3210)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.3786)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.5015)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.2793)
loss: CE learning rate: 0.0003520000000000001
training loss: tensor(0.2242)
Global Model Acc on global data: 0.4251 length of data: 10000
-------------Round number:  37  -------------
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.3878)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2729)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2644)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.3542)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.3669)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.0738)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.3009)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.2889)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.1130)
loss: CE learning rate: 0.0003340000000000001
training loss: tensor(0.3467)
Global Model Acc on global data: 0.4875 length of data: 10000
save a model
-------------Round number:  38  -------------
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.1611)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.4156)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.2213)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.3777)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.5657)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.5916)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.3138)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.3102)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.4619)
loss: CE learning rate: 0.0003160000000000001
training loss: tensor(0.4868)
Global Model Acc on global data: 0.4943 length of data: 10000
save a model
-------------Round number:  39  -------------
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.3642)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.3020)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.3017)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.1876)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2539)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2361)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.1335)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2924)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.2196)
loss: CE learning rate: 0.00029800000000000003
training loss: tensor(0.4334)
Global Model Acc on global data: 0.4955 length of data: 10000
save a model
-------------Round number:  40  -------------
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.4221)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.3424)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.3796)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.2888)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.5927)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.7263)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.0962)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.2317)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.3244)
loss: CE learning rate: 0.00028000000000000003
training loss: tensor(0.1166)
Global Model Acc on global data: 0.4908 length of data: 10000
-------------Round number:  41  -------------
loss: CE learning rate: 0.000262
training loss: tensor(0.3174)
loss: CE learning rate: 0.000262
training loss: tensor(0.5079)
loss: CE learning rate: 0.000262
training loss: tensor(0.1791)
loss: CE learning rate: 0.000262
training loss: tensor(0.1719)
loss: CE learning rate: 0.000262
training loss: tensor(0.5450)
loss: CE learning rate: 0.000262
training loss: tensor(0.6546)
loss: CE learning rate: 0.000262
training loss: tensor(0.3376)
loss: CE learning rate: 0.000262
training loss: tensor(0.2125)
loss: CE learning rate: 0.000262
training loss: tensor(0.1473)
loss: CE learning rate: 0.000262
training loss: tensor(0.2506)
Global Model Acc on global data: 0.4788 length of data: 10000
-------------Round number:  42  -------------
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.2718)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.3377)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.1531)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.1673)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.5194)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.3669)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.3032)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.1895)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.1835)
loss: CE learning rate: 0.0002440000000000001
training loss: tensor(0.0998)
Global Model Acc on global data: 0.4902 length of data: 10000
-------------Round number:  43  -------------
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.3710)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2203)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.1577)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2468)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.1284)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.0226)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2581)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2641)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.7285)
loss: CE learning rate: 0.0002260000000000001
training loss: tensor(0.2945)
Global Model Acc on global data: 0.4954 length of data: 10000
-------------Round number:  44  -------------
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.2067)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(1.0056)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.0571)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.2142)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.2355)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.5908)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.3628)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.8629)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.2561)
loss: CE learning rate: 0.00020800000000000007
training loss: tensor(0.3552)
Global Model Acc on global data: 0.4687 length of data: 10000
-------------Round number:  45  -------------
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.1489)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.5155)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.1066)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.3003)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.1251)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.3840)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.3603)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.5357)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.1552)
loss: CE learning rate: 0.00019000000000000006
training loss: tensor(0.2387)
Global Model Acc on global data: 0.4958 length of data: 10000
save a model
-------------Round number:  46  -------------
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.5140)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.1373)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.1014)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.0776)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.4213)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.4634)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.1940)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.3972)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.3780)
loss: CE learning rate: 0.00017200000000000003
training loss: tensor(0.2341)
Global Model Acc on global data: 0.4809 length of data: 10000
-------------Round number:  47  -------------
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.2539)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.3647)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.4076)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.3560)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.3429)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.4195)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.4457)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.1458)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.2492)
loss: CE learning rate: 0.00015400000000000003
training loss: tensor(0.2397)
Global Model Acc on global data: 0.4849 length of data: 10000
-------------Round number:  48  -------------
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.1885)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2208)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.3753)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.4872)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.3958)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.3506)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.1394)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.3820)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2820)
loss: CE learning rate: 0.00013600000000000013
training loss: tensor(0.2382)
Global Model Acc on global data: 0.4838 length of data: 10000
-------------Round number:  49  -------------
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.3186)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.4041)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.5051)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.2339)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.6501)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.1402)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.2666)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.2738)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.1998)
loss: CE learning rate: 0.0001180000000000001
training loss: tensor(0.4755)
Global Model Acc on global data: 0.502 length of data: 10000
save a model
-------------Round number:  50  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1800)
loss: CE learning rate: 0.0001
training loss: tensor(0.2623)
loss: CE learning rate: 0.0001
training loss: tensor(0.3203)
loss: CE learning rate: 0.0001
training loss: tensor(0.1759)
loss: CE learning rate: 0.0001
training loss: tensor(0.2235)
loss: CE learning rate: 0.0001
training loss: tensor(0.1017)
loss: CE learning rate: 0.0001
training loss: tensor(0.2485)
loss: CE learning rate: 0.0001
training loss: tensor(0.1502)
loss: CE learning rate: 0.0001
training loss: tensor(0.1096)
loss: CE learning rate: 0.0001
training loss: tensor(0.3115)
Global Model Acc on global data: 0.4857 length of data: 10000
-------------Round number:  51  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3157)
loss: CE learning rate: 0.0001
training loss: tensor(0.4161)
loss: CE learning rate: 0.0001
training loss: tensor(0.8523)
loss: CE learning rate: 0.0001
training loss: tensor(0.2058)
loss: CE learning rate: 0.0001
training loss: tensor(0.2398)
loss: CE learning rate: 0.0001
training loss: tensor(0.5045)
loss: CE learning rate: 0.0001
training loss: tensor(0.1145)
loss: CE learning rate: 0.0001
training loss: tensor(0.4690)
loss: CE learning rate: 0.0001
training loss: tensor(0.2324)
loss: CE learning rate: 0.0001
training loss: tensor(0.1842)
Global Model Acc on global data: 0.4857 length of data: 10000
-------------Round number:  52  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3427)
loss: CE learning rate: 0.0001
training loss: tensor(0.3912)
loss: CE learning rate: 0.0001
training loss: tensor(0.1738)
loss: CE learning rate: 0.0001
training loss: tensor(0.3693)
loss: CE learning rate: 0.0001
training loss: tensor(0.3056)
loss: CE learning rate: 0.0001
training loss: tensor(0.2657)
loss: CE learning rate: 0.0001
training loss: tensor(0.0740)
loss: CE learning rate: 0.0001
training loss: tensor(0.0514)
loss: CE learning rate: 0.0001
training loss: tensor(0.1763)
loss: CE learning rate: 0.0001
training loss: tensor(0.3516)
Global Model Acc on global data: 0.4833 length of data: 10000
-------------Round number:  53  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3517)
loss: CE learning rate: 0.0001
training loss: tensor(0.1039)
loss: CE learning rate: 0.0001
training loss: tensor(0.7861)
loss: CE learning rate: 0.0001
training loss: tensor(0.1714)
loss: CE learning rate: 0.0001
training loss: tensor(0.1167)
loss: CE learning rate: 0.0001
training loss: tensor(0.7286)
loss: CE learning rate: 0.0001
training loss: tensor(0.3354)
loss: CE learning rate: 0.0001
training loss: tensor(0.2824)
loss: CE learning rate: 0.0001
training loss: tensor(0.4388)
loss: CE learning rate: 0.0001
training loss: tensor(0.6200)
Global Model Acc on global data: 0.4812 length of data: 10000
-------------Round number:  54  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.6350)
loss: CE learning rate: 0.0001
training loss: tensor(0.2979)
loss: CE learning rate: 0.0001
training loss: tensor(0.4646)
loss: CE learning rate: 0.0001
training loss: tensor(0.2756)
loss: CE learning rate: 0.0001
training loss: tensor(0.2427)
loss: CE learning rate: 0.0001
training loss: tensor(0.3189)
loss: CE learning rate: 0.0001
training loss: tensor(0.4899)
loss: CE learning rate: 0.0001
training loss: tensor(0.2743)
loss: CE learning rate: 0.0001
training loss: tensor(0.3938)
loss: CE learning rate: 0.0001
training loss: tensor(0.1927)
Global Model Acc on global data: 0.4712 length of data: 10000
-------------Round number:  55  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3569)
loss: CE learning rate: 0.0001
training loss: tensor(0.2901)
loss: CE learning rate: 0.0001
training loss: tensor(0.2003)
loss: CE learning rate: 0.0001
training loss: tensor(0.1068)
loss: CE learning rate: 0.0001
training loss: tensor(0.1605)
loss: CE learning rate: 0.0001
training loss: tensor(0.2986)
loss: CE learning rate: 0.0001
training loss: tensor(0.3369)
loss: CE learning rate: 0.0001
training loss: tensor(0.1262)
loss: CE learning rate: 0.0001
training loss: tensor(0.1217)
loss: CE learning rate: 0.0001
training loss: tensor(0.1289)
Global Model Acc on global data: 0.4725 length of data: 10000
-------------Round number:  56  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1832)
loss: CE learning rate: 0.0001
training loss: tensor(0.1674)
loss: CE learning rate: 0.0001
training loss: tensor(0.4389)
loss: CE learning rate: 0.0001
training loss: tensor(0.3799)
loss: CE learning rate: 0.0001
training loss: tensor(0.1598)
loss: CE learning rate: 0.0001
training loss: tensor(0.3263)
loss: CE learning rate: 0.0001
training loss: tensor(0.1530)
loss: CE learning rate: 0.0001
training loss: tensor(0.4333)
loss: CE learning rate: 0.0001
training loss: tensor(0.2328)
loss: CE learning rate: 0.0001
training loss: tensor(0.1392)
Global Model Acc on global data: 0.4802 length of data: 10000
-------------Round number:  57  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0991)
loss: CE learning rate: 0.0001
training loss: tensor(0.1073)
loss: CE learning rate: 0.0001
training loss: tensor(0.3497)
loss: CE learning rate: 0.0001
training loss: tensor(0.1842)
loss: CE learning rate: 0.0001
training loss: tensor(0.2183)
loss: CE learning rate: 0.0001
training loss: tensor(0.1412)
loss: CE learning rate: 0.0001
training loss: tensor(0.1595)
loss: CE learning rate: 0.0001
training loss: tensor(0.3870)
loss: CE learning rate: 0.0001
training loss: tensor(0.1221)
loss: CE learning rate: 0.0001
training loss: tensor(0.2833)
Global Model Acc on global data: 0.4904 length of data: 10000
-------------Round number:  58  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3486)
loss: CE learning rate: 0.0001
training loss: tensor(0.2288)
loss: CE learning rate: 0.0001
training loss: tensor(0.1198)
loss: CE learning rate: 0.0001
training loss: tensor(0.1638)
loss: CE learning rate: 0.0001
training loss: tensor(0.2258)
loss: CE learning rate: 0.0001
training loss: tensor(0.1842)
loss: CE learning rate: 0.0001
training loss: tensor(0.1593)
loss: CE learning rate: 0.0001
training loss: tensor(0.0779)
loss: CE learning rate: 0.0001
training loss: tensor(0.1325)
loss: CE learning rate: 0.0001
training loss: tensor(0.5324)
Global Model Acc on global data: 0.4627 length of data: 10000
-------------Round number:  59  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2586)
loss: CE learning rate: 0.0001
training loss: tensor(0.2191)
loss: CE learning rate: 0.0001
training loss: tensor(0.1106)
loss: CE learning rate: 0.0001
training loss: tensor(0.4114)
loss: CE learning rate: 0.0001
training loss: tensor(0.4021)
loss: CE learning rate: 0.0001
training loss: tensor(0.2916)
loss: CE learning rate: 0.0001
training loss: tensor(0.5349)
loss: CE learning rate: 0.0001
training loss: tensor(0.5411)
loss: CE learning rate: 0.0001
training loss: tensor(0.2275)
loss: CE learning rate: 0.0001
training loss: tensor(0.3512)
Global Model Acc on global data: 0.4679 length of data: 10000
-------------Round number:  60  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.6612)
loss: CE learning rate: 0.0001
training loss: tensor(0.5482)
loss: CE learning rate: 0.0001
training loss: tensor(0.1716)
loss: CE learning rate: 0.0001
training loss: tensor(0.0348)
loss: CE learning rate: 0.0001
training loss: tensor(0.1965)
loss: CE learning rate: 0.0001
training loss: tensor(0.1366)
loss: CE learning rate: 0.0001
training loss: tensor(0.2547)
loss: CE learning rate: 0.0001
training loss: tensor(0.4037)
loss: CE learning rate: 0.0001
training loss: tensor(0.2566)
loss: CE learning rate: 0.0001
training loss: tensor(0.4271)
Global Model Acc on global data: 0.4734 length of data: 10000
-------------Round number:  61  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1237)
loss: CE learning rate: 0.0001
training loss: tensor(0.2901)
loss: CE learning rate: 0.0001
training loss: tensor(0.0586)
loss: CE learning rate: 0.0001
training loss: tensor(0.3466)
loss: CE learning rate: 0.0001
training loss: tensor(0.1251)
loss: CE learning rate: 0.0001
training loss: tensor(0.3343)
loss: CE learning rate: 0.0001
training loss: tensor(0.5714)
loss: CE learning rate: 0.0001
training loss: tensor(0.1032)
loss: CE learning rate: 0.0001
training loss: tensor(0.2137)
loss: CE learning rate: 0.0001
training loss: tensor(0.1824)
Global Model Acc on global data: 0.4706 length of data: 10000
-------------Round number:  62  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1420)
loss: CE learning rate: 0.0001
training loss: tensor(0.4037)
loss: CE learning rate: 0.0001
training loss: tensor(0.1021)
loss: CE learning rate: 0.0001
training loss: tensor(0.1556)
loss: CE learning rate: 0.0001
training loss: tensor(0.0223)
loss: CE learning rate: 0.0001
training loss: tensor(0.2337)
loss: CE learning rate: 0.0001
training loss: tensor(0.4910)
loss: CE learning rate: 0.0001
training loss: tensor(0.0616)
loss: CE learning rate: 0.0001
training loss: tensor(0.2818)
loss: CE learning rate: 0.0001
training loss: tensor(0.1055)
Global Model Acc on global data: 0.4927 length of data: 10000
-------------Round number:  63  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4336)
loss: CE learning rate: 0.0001
training loss: tensor(0.1406)
loss: CE learning rate: 0.0001
training loss: tensor(0.2106)
loss: CE learning rate: 0.0001
training loss: tensor(0.3039)
loss: CE learning rate: 0.0001
training loss: tensor(0.0738)
loss: CE learning rate: 0.0001
training loss: tensor(0.3408)
loss: CE learning rate: 0.0001
training loss: tensor(0.1592)
loss: CE learning rate: 0.0001
training loss: tensor(0.2325)
loss: CE learning rate: 0.0001
training loss: tensor(0.4832)
loss: CE learning rate: 0.0001
training loss: tensor(0.4539)
Global Model Acc on global data: 0.494 length of data: 10000
-------------Round number:  64  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0418)
loss: CE learning rate: 0.0001
training loss: tensor(0.1800)
loss: CE learning rate: 0.0001
training loss: tensor(0.2407)
loss: CE learning rate: 0.0001
training loss: tensor(0.2634)
loss: CE learning rate: 0.0001
training loss: tensor(0.2685)
loss: CE learning rate: 0.0001
training loss: tensor(0.2870)
loss: CE learning rate: 0.0001
training loss: tensor(0.3233)
loss: CE learning rate: 0.0001
training loss: tensor(0.1012)
loss: CE learning rate: 0.0001
training loss: tensor(0.1337)
loss: CE learning rate: 0.0001
training loss: tensor(0.1793)
Global Model Acc on global data: 0.4812 length of data: 10000
-------------Round number:  65  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0492)
loss: CE learning rate: 0.0001
training loss: tensor(0.2978)
loss: CE learning rate: 0.0001
training loss: tensor(0.1826)
loss: CE learning rate: 0.0001
training loss: tensor(0.3932)
loss: CE learning rate: 0.0001
training loss: tensor(0.0918)
loss: CE learning rate: 0.0001
training loss: tensor(0.2622)
loss: CE learning rate: 0.0001
training loss: tensor(0.0643)
loss: CE learning rate: 0.0001
training loss: tensor(0.1751)
loss: CE learning rate: 0.0001
training loss: tensor(0.2918)
loss: CE learning rate: 0.0001
training loss: tensor(0.0705)
Global Model Acc on global data: 0.4819 length of data: 10000
-------------Round number:  66  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1944)
loss: CE learning rate: 0.0001
training loss: tensor(0.1828)
loss: CE learning rate: 0.0001
training loss: tensor(0.3322)
loss: CE learning rate: 0.0001
training loss: tensor(0.2367)
loss: CE learning rate: 0.0001
training loss: tensor(0.0747)
loss: CE learning rate: 0.0001
training loss: tensor(0.2897)
loss: CE learning rate: 0.0001
training loss: tensor(0.2486)
loss: CE learning rate: 0.0001
training loss: tensor(0.2917)
loss: CE learning rate: 0.0001
training loss: tensor(0.4253)
loss: CE learning rate: 0.0001
training loss: tensor(0.3554)
Global Model Acc on global data: 0.4889 length of data: 10000
-------------Round number:  67  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0221)
loss: CE learning rate: 0.0001
training loss: tensor(0.0965)
loss: CE learning rate: 0.0001
training loss: tensor(0.3140)
loss: CE learning rate: 0.0001
training loss: tensor(0.0490)
loss: CE learning rate: 0.0001
training loss: tensor(0.4052)
loss: CE learning rate: 0.0001
training loss: tensor(0.0850)
loss: CE learning rate: 0.0001
training loss: tensor(0.1209)
loss: CE learning rate: 0.0001
training loss: tensor(0.2013)
loss: CE learning rate: 0.0001
training loss: tensor(0.2875)
loss: CE learning rate: 0.0001
training loss: tensor(0.8465)
Global Model Acc on global data: 0.4753 length of data: 10000
-------------Round number:  68  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0697)
loss: CE learning rate: 0.0001
training loss: tensor(0.3211)
loss: CE learning rate: 0.0001
training loss: tensor(0.5020)
loss: CE learning rate: 0.0001
training loss: tensor(0.1790)
loss: CE learning rate: 0.0001
training loss: tensor(0.4142)
loss: CE learning rate: 0.0001
training loss: tensor(0.2262)
loss: CE learning rate: 0.0001
training loss: tensor(0.0937)
loss: CE learning rate: 0.0001
training loss: tensor(0.0910)
loss: CE learning rate: 0.0001
training loss: tensor(0.4551)
loss: CE learning rate: 0.0001
training loss: tensor(0.1047)
Global Model Acc on global data: 0.4686 length of data: 10000
-------------Round number:  69  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3612)
loss: CE learning rate: 0.0001
training loss: tensor(0.0662)
loss: CE learning rate: 0.0001
training loss: tensor(0.4068)
loss: CE learning rate: 0.0001
training loss: tensor(0.0681)
loss: CE learning rate: 0.0001
training loss: tensor(0.0875)
loss: CE learning rate: 0.0001
training loss: tensor(0.0966)
loss: CE learning rate: 0.0001
training loss: tensor(0.7380)
loss: CE learning rate: 0.0001
training loss: tensor(0.0341)
loss: CE learning rate: 0.0001
training loss: tensor(0.2292)
loss: CE learning rate: 0.0001
training loss: tensor(0.3224)
Global Model Acc on global data: 0.4697 length of data: 10000
-------------Round number:  70  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2509)
loss: CE learning rate: 0.0001
training loss: tensor(0.2706)
loss: CE learning rate: 0.0001
training loss: tensor(0.4957)
loss: CE learning rate: 0.0001
training loss: tensor(0.7091)
loss: CE learning rate: 0.0001
training loss: tensor(0.0927)
loss: CE learning rate: 0.0001
training loss: tensor(0.1140)
loss: CE learning rate: 0.0001
training loss: tensor(0.1552)
loss: CE learning rate: 0.0001
training loss: tensor(0.2035)
loss: CE learning rate: 0.0001
training loss: tensor(0.1665)
loss: CE learning rate: 0.0001
training loss: tensor(0.2976)
Global Model Acc on global data: 0.4635 length of data: 10000
-------------Round number:  71  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3818)
loss: CE learning rate: 0.0001
training loss: tensor(0.1659)
loss: CE learning rate: 0.0001
training loss: tensor(0.1494)
loss: CE learning rate: 0.0001
training loss: tensor(0.0952)
loss: CE learning rate: 0.0001
training loss: tensor(0.3548)
loss: CE learning rate: 0.0001
training loss: tensor(0.4897)
loss: CE learning rate: 0.0001
training loss: tensor(0.3541)
loss: CE learning rate: 0.0001
training loss: tensor(0.4001)
loss: CE learning rate: 0.0001
training loss: tensor(0.0565)
loss: CE learning rate: 0.0001
training loss: tensor(0.0934)
Global Model Acc on global data: 0.4714 length of data: 10000
-------------Round number:  72  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0186)
loss: CE learning rate: 0.0001
training loss: tensor(0.3197)
loss: CE learning rate: 0.0001
training loss: tensor(0.2271)
loss: CE learning rate: 0.0001
training loss: tensor(0.3520)
loss: CE learning rate: 0.0001
training loss: tensor(0.0645)
loss: CE learning rate: 0.0001
training loss: tensor(0.0415)
loss: CE learning rate: 0.0001
training loss: tensor(0.1197)
loss: CE learning rate: 0.0001
training loss: tensor(0.0243)
loss: CE learning rate: 0.0001
training loss: tensor(0.2891)
loss: CE learning rate: 0.0001
training loss: tensor(0.1526)
Global Model Acc on global data: 0.4549 length of data: 10000
-------------Round number:  73  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2678)
loss: CE learning rate: 0.0001
training loss: tensor(0.0967)
loss: CE learning rate: 0.0001
training loss: tensor(0.2483)
loss: CE learning rate: 0.0001
training loss: tensor(0.2466)
loss: CE learning rate: 0.0001
training loss: tensor(0.1919)
loss: CE learning rate: 0.0001
training loss: tensor(0.5187)
loss: CE learning rate: 0.0001
training loss: tensor(0.5342)
loss: CE learning rate: 0.0001
training loss: tensor(0.0121)
loss: CE learning rate: 0.0001
training loss: tensor(0.0322)
loss: CE learning rate: 0.0001
training loss: tensor(0.3245)
Global Model Acc on global data: 0.4806 length of data: 10000
-------------Round number:  74  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2067)
loss: CE learning rate: 0.0001
training loss: tensor(0.1526)
loss: CE learning rate: 0.0001
training loss: tensor(0.1732)
loss: CE learning rate: 0.0001
training loss: tensor(0.0538)
loss: CE learning rate: 0.0001
training loss: tensor(0.1891)
loss: CE learning rate: 0.0001
training loss: tensor(0.3417)
loss: CE learning rate: 0.0001
training loss: tensor(0.1587)
loss: CE learning rate: 0.0001
training loss: tensor(0.0790)
loss: CE learning rate: 0.0001
training loss: tensor(0.1979)
loss: CE learning rate: 0.0001
training loss: tensor(0.0799)
Global Model Acc on global data: 0.4897 length of data: 10000
-------------Round number:  75  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4566)
loss: CE learning rate: 0.0001
training loss: tensor(0.1191)
loss: CE learning rate: 0.0001
training loss: tensor(0.0950)
loss: CE learning rate: 0.0001
training loss: tensor(0.1645)
loss: CE learning rate: 0.0001
training loss: tensor(0.2060)
loss: CE learning rate: 0.0001
training loss: tensor(0.7789)
loss: CE learning rate: 0.0001
training loss: tensor(0.1181)
loss: CE learning rate: 0.0001
training loss: tensor(0.0687)
loss: CE learning rate: 0.0001
training loss: tensor(0.2330)
loss: CE learning rate: 0.0001
training loss: tensor(0.4509)
Global Model Acc on global data: 0.4969 length of data: 10000
-------------Round number:  76  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4210)
loss: CE learning rate: 0.0001
training loss: tensor(0.0694)
loss: CE learning rate: 0.0001
training loss: tensor(0.8061)
loss: CE learning rate: 0.0001
training loss: tensor(0.1673)
loss: CE learning rate: 0.0001
training loss: tensor(0.0720)
loss: CE learning rate: 0.0001
training loss: tensor(0.1075)
loss: CE learning rate: 0.0001
training loss: tensor(0.2407)
loss: CE learning rate: 0.0001
training loss: tensor(0.3613)
loss: CE learning rate: 0.0001
training loss: tensor(0.1406)
loss: CE learning rate: 0.0001
training loss: tensor(0.2763)
Global Model Acc on global data: 0.4893 length of data: 10000
-------------Round number:  77  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2433)
loss: CE learning rate: 0.0001
training loss: tensor(0.1464)
loss: CE learning rate: 0.0001
training loss: tensor(0.3188)
loss: CE learning rate: 0.0001
training loss: tensor(0.6337)
loss: CE learning rate: 0.0001
training loss: tensor(0.3601)
loss: CE learning rate: 0.0001
training loss: tensor(0.0731)
loss: CE learning rate: 0.0001
training loss: tensor(0.0877)
loss: CE learning rate: 0.0001
training loss: tensor(0.0255)
loss: CE learning rate: 0.0001
training loss: tensor(0.0770)
loss: CE learning rate: 0.0001
training loss: tensor(0.1995)
Global Model Acc on global data: 0.4742 length of data: 10000
-------------Round number:  78  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2247)
loss: CE learning rate: 0.0001
training loss: tensor(0.2354)
loss: CE learning rate: 0.0001
training loss: tensor(0.0916)
loss: CE learning rate: 0.0001
training loss: tensor(0.1328)
loss: CE learning rate: 0.0001
training loss: tensor(0.2231)
loss: CE learning rate: 0.0001
training loss: tensor(0.3236)
loss: CE learning rate: 0.0001
training loss: tensor(0.3151)
loss: CE learning rate: 0.0001
training loss: tensor(0.0759)
loss: CE learning rate: 0.0001
training loss: tensor(0.0807)
loss: CE learning rate: 0.0001
training loss: tensor(0.4001)
Global Model Acc on global data: 0.477 length of data: 10000
-------------Round number:  79  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2665)
loss: CE learning rate: 0.0001
training loss: tensor(0.1959)
loss: CE learning rate: 0.0001
training loss: tensor(0.1381)
loss: CE learning rate: 0.0001
training loss: tensor(0.3072)
loss: CE learning rate: 0.0001
training loss: tensor(0.1405)
loss: CE learning rate: 0.0001
training loss: tensor(0.3799)
loss: CE learning rate: 0.0001
training loss: tensor(0.2966)
loss: CE learning rate: 0.0001
training loss: tensor(0.0905)
loss: CE learning rate: 0.0001
training loss: tensor(0.1012)
loss: CE learning rate: 0.0001
training loss: tensor(0.2076)
Global Model Acc on global data: 0.4863 length of data: 10000
-------------Round number:  80  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0683)
loss: CE learning rate: 0.0001
training loss: tensor(0.7336)
loss: CE learning rate: 0.0001
training loss: tensor(0.0449)
loss: CE learning rate: 0.0001
training loss: tensor(0.1462)
loss: CE learning rate: 0.0001
training loss: tensor(0.4501)
loss: CE learning rate: 0.0001
training loss: tensor(0.0827)
loss: CE learning rate: 0.0001
training loss: tensor(0.1691)
loss: CE learning rate: 0.0001
training loss: tensor(0.1596)
loss: CE learning rate: 0.0001
training loss: tensor(0.0841)
loss: CE learning rate: 0.0001
training loss: tensor(0.2322)
Global Model Acc on global data: 0.4844 length of data: 10000
-------------Round number:  81  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0799)
loss: CE learning rate: 0.0001
training loss: tensor(0.0365)
loss: CE learning rate: 0.0001
training loss: tensor(0.2430)
loss: CE learning rate: 0.0001
training loss: tensor(0.0921)
loss: CE learning rate: 0.0001
training loss: tensor(0.3167)
loss: CE learning rate: 0.0001
training loss: tensor(0.2610)
loss: CE learning rate: 0.0001
training loss: tensor(0.0841)
loss: CE learning rate: 0.0001
training loss: tensor(0.0956)
loss: CE learning rate: 0.0001
training loss: tensor(0.1232)
loss: CE learning rate: 0.0001
training loss: tensor(0.1324)
Global Model Acc on global data: 0.4788 length of data: 10000
-------------Round number:  82  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3912)
loss: CE learning rate: 0.0001
training loss: tensor(0.0652)
loss: CE learning rate: 0.0001
training loss: tensor(0.0415)
loss: CE learning rate: 0.0001
training loss: tensor(0.3658)
loss: CE learning rate: 0.0001
training loss: tensor(0.1480)
loss: CE learning rate: 0.0001
training loss: tensor(0.2498)
loss: CE learning rate: 0.0001
training loss: tensor(1.2636)
loss: CE learning rate: 0.0001
training loss: tensor(0.2328)
loss: CE learning rate: 0.0001
training loss: tensor(0.2401)
loss: CE learning rate: 0.0001
training loss: tensor(0.0991)
Global Model Acc on global data: 0.4861 length of data: 10000
-------------Round number:  83  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0736)
loss: CE learning rate: 0.0001
training loss: tensor(0.5194)
loss: CE learning rate: 0.0001
training loss: tensor(0.0289)
loss: CE learning rate: 0.0001
training loss: tensor(0.1164)
loss: CE learning rate: 0.0001
training loss: tensor(0.3351)
loss: CE learning rate: 0.0001
training loss: tensor(0.2561)
loss: CE learning rate: 0.0001
training loss: tensor(0.0750)
loss: CE learning rate: 0.0001
training loss: tensor(0.0391)
loss: CE learning rate: 0.0001
training loss: tensor(0.3339)
loss: CE learning rate: 0.0001
training loss: tensor(0.3659)
Global Model Acc on global data: 0.4952 length of data: 10000
-------------Round number:  84  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0868)
loss: CE learning rate: 0.0001
training loss: tensor(0.0635)
loss: CE learning rate: 0.0001
training loss: tensor(0.0710)
loss: CE learning rate: 0.0001
training loss: tensor(0.2375)
loss: CE learning rate: 0.0001
training loss: tensor(0.1062)
loss: CE learning rate: 0.0001
training loss: tensor(0.7798)
loss: CE learning rate: 0.0001
training loss: tensor(0.2741)
loss: CE learning rate: 0.0001
training loss: tensor(0.1056)
loss: CE learning rate: 0.0001
training loss: tensor(0.0504)
loss: CE learning rate: 0.0001
training loss: tensor(0.1954)
Global Model Acc on global data: 0.4894 length of data: 10000
-------------Round number:  85  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1276)
loss: CE learning rate: 0.0001
training loss: tensor(0.3081)
loss: CE learning rate: 0.0001
training loss: tensor(0.0855)
loss: CE learning rate: 0.0001
training loss: tensor(0.3209)
loss: CE learning rate: 0.0001
training loss: tensor(0.2602)
loss: CE learning rate: 0.0001
training loss: tensor(0.2140)
loss: CE learning rate: 0.0001
training loss: tensor(0.0402)
loss: CE learning rate: 0.0001
training loss: tensor(0.0514)
loss: CE learning rate: 0.0001
training loss: tensor(0.0929)
loss: CE learning rate: 0.0001
training loss: tensor(0.8870)
Global Model Acc on global data: 0.482 length of data: 10000
-------------Round number:  86  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0555)
loss: CE learning rate: 0.0001
training loss: tensor(0.3459)
loss: CE learning rate: 0.0001
training loss: tensor(0.2690)
loss: CE learning rate: 0.0001
training loss: tensor(0.3455)
loss: CE learning rate: 0.0001
training loss: tensor(0.0963)
loss: CE learning rate: 0.0001
training loss: tensor(0.1786)
loss: CE learning rate: 0.0001
training loss: tensor(0.3771)
loss: CE learning rate: 0.0001
training loss: tensor(0.3588)
loss: CE learning rate: 0.0001
training loss: tensor(0.0676)
loss: CE learning rate: 0.0001
training loss: tensor(0.1206)
Global Model Acc on global data: 0.4837 length of data: 10000
-------------Round number:  87  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3216)
loss: CE learning rate: 0.0001
training loss: tensor(0.0846)
loss: CE learning rate: 0.0001
training loss: tensor(0.1981)
loss: CE learning rate: 0.0001
training loss: tensor(0.2086)
loss: CE learning rate: 0.0001
training loss: tensor(0.1771)
loss: CE learning rate: 0.0001
training loss: tensor(0.1801)
loss: CE learning rate: 0.0001
training loss: tensor(0.9046)
loss: CE learning rate: 0.0001
training loss: tensor(0.1327)
loss: CE learning rate: 0.0001
training loss: tensor(0.6531)
loss: CE learning rate: 0.0001
training loss: tensor(0.0997)
Global Model Acc on global data: 0.4791 length of data: 10000
-------------Round number:  88  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0273)
loss: CE learning rate: 0.0001
training loss: tensor(0.0656)
loss: CE learning rate: 0.0001
training loss: tensor(0.2011)
loss: CE learning rate: 0.0001
training loss: tensor(0.0378)
loss: CE learning rate: 0.0001
training loss: tensor(0.1947)
loss: CE learning rate: 0.0001
training loss: tensor(0.1838)
loss: CE learning rate: 0.0001
training loss: tensor(0.0952)
loss: CE learning rate: 0.0001
training loss: tensor(0.0659)
loss: CE learning rate: 0.0001
training loss: tensor(0.7212)
loss: CE learning rate: 0.0001
training loss: tensor(0.2832)
Global Model Acc on global data: 0.4876 length of data: 10000
-------------Round number:  89  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3247)
loss: CE learning rate: 0.0001
training loss: tensor(0.2663)
loss: CE learning rate: 0.0001
training loss: tensor(0.2788)
loss: CE learning rate: 0.0001
training loss: tensor(0.0437)
loss: CE learning rate: 0.0001
training loss: tensor(0.1717)
loss: CE learning rate: 0.0001
training loss: tensor(0.2373)
loss: CE learning rate: 0.0001
training loss: tensor(0.2745)
loss: CE learning rate: 0.0001
training loss: tensor(0.2494)
loss: CE learning rate: 0.0001
training loss: tensor(0.3469)
loss: CE learning rate: 0.0001
training loss: tensor(0.1184)
Global Model Acc on global data: 0.4877 length of data: 10000
-------------Round number:  90  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1777)
loss: CE learning rate: 0.0001
training loss: tensor(0.1401)
loss: CE learning rate: 0.0001
training loss: tensor(0.7078)
loss: CE learning rate: 0.0001
training loss: tensor(0.0277)
loss: CE learning rate: 0.0001
training loss: tensor(0.4231)
loss: CE learning rate: 0.0001
training loss: tensor(0.0778)
loss: CE learning rate: 0.0001
training loss: tensor(0.0627)
loss: CE learning rate: 0.0001
training loss: tensor(0.2236)
loss: CE learning rate: 0.0001
training loss: tensor(0.3357)
loss: CE learning rate: 0.0001
training loss: tensor(0.1475)
Global Model Acc on global data: 0.4751 length of data: 10000
-------------Round number:  91  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1341)
loss: CE learning rate: 0.0001
training loss: tensor(0.1264)
loss: CE learning rate: 0.0001
training loss: tensor(0.0955)
loss: CE learning rate: 0.0001
training loss: tensor(0.1181)
loss: CE learning rate: 0.0001
training loss: tensor(0.0826)
loss: CE learning rate: 0.0001
training loss: tensor(0.7483)
loss: CE learning rate: 0.0001
training loss: tensor(0.1203)
loss: CE learning rate: 0.0001
training loss: tensor(0.9478)
loss: CE learning rate: 0.0001
training loss: tensor(0.0962)
loss: CE learning rate: 0.0001
training loss: tensor(0.3303)
Global Model Acc on global data: 0.4905 length of data: 10000
-------------Round number:  92  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0842)
loss: CE learning rate: 0.0001
training loss: tensor(0.5870)
loss: CE learning rate: 0.0001
training loss: tensor(0.2400)
loss: CE learning rate: 0.0001
training loss: tensor(0.2308)
loss: CE learning rate: 0.0001
training loss: tensor(0.4444)
loss: CE learning rate: 0.0001
training loss: tensor(0.3087)
loss: CE learning rate: 0.0001
training loss: tensor(0.1769)
loss: CE learning rate: 0.0001
training loss: tensor(0.1309)
loss: CE learning rate: 0.0001
training loss: tensor(0.6643)
loss: CE learning rate: 0.0001
training loss: tensor(0.0268)
Global Model Acc on global data: 0.4918 length of data: 10000
-------------Round number:  93  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.1374)
loss: CE learning rate: 0.0001
training loss: tensor(0.1660)
loss: CE learning rate: 0.0001
training loss: tensor(0.1035)
loss: CE learning rate: 0.0001
training loss: tensor(0.0726)
loss: CE learning rate: 0.0001
training loss: tensor(0.1066)
loss: CE learning rate: 0.0001
training loss: tensor(0.1224)
loss: CE learning rate: 0.0001
training loss: tensor(0.1981)
loss: CE learning rate: 0.0001
training loss: tensor(0.1790)
loss: CE learning rate: 0.0001
training loss: tensor(0.2849)
loss: CE learning rate: 0.0001
training loss: tensor(0.1650)
Global Model Acc on global data: 0.4734 length of data: 10000
-------------Round number:  94  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.4235)
loss: CE learning rate: 0.0001
training loss: tensor(0.1741)
loss: CE learning rate: 0.0001
training loss: tensor(0.1238)
loss: CE learning rate: 0.0001
training loss: tensor(0.1740)
loss: CE learning rate: 0.0001
training loss: tensor(0.3302)
loss: CE learning rate: 0.0001
training loss: tensor(0.3721)
loss: CE learning rate: 0.0001
training loss: tensor(0.0901)
loss: CE learning rate: 0.0001
training loss: tensor(0.2957)
loss: CE learning rate: 0.0001
training loss: tensor(0.0462)
loss: CE learning rate: 0.0001
training loss: tensor(0.0413)
Global Model Acc on global data: 0.479 length of data: 10000
-------------Round number:  95  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.3832)
loss: CE learning rate: 0.0001
training loss: tensor(0.2966)
loss: CE learning rate: 0.0001
training loss: tensor(0.1117)
loss: CE learning rate: 0.0001
training loss: tensor(0.1109)
loss: CE learning rate: 0.0001
training loss: tensor(0.0311)
loss: CE learning rate: 0.0001
training loss: tensor(0.5992)
loss: CE learning rate: 0.0001
training loss: tensor(0.0687)
loss: CE learning rate: 0.0001
training loss: tensor(0.2552)
loss: CE learning rate: 0.0001
training loss: tensor(0.2820)
loss: CE learning rate: 0.0001
training loss: tensor(0.1882)
Global Model Acc on global data: 0.484 length of data: 10000
-------------Round number:  96  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.2381)
loss: CE learning rate: 0.0001
training loss: tensor(0.2518)
loss: CE learning rate: 0.0001
training loss: tensor(0.0526)
loss: CE learning rate: 0.0001
training loss: tensor(0.1294)
loss: CE learning rate: 0.0001
training loss: tensor(0.3091)
loss: CE learning rate: 0.0001
training loss: tensor(0.0419)
loss: CE learning rate: 0.0001
training loss: tensor(0.1527)
loss: CE learning rate: 0.0001
training loss: tensor(0.0659)
loss: CE learning rate: 0.0001
training loss: tensor(0.0431)
loss: CE learning rate: 0.0001
training loss: tensor(0.2189)
Global Model Acc on global data: 0.4924 length of data: 10000
-------------Round number:  97  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0456)
loss: CE learning rate: 0.0001
training loss: tensor(0.1391)
loss: CE learning rate: 0.0001
training loss: tensor(0.1893)
loss: CE learning rate: 0.0001
training loss: tensor(0.1802)
loss: CE learning rate: 0.0001
training loss: tensor(0.1639)
loss: CE learning rate: 0.0001
training loss: tensor(0.1607)
loss: CE learning rate: 0.0001
training loss: tensor(0.1730)
loss: CE learning rate: 0.0001
training loss: tensor(0.0955)
loss: CE learning rate: 0.0001
training loss: tensor(0.2150)
loss: CE learning rate: 0.0001
training loss: tensor(0.0466)
Global Model Acc on global data: 0.4819 length of data: 10000
-------------Round number:  98  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.8398)
loss: CE learning rate: 0.0001
training loss: tensor(0.2895)
loss: CE learning rate: 0.0001
training loss: tensor(0.1548)
loss: CE learning rate: 0.0001
training loss: tensor(0.0586)
loss: CE learning rate: 0.0001
training loss: tensor(0.2384)
loss: CE learning rate: 0.0001
training loss: tensor(0.3473)
loss: CE learning rate: 0.0001
training loss: tensor(0.5737)
loss: CE learning rate: 0.0001
training loss: tensor(0.1696)
loss: CE learning rate: 0.0001
training loss: tensor(0.0596)
loss: CE learning rate: 0.0001
training loss: tensor(0.1234)
Global Model Acc on global data: 0.5036 length of data: 10000
save a model
-------------Round number:  99  -------------
loss: CE learning rate: 0.0001
training loss: tensor(0.0108)
loss: CE learning rate: 0.0001
training loss: tensor(0.2143)
loss: CE learning rate: 0.0001
training loss: tensor(0.0690)
loss: CE learning rate: 0.0001
training loss: tensor(0.0717)
loss: CE learning rate: 0.0001
training loss: tensor(0.3508)
loss: CE learning rate: 0.0001
training loss: tensor(0.1078)
loss: CE learning rate: 0.0001
training loss: tensor(0.0501)
loss: CE learning rate: 0.0001
training loss: tensor(0.4488)
loss: CE learning rate: 0.0001
training loss: tensor(0.0383)
loss: CE learning rate: 0.0001
training loss: tensor(0.0640)
Global Model Acc on global data: 0.4826 length of data: 10000
================================================================================
Summary of training process:
Dataset                : Cifar10
Batch size             : 64
Learing rate           : 0.001
Number of total clients: 100
Split method           : distribution
Split parameter        : 0.5
Clients per round      : 10
Number of global rounds: 100
Number of local rounds : 10
Feature from layer     : 11
Feature percentage     : 0.1
Local training loss    : NT_CE
Loss of beta           : 1.0
Algorithm              : FedNTD
Modelname              : MOBNET
Mode                   : training
Seed                   : 0
================================================================================
Files already downloaded and verified
Files already downloaded and verified
Here
Done
Class frequencies:
client,class0,class1,class2,class3,class4,class5,class6,class7,class8,class9,Amount
Client   0,0.007,0.034,0.041,0.104,0.007,0.108,0.00,0.698,0.00,0.00,729
Client   1,0.146,0.027,0.017,0.00,0.124,0.407,0.080,0.129,0.029,0.039,410
Client   2,0.037,0.163,0.019,0.023,0.326,0.028,0.023,0.302,0.023,0.056,215
Client   3,0.175,0.006,0.004,0.004,0.006,0.044,0.519,0.138,0.105,0.00,543
Client   4,0.002,0.079,0.002,0.002,0.068,0.075,0.066,0.599,0.042,0.066,456
Client   5,0.330,0.028,0.075,0.511,0.057,0.00,0.00,0.00,0.00,0.00,509
Client   6,0.002,0.317,0.081,0.006,0.106,0.092,0.017,0.224,0.156,0.00,545
Client   7,0.059,0.061,0.107,0.311,0.046,0.162,0.007,0.099,0.149,0.00,544
Client   8,0.055,0.003,0.227,0.428,0.095,0.006,0.00,0.014,0.034,0.138,348
Client   9,0.063,0.058,0.150,0.225,0.124,0.043,0.075,0.017,0.184,0.061,347
Client  10,0.007,0.078,0.129,0.035,0.001,0.136,0.003,0.036,0.149,0.424,689
Client  11,0.051,0.004,0.004,0.022,0.125,0.022,0.060,0.00,0.575,0.136,449
Client  12,0.060,0.107,0.324,0.013,0.00,0.130,0.003,0.157,0.104,0.100,299
Client  13,0.00,0.003,0.152,0.200,0.031,0.00,0.005,0.120,0.00,0.489,585
Client  14,0.003,0.243,0.119,0.228,0.044,0.095,0.00,0.00,0.268,0.00,639
Client  15,0.030,0.234,0.017,0.019,0.177,0.00,0.298,0.226,0.00,0.00,531
Client  16,0.028,0.00,0.135,0.403,0.015,0.220,0.163,0.037,0.00,0.00,400
Client  17,0.099,0.005,0.056,0.025,0.064,0.285,0.016,0.451,0.00,0.00,628
Client  18,0.157,0.133,0.215,0.013,0.045,0.011,0.077,0.096,0.202,0.051,376
Client  19,0.385,0.093,0.056,0.096,0.00,0.119,0.093,0.152,0.00,0.007,270
Client  20,0.136,0.00,0.056,0.032,0.136,0.096,0.144,0.160,0.128,0.112,125
Client  21,0.013,0.051,0.115,0.194,0.013,0.120,0.071,0.231,0.192,0.00,468
Client  22,0.246,0.004,0.011,0.011,0.022,0.138,0.062,0.098,0.199,0.210,276
Client  23,0.019,0.087,0.049,0.107,0.132,0.371,0.060,0.175,0.00,0.00,531
Client  24,0.006,0.099,0.227,0.193,0.066,0.068,0.163,0.139,0.040,0.00,503
Client  25,0.073,0.034,0.015,0.017,0.228,0.002,0.007,0.545,0.041,0.039,413
Client  26,0.049,0.081,0.104,0.099,0.003,0.003,0.265,0.200,0.010,0.187,385
Client  27,0.024,0.409,0.001,0.00,0.566,0.00,0.00,0.00,0.00,0.00,1007
Client  28,0.025,0.013,0.134,0.293,0.108,0.006,0.089,0.00,0.070,0.261,314
Client  29,0.00,0.580,0.063,0.357,0.00,0.00,0.00,0.00,0.00,0.00,607
Client  30,0.011,0.006,0.00,0.057,0.001,0.00,0.016,0.093,0.815,0.00,795
Client  31,0.218,0.244,0.171,0.367,0.00,0.00,0.00,0.00,0.00,0.00,532
Client  32,0.001,0.00,0.310,0.00,0.069,0.057,0.048,0.007,0.016,0.492,706
Client  33,0.163,0.002,0.261,0.149,0.021,0.00,0.274,0.130,0.00,0.00,529
Client  34,0.030,0.003,0.090,0.276,0.047,0.017,0.196,0.047,0.083,0.213,301
Client  35,0.021,0.213,0.002,0.034,0.247,0.484,0.00,0.00,0.00,0.00,616
Client  36,0.020,0.190,0.018,0.108,0.029,0.119,0.143,0.192,0.033,0.148,453
Client  37,0.355,0.040,0.064,0.012,0.171,0.202,0.004,0.008,0.083,0.062,519
Client  38,0.00,0.023,0.438,0.355,0.011,0.109,0.004,0.019,0.042,0.00,265
Client  39,0.175,0.025,0.196,0.153,0.004,0.015,0.167,0.00,0.004,0.262,275
Client  40,0.482,0.324,0.033,0.004,0.158,0.00,0.00,0.00,0.00,0.00,546
Client  41,0.170,0.021,0.00,0.503,0.107,0.010,0.016,0.079,0.018,0.076,382
Client  42,0.004,0.002,0.503,0.143,0.111,0.238,0.00,0.00,0.00,0.00,505
Client  43,0.013,0.294,0.243,0.013,0.141,0.00,0.192,0.00,0.105,0.00,313
Client  44,0.158,0.047,0.011,0.158,0.007,0.007,0.452,0.079,0.029,0.054,279
Client  45,0.00,0.285,0.226,0.159,0.114,0.059,0.114,0.043,0.00,0.00,509
Client  46,0.525,0.191,0.284,0.00,0.00,0.00,0.00,0.00,0.00,0.00,598
Client  47,0.019,0.201,0.002,0.011,0.062,0.148,0.113,0.024,0.420,0.00,628
Client  48,0.028,0.109,0.019,0.057,0.057,0.066,0.209,0.090,0.289,0.076,211
Client  49,0.049,0.106,0.036,0.515,0.155,0.140,0.00,0.00,0.00,0.00,530
Client  50,0.029,0.044,0.323,0.223,0.006,0.019,0.356,0.00,0.00,0.00,685
Client  51,0.072,0.003,0.00,0.038,0.107,0.433,0.012,0.060,0.275,0.00,582
Client  52,0.001,0.011,0.050,0.030,0.026,0.00,0.178,0.008,0.154,0.543,741
Client  53,0.025,0.035,0.003,0.203,0.00,0.008,0.013,0.062,0.007,0.643,597
Client  54,0.027,0.263,0.015,0.042,0.066,0.355,0.00,0.216,0.004,0.012,259
Client  55,0.008,0.093,0.097,0.00,0.068,0.545,0.004,0.027,0.129,0.027,473
Client  56,0.077,0.009,0.009,0.014,0.009,0.066,0.229,0.318,0.267,0.00,647
Client  57,0.142,0.453,0.015,0.088,0.301,0.00,0.00,0.00,0.00,0.00,534
Client  58,0.161,0.074,0.107,0.004,0.009,0.035,0.090,0.057,0.464,0.00,690
Client  59,0.101,0.149,0.073,0.261,0.054,0.363,0.00,0.00,0.00,0.00,537
Client  60,0.066,0.002,0.191,0.026,0.033,0.193,0.060,0.002,0.153,0.274,580
Client  61,0.00,0.048,0.00,0.213,0.002,0.046,0.065,0.120,0.179,0.326,475
Client  62,0.030,0.001,0.001,0.006,0.074,0.008,0.009,0.258,0.613,0.00,979
Client  63,0.019,0.017,0.008,0.053,0.212,0.006,0.168,0.452,0.065,0.00,524
Client  64,0.005,0.010,0.007,0.010,0.421,0.300,0.00,0.058,0.002,0.188,416
Client  65,0.002,0.037,0.202,0.019,0.301,0.041,0.287,0.031,0.080,0.00,515
Client  66,0.009,0.00,0.251,0.136,0.002,0.319,0.049,0.049,0.178,0.007,427
Client  67,0.266,0.074,0.039,0.238,0.004,0.006,0.271,0.027,0.074,0.00,512
Client  68,0.532,0.037,0.005,0.053,0.005,0.037,0.163,0.142,0.016,0.011,190
Client  69,0.324,0.054,0.058,0.002,0.00,0.135,0.002,0.137,0.258,0.029,481
Client  70,0.048,0.006,0.075,0.025,0.847,0.00,0.00,0.00,0.00,0.00,881
Client  71,0.124,0.00,0.002,0.020,0.005,0.005,0.845,0.00,0.00,0.00,611
Client  72,0.023,0.105,0.041,0.395,0.013,0.020,0.005,0.212,0.00,0.186,392
Client  73,0.002,0.135,0.211,0.052,0.023,0.002,0.478,0.002,0.095,0.00,517
Client  74,0.024,0.128,0.104,0.002,0.177,0.234,0.051,0.113,0.077,0.091,453
Client  75,0.302,0.004,0.028,0.004,0.317,0.002,0.043,0.265,0.028,0.006,464
Client  76,0.015,0.097,0.005,0.077,0.005,0.116,0.192,0.035,0.018,0.441,662
Client  77,0.105,0.003,0.022,0.301,0.056,0.121,0.003,0.263,0.070,0.056,372
Client  78,0.278,0.045,0.106,0.036,0.005,0.025,0.323,0.009,0.097,0.077,443
Client  79,0.239,0.00,0.190,0.104,0.019,0.321,0.019,0.011,0.004,0.093,268
Client  80,0.002,0.011,0.257,0.097,0.037,0.037,0.054,0.166,0.006,0.333,463
Client  81,0.039,0.068,0.093,0.148,0.005,0.015,0.017,0.005,0.011,0.600,648
Client  82,0.253,0.387,0.048,0.089,0.007,0.00,0.010,0.051,0.144,0.010,292
Client  83,0.176,0.223,0.144,0.036,0.005,0.106,0.310,0.00,0.00,0.00,632
Client  84,0.035,0.009,0.200,0.016,0.167,0.019,0.00,0.100,0.012,0.442,430
Client  85,0.390,0.292,0.00,0.00,0.003,0.019,0.066,0.027,0.077,0.127,377
Client  86,0.182,0.364,0.016,0.022,0.057,0.149,0.00,0.030,0.041,0.139,368
Client  87,0.048,0.402,0.088,0.108,0.088,0.253,0.008,0.00,0.004,0.00,249
Client  88,0.120,0.032,0.162,0.056,0.00,0.241,0.00,0.074,0.134,0.181,216
Client  89,0.219,0.119,0.144,0.039,0.054,0.027,0.399,0.00,0.00,0.00,672
Client  90,0.417,0.008,0.00,0.035,0.009,0.00,0.121,0.020,0.390,0.00,782
Client  91,0.017,0.00,0.247,0.023,0.008,0.00,0.064,0.099,0.012,0.530,922
Client  92,0.367,0.004,0.074,0.020,0.026,0.00,0.028,0.032,0.072,0.377,501
Client  93,0.087,0.049,0.042,0.069,0.286,0.016,0.016,0.435,0.00,0.00,612
Client  94,0.015,0.092,0.007,0.033,0.261,0.592,0.00,0.00,0.00,0.00,671
Client  95,0.136,0.003,0.413,0.043,0.00,0.050,0.355,0.00,0.00,0.00,698
Client  96,0.046,0.034,0.114,0.009,0.462,0.052,0.009,0.00,0.135,0.138,325
Client  97,0.247,0.006,0.071,0.010,0.066,0.164,0.048,0.301,0.087,0.00,518
Client  98,0.011,0.019,0.162,0.151,0.207,0.323,0.127,0.00,0.00,0.00,569
Client  99,0.122,0.619,0.006,0.012,0.107,0.132,0.00,0.002,0.00,0.00,515
Num_samples of Training set per client: [729, 410, 215, 543, 456, 509, 545, 544, 348, 347, 689, 449, 299, 585, 639, 531, 400, 628, 376, 270, 125, 468, 276, 531, 503, 413, 385, 1007, 314, 607, 795, 532, 706, 529, 301, 616, 453, 519, 265, 275, 546, 382, 505, 313, 279, 509, 598, 628, 211, 530, 685, 582, 741, 597, 259, 473, 647, 534, 690, 537, 580, 475, 979, 524, 416, 515, 427, 512, 190, 481, 881, 611, 392, 517, 453, 464, 662, 372, 443, 268, 463, 648, 292, 632, 430, 377, 368, 249, 216, 672, 782, 922, 501, 612, 671, 698, 325, 518, 569, 515]
Total_training_samples: 50000
Global test set: 10000
Finish Generating Samples, distribution saved
MobileNetV2(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layers): Sequential(
    (0): Block(
      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Block(
      (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (6): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (7): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (8): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (9): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (10): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (12): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (13): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (14): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (15): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (16): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv2): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear): Linear(in_features=1280, out_features=10, bias=True)
)
number of parameters: 2296922
clients initializting...
output size: 10
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<01:15,  1.31it/s]  2%|▏         | 2/100 [00:01<01:18,  1.24it/s]  3%|▎         | 3/100 [00:02<01:22,  1.18it/s]  4%|▍         | 4/100 [00:03<01:22,  1.16it/s]  5%|▌         | 5/100 [00:04<01:22,  1.15it/s]  6%|▌         | 6/100 [00:05<01:17,  1.21it/s]  7%|▋         | 7/100 [00:06<01:19,  1.17it/s]  8%|▊         | 8/100 [00:06<01:19,  1.16it/s]  9%|▉         | 9/100 [00:07<01:18,  1.16it/s] 10%|█         | 10/100 [00:08<01:18,  1.15it/s] 11%|█         | 11/100 [00:09<01:18,  1.13it/s] 12%|█▏        | 12/100 [00:10<01:17,  1.14it/s] 13%|█▎        | 13/100 [00:11<01:15,  1.15it/s] 14%|█▍        | 14/100 [00:12<01:12,  1.18it/s] 15%|█▌        | 15/100 [00:12<01:10,  1.20it/s] 16%|█▌        | 16/100 [00:13<01:08,  1.22it/s] 17%|█▋        | 17/100 [00:14<01:07,  1.23it/s] 18%|█▊        | 18/100 [00:15<01:06,  1.23it/s] 19%|█▉        | 19/100 [00:16<01:09,  1.17it/s] 20%|██        | 20/100 [00:17<01:07,  1.19it/s] 21%|██        | 21/100 [00:17<01:06,  1.18it/s] 22%|██▏       | 22/100 [00:18<01:05,  1.18it/s] 23%|██▎       | 23/100 [00:19<01:05,  1.18it/s] 24%|██▍       | 24/100 [00:20<01:03,  1.20it/s] 25%|██▌       | 25/100 [00:21<01:02,  1.19it/s] 26%|██▌       | 26/100 [00:25<02:15,  1.83s/it] 27%|██▋       | 27/100 [00:26<01:52,  1.54s/it] 28%|██▊       | 28/100 [00:26<01:32,  1.28s/it] 29%|██▉       | 29/100 [00:27<01:21,  1.15s/it] 30%|███       | 30/100 [00:28<01:09,  1.01it/s] 31%|███       | 31/100 [00:29<01:03,  1.08it/s] 32%|███▏      | 32/100 [00:29<00:57,  1.18it/s] 33%|███▎      | 33/100 [00:30<00:56,  1.18it/s] 34%|███▍      | 34/100 [00:31<00:54,  1.21it/s] 35%|███▌      | 35/100 [00:32<00:55,  1.17it/s] 36%|███▌      | 36/100 [00:33<00:53,  1.19it/s] 37%|███▋      | 37/100 [00:34<00:56,  1.12it/s] 38%|███▊      | 38/100 [00:35<00:56,  1.10it/s] 39%|███▉      | 39/100 [00:36<00:55,  1.10it/s] 40%|████      | 40/100 [00:37<00:54,  1.09it/s] 41%|████      | 41/100 [00:37<00:51,  1.14it/s] 42%|████▏     | 42/100 [00:38<00:51,  1.12it/s] 43%|████▎     | 43/100 [00:39<00:49,  1.15it/s] 44%|████▍     | 44/100 [00:40<00:48,  1.17it/s] 45%|████▌     | 45/100 [00:41<00:49,  1.11it/s] 46%|████▌     | 46/100 [00:42<00:47,  1.13it/s] 47%|████▋     | 47/100 [00:42<00:43,  1.23it/s] 48%|████▊     | 48/100 [00:43<00:43,  1.19it/s] 49%|████▉     | 49/100 [00:44<00:44,  1.15it/s] 50%|█████     | 50/100 [00:45<00:41,  1.21it/s] 51%|█████     | 51/100 [00:46<00:40,  1.22it/s] 52%|█████▏    | 52/100 [00:47<00:39,  1.22it/s] 53%|█████▎    | 53/100 [00:47<00:39,  1.20it/s] 54%|█████▍    | 54/100 [00:52<01:24,  1.85s/it] 55%|█████▌    | 55/100 [00:53<01:10,  1.56s/it] 56%|█████▌    | 56/100 [00:53<00:59,  1.35s/it] 57%|█████▋    | 57/100 [00:54<00:51,  1.21s/it] 58%|█████▊    | 58/100 [00:55<00:44,  1.06s/it] 59%|█████▉    | 59/100 [00:56<00:41,  1.02s/it] 60%|██████    | 60/100 [00:57<00:37,  1.05it/s] 61%|██████    | 61/100 [00:58<00:36,  1.06it/s] 62%|██████▏   | 62/100 [00:58<00:34,  1.09it/s] 63%|██████▎   | 63/100 [00:59<00:34,  1.07it/s] 64%|██████▍   | 64/100 [01:00<00:33,  1.09it/s] 65%|██████▌   | 65/100 [01:01<00:31,  1.10it/s] 66%|██████▌   | 66/100 [01:02<00:30,  1.11it/s] 67%|██████▋   | 67/100 [01:03<00:29,  1.13it/s] 68%|██████▊   | 68/100 [01:04<00:28,  1.13it/s] 69%|██████▉   | 69/100 [01:05<00:27,  1.11it/s] 70%|███████   | 70/100 [01:06<00:26,  1.12it/s] 71%|███████   | 71/100 [01:06<00:24,  1.19it/s] 72%|███████▏  | 72/100 [01:07<00:22,  1.22it/s] 73%|███████▎  | 73/100 [01:08<00:22,  1.20it/s] 74%|███████▍  | 74/100 [01:09<00:22,  1.17it/s] 75%|███████▌  | 75/100 [01:10<00:22,  1.13it/s] 76%|███████▌  | 76/100 [01:11<00:21,  1.12it/s] 77%|███████▋  | 77/100 [01:12<00:21,  1.09it/s] 78%|███████▊  | 78/100 [01:13<00:20,  1.08it/s] 79%|███████▉  | 79/100 [01:14<00:19,  1.09it/s] 80%|████████  | 80/100 [01:18<00:39,  1.95s/it] 81%|████████  | 81/100 [01:19<00:31,  1.64s/it] 82%|████████▏ | 82/100 [01:20<00:25,  1.42s/it] 83%|████████▎ | 83/100 [01:21<00:21,  1.25s/it] 84%|████████▍ | 84/100 [01:21<00:17,  1.11s/it] 85%|████████▌ | 85/100 [01:22<00:15,  1.05s/it] 86%|████████▌ | 86/100 [01:23<00:13,  1.01it/s] 87%|████████▋ | 87/100 [01:24<00:12,  1.05it/s] 88%|████████▊ | 88/100 [01:25<00:10,  1.10it/s] 89%|████████▉ | 89/100 [01:26<00:09,  1.12it/s] 90%|█████████ | 90/100 [01:26<00:08,  1.16it/s] 91%|█████████ | 91/100 [01:27<00:07,  1.18it/s] 92%|█████████▏| 92/100 [01:28<00:06,  1.17it/s] 93%|█████████▎| 93/100 [01:29<00:06,  1.17it/s] 94%|█████████▍| 94/100 [01:30<00:05,  1.18it/s] 95%|█████████▌| 95/100 [01:31<00:04,  1.20it/s] 96%|█████████▌| 96/100 [01:31<00:03,  1.23it/s] 97%|█████████▋| 97/100 [01:32<00:02,  1.20it/s] 98%|█████████▊| 98/100 [01:33<00:01,  1.18it/s] 99%|█████████▉| 99/100 [01:34<00:00,  1.19it/s]100%|██████████| 100/100 [01:35<00:00,  1.21it/s]100%|██████████| 100/100 [01:35<00:00,  1.05it/s]
Number of users per round / total users: 10  /  100
Finished creating FL server.
=== Training starts: algorithm FedNTD ===
-------------Round number:  0  -------------
loss: NT_CE learning rate: 0.001
training loss: tensor(1.5647) KL loss: tensor(0.2245)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.3700) KL loss: tensor(0.2250)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.3377) KL loss: tensor(0.2261)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.1991) KL loss: tensor(0.2113)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.3410) KL loss: tensor(0.2321)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.3757) KL loss: tensor(0.2089)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.4783) KL loss: tensor(0.2547)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.3016) KL loss: tensor(0.2340)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.5145) KL loss: tensor(0.2676)
loss: NT_CE learning rate: 0.001
training loss: tensor(1.1422) KL loss: tensor(0.2247)
Global Model Acc on global data: 0.1 length of data: 10000
save a model
-------------Round number:  1  -------------
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.1516) KL loss: tensor(0.1988)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.0130) KL loss: tensor(0.1948)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.1017) KL loss: tensor(0.2196)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.0389) KL loss: tensor(0.1907)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.0155) KL loss: tensor(0.1893)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.0265) KL loss: tensor(0.1976)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.3222) KL loss: tensor(0.2502)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.0362) KL loss: tensor(0.2302)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.1719) KL loss: tensor(0.2451)
loss: NT_CE learning rate: 0.000982
training loss: tensor(1.2296) KL loss: tensor(0.2311)
Global Model Acc on global data: 0.1056 length of data: 10000
save a model
-------------Round number:  2  -------------
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.8751) KL loss: tensor(0.2092)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.4997) KL loss: tensor(0.1297)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.9612) KL loss: tensor(0.2312)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.9810) KL loss: tensor(0.2088)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.9098) KL loss: tensor(0.2194)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.9717) KL loss: tensor(0.2198)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.7707) KL loss: tensor(0.1992)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.3525) KL loss: tensor(0.1177)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.7872) KL loss: tensor(0.1902)
loss: NT_CE learning rate: 0.000964
training loss: tensor(0.7715) KL loss: tensor(0.1761)
Global Model Acc on global data: 0.1 length of data: 10000
-------------Round number:  3  -------------
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.6492) KL loss: tensor(0.1648)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.6769) KL loss: tensor(0.1921)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.7772) KL loss: tensor(0.1994)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.5618) KL loss: tensor(0.1485)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.7425) KL loss: tensor(0.1991)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.6688) KL loss: tensor(0.1924)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.8236) KL loss: tensor(0.1768)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.6325) KL loss: tensor(0.1548)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.6372) KL loss: tensor(0.1746)
loss: NT_CE learning rate: 0.000946
training loss: tensor(0.7462) KL loss: tensor(0.1879)
Global Model Acc on global data: 0.1089 length of data: 10000
save a model
-------------Round number:  4  -------------
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.7116) KL loss: tensor(0.1847)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.7806) KL loss: tensor(0.2392)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.7057) KL loss: tensor(0.2260)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.8228) KL loss: tensor(0.2258)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.6945) KL loss: tensor(0.2025)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.7770) KL loss: tensor(0.2117)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.6586) KL loss: tensor(0.2020)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.8879) KL loss: tensor(0.2656)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.6468) KL loss: tensor(0.1936)
loss: NT_CE learning rate: 0.0009280000000000001
training loss: tensor(0.7053) KL loss: tensor(0.2159)
Global Model Acc on global data: 0.1628 length of data: 10000
save a model
-------------Round number:  5  -------------
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.6863) KL loss: tensor(0.2198)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.7551) KL loss: tensor(0.2746)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.5849) KL loss: tensor(0.2135)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.6383) KL loss: tensor(0.2141)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.8868) KL loss: tensor(0.2855)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.7772) KL loss: tensor(0.2500)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.6007) KL loss: tensor(0.2267)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.5809) KL loss: tensor(0.1915)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.7219) KL loss: tensor(0.2280)
loss: NT_CE learning rate: 0.00091
training loss: tensor(0.5650) KL loss: tensor(0.2128)
Global Model Acc on global data: 0.176 length of data: 10000
save a model
-------------Round number:  6  -------------
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.5658) KL loss: tensor(0.2092)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.6647) KL loss: tensor(0.2248)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.6285) KL loss: tensor(0.2220)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.6456) KL loss: tensor(0.2149)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.7852) KL loss: tensor(0.2617)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.6564) KL loss: tensor(0.2236)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.5554) KL loss: tensor(0.2046)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.5311) KL loss: tensor(0.2056)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.7579) KL loss: tensor(0.2505)
loss: NT_CE learning rate: 0.000892
training loss: tensor(0.6302) KL loss: tensor(0.2248)
Global Model Acc on global data: 0.2381 length of data: 10000
save a model
-------------Round number:  7  -------------
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.7425) KL loss: tensor(0.2792)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.9651) KL loss: tensor(0.2546)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.6360) KL loss: tensor(0.2260)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.5747) KL loss: tensor(0.2361)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.6290) KL loss: tensor(0.2407)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.6003) KL loss: tensor(0.2271)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.5600) KL loss: tensor(0.2228)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.3794) KL loss: tensor(0.1465)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.6061) KL loss: tensor(0.2402)
loss: NT_CE learning rate: 0.000874
training loss: tensor(0.8052) KL loss: tensor(0.2803)
Global Model Acc on global data: 0.2635 length of data: 10000
save a model
-------------Round number:  8  -------------
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.5016) KL loss: tensor(0.1981)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.6883) KL loss: tensor(0.2578)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.6258) KL loss: tensor(0.2344)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.5456) KL loss: tensor(0.2009)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.5882) KL loss: tensor(0.2348)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.6182) KL loss: tensor(0.2274)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.5166) KL loss: tensor(0.2097)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.4375) KL loss: tensor(0.1865)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.4766) KL loss: tensor(0.2141)
loss: NT_CE learning rate: 0.000856
training loss: tensor(0.6550) KL loss: tensor(0.2389)
Global Model Acc on global data: 0.3089 length of data: 10000
save a model
-------------Round number:  9  -------------
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.5517) KL loss: tensor(0.2281)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.5711) KL loss: tensor(0.2134)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.4895) KL loss: tensor(0.2041)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.5630) KL loss: tensor(0.1867)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.5180) KL loss: tensor(0.2039)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.6866) KL loss: tensor(0.2749)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.5633) KL loss: tensor(0.2385)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.4745) KL loss: tensor(0.2115)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.3472) KL loss: tensor(0.1780)
loss: NT_CE learning rate: 0.0008380000000000001
training loss: tensor(0.5709) KL loss: tensor(0.2404)
Global Model Acc on global data: 0.2319 length of data: 10000
-------------Round number:  10  -------------
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.5824) KL loss: tensor(0.1804)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.5303) KL loss: tensor(0.1945)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.4483) KL loss: tensor(0.1591)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.6481) KL loss: tensor(0.2365)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.5197) KL loss: tensor(0.1658)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.3631) KL loss: tensor(0.1402)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.6282) KL loss: tensor(0.2127)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.5882) KL loss: tensor(0.1889)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.5684) KL loss: tensor(0.1719)
loss: NT_CE learning rate: 0.0008200000000000001
training loss: tensor(0.4440) KL loss: tensor(0.1261)
Global Model Acc on global data: 0.3392 length of data: 10000
save a model
-------------Round number:  11  -------------
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.3763) KL loss: tensor(0.1860)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.5838) KL loss: tensor(0.2415)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.5981) KL loss: tensor(0.2421)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.5268) KL loss: tensor(0.2177)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.4706) KL loss: tensor(0.2042)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.6791) KL loss: tensor(0.2443)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.3791) KL loss: tensor(0.1926)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.3903) KL loss: tensor(0.1753)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.3677) KL loss: tensor(0.1771)
loss: NT_CE learning rate: 0.0008020000000000001
training loss: tensor(0.6241) KL loss: tensor(0.2512)
Global Model Acc on global data: 0.3101 length of data: 10000
-------------Round number:  12  -------------
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.5100) KL loss: tensor(0.1980)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.4189) KL loss: tensor(0.1839)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.4997) KL loss: tensor(0.2021)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.3726) KL loss: tensor(0.1815)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.3595) KL loss: tensor(0.1369)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.4643) KL loss: tensor(0.1990)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.5130) KL loss: tensor(0.2037)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.5030) KL loss: tensor(0.2018)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.6136) KL loss: tensor(0.2298)
loss: NT_CE learning rate: 0.0007840000000000001
training loss: tensor(0.7969) KL loss: tensor(0.3442)
Global Model Acc on global data: 0.3604 length of data: 10000
save a model
-------------Round number:  13  -------------
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.5559) KL loss: tensor(0.2577)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.2588) KL loss: tensor(0.1594)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.4727) KL loss: tensor(0.2070)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.3026) KL loss: tensor(0.1691)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.4409) KL loss: tensor(0.1950)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.5999) KL loss: tensor(0.2462)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.5502) KL loss: tensor(0.2274)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.3705) KL loss: tensor(0.1831)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.5104) KL loss: tensor(0.2070)
loss: NT_CE learning rate: 0.0007660000000000001
training loss: tensor(0.3995) KL loss: tensor(0.1985)
Global Model Acc on global data: 0.327 length of data: 10000
-------------Round number:  14  -------------
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.5729) KL loss: tensor(0.2136)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.5715) KL loss: tensor(0.2221)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.4796) KL loss: tensor(0.1936)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.4998) KL loss: tensor(0.2041)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.5178) KL loss: tensor(0.2117)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.3022) KL loss: tensor(0.1656)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.4289) KL loss: tensor(0.1724)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.3343) KL loss: tensor(0.1541)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.4731) KL loss: tensor(0.2166)
loss: NT_CE learning rate: 0.000748
training loss: tensor(0.7393) KL loss: tensor(0.2918)
Global Model Acc on global data: 0.2999 length of data: 10000
-------------Round number:  15  -------------
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.4491) KL loss: tensor(0.1683)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.7980) KL loss: tensor(0.2885)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.2875) KL loss: tensor(0.1515)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.5802) KL loss: tensor(0.2497)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.2858) KL loss: tensor(0.1324)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.4442) KL loss: tensor(0.1631)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.4486) KL loss: tensor(0.1626)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.2621) KL loss: tensor(0.1401)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.6518) KL loss: tensor(0.3180)
loss: NT_CE learning rate: 0.00073
training loss: tensor(0.5417) KL loss: tensor(0.1851)
Global Model Acc on global data: 0.3471 length of data: 10000
-------------Round number:  16  -------------
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.5743) KL loss: tensor(0.2053)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.5132) KL loss: tensor(0.1744)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.3536) KL loss: tensor(0.1341)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.5148) KL loss: tensor(0.1836)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.3610) KL loss: tensor(0.1657)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.2466) KL loss: tensor(0.1230)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.5170) KL loss: tensor(0.1964)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.6312) KL loss: tensor(0.2130)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.5750) KL loss: tensor(0.2111)
loss: NT_CE learning rate: 0.000712
training loss: tensor(0.4883) KL loss: tensor(0.1908)
Global Model Acc on global data: 0.3692 length of data: 10000
save a model
-------------Round number:  17  -------------
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.2156) KL loss: tensor(0.1054)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.5136) KL loss: tensor(0.1902)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.4086) KL loss: tensor(0.1819)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.5152) KL loss: tensor(0.1894)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.5301) KL loss: tensor(0.2414)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.5132) KL loss: tensor(0.2046)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.4156) KL loss: tensor(0.1752)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.4821) KL loss: tensor(0.1882)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.3010) KL loss: tensor(0.1671)
loss: NT_CE learning rate: 0.000694
training loss: tensor(0.4210) KL loss: tensor(0.2029)
Global Model Acc on global data: 0.4018 length of data: 10000
save a model
-------------Round number:  18  -------------
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.4676) KL loss: tensor(0.1886)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.2357) KL loss: tensor(0.1411)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.3608) KL loss: tensor(0.1600)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(1.0119) KL loss: tensor(0.3362)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.4492) KL loss: tensor(0.1757)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.4479) KL loss: tensor(0.1959)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.4839) KL loss: tensor(0.1632)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.4939) KL loss: tensor(0.1908)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.4172) KL loss: tensor(0.1953)
loss: NT_CE learning rate: 0.0006760000000000001
training loss: tensor(0.3168) KL loss: tensor(0.1666)
Global Model Acc on global data: 0.4426 length of data: 10000
save a model
-------------Round number:  19  -------------
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.5073) KL loss: tensor(0.1981)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.5401) KL loss: tensor(0.2421)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.6934) KL loss: tensor(0.2584)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.5273) KL loss: tensor(0.2125)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.4804) KL loss: tensor(0.2019)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.6416) KL loss: tensor(0.2408)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.4226) KL loss: tensor(0.1829)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.2677) KL loss: tensor(0.1602)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.3899) KL loss: tensor(0.1872)
loss: NT_CE learning rate: 0.0006580000000000001
training loss: tensor(0.4582) KL loss: tensor(0.2145)
Global Model Acc on global data: 0.4242 length of data: 10000
-------------Round number:  20  -------------
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.3264) KL loss: tensor(0.1762)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.6544) KL loss: tensor(0.2632)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.5136) KL loss: tensor(0.1928)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.4157) KL loss: tensor(0.1774)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.4236) KL loss: tensor(0.1943)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.4599) KL loss: tensor(0.2013)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.4082) KL loss: tensor(0.1701)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.5726) KL loss: tensor(0.2380)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.3894) KL loss: tensor(0.1613)
loss: NT_CE learning rate: 0.00064
training loss: tensor(0.4603) KL loss: tensor(0.1979)
Global Model Acc on global data: 0.4033 length of data: 10000
-------------Round number:  21  -------------
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.4557) KL loss: tensor(0.1733)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.5077) KL loss: tensor(0.2088)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.4650) KL loss: tensor(0.1805)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.4454) KL loss: tensor(0.1682)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.2117) KL loss: tensor(0.1348)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.2961) KL loss: tensor(0.1279)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.4084) KL loss: tensor(0.1499)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.3438) KL loss: tensor(0.1450)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.3269) KL loss: tensor(0.1431)
loss: NT_CE learning rate: 0.0006220000000000002
training loss: tensor(0.6882) KL loss: tensor(0.3256)
Global Model Acc on global data: 0.4084 length of data: 10000
-------------Round number:  22  -------------
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.3992) KL loss: tensor(0.1727)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.3403) KL loss: tensor(0.1560)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.3403) KL loss: tensor(0.1264)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.2465) KL loss: tensor(0.1189)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.3456) KL loss: tensor(0.1676)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.3230) KL loss: tensor(0.1319)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.4158) KL loss: tensor(0.1392)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.3782) KL loss: tensor(0.1556)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.4379) KL loss: tensor(0.1832)
loss: NT_CE learning rate: 0.0006040000000000002
training loss: tensor(0.2991) KL loss: tensor(0.1223)
Global Model Acc on global data: 0.4234 length of data: 10000
-------------Round number:  23  -------------
loss: NT_CE learning rate: 0.000586
training loss: tensor(1.0688) KL loss: tensor(0.4114)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.5007) KL loss: tensor(0.1973)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.2991) KL loss: tensor(0.1419)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.4091) KL loss: tensor(0.1451)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.5243) KL loss: tensor(0.1798)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.3173) KL loss: tensor(0.1750)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.3873) KL loss: tensor(0.1505)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.3847) KL loss: tensor(0.1617)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.6290) KL loss: tensor(0.2060)
loss: NT_CE learning rate: 0.000586
training loss: tensor(0.4368) KL loss: tensor(0.1596)
Global Model Acc on global data: 0.4955 length of data: 10000
save a model
-------------Round number:  24  -------------
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.8207) KL loss: tensor(0.3074)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.2680) KL loss: tensor(0.1544)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.2473) KL loss: tensor(0.1589)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.3760) KL loss: tensor(0.1602)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.4698) KL loss: tensor(0.1907)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.2795) KL loss: tensor(0.1506)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.4908) KL loss: tensor(0.1804)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.4413) KL loss: tensor(0.2000)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.2823) KL loss: tensor(0.1235)
loss: NT_CE learning rate: 0.000568
training loss: tensor(0.3427) KL loss: tensor(0.1550)
Global Model Acc on global data: 0.4919 length of data: 10000
-------------Round number:  25  -------------
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.3508) KL loss: tensor(0.1456)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.2158) KL loss: tensor(0.1149)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.3589) KL loss: tensor(0.1578)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.7051) KL loss: tensor(0.2886)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.5650) KL loss: tensor(0.2292)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.3917) KL loss: tensor(0.1616)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.2394) KL loss: tensor(0.1306)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.3413) KL loss: tensor(0.1527)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.3386) KL loss: tensor(0.1306)
loss: NT_CE learning rate: 0.00055
training loss: tensor(0.5143) KL loss: tensor(0.1884)
Global Model Acc on global data: 0.4781 length of data: 10000
-------------Round number:  26  -------------
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.4645) KL loss: tensor(0.1654)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.4174) KL loss: tensor(0.1616)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.3749) KL loss: tensor(0.1488)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.2985) KL loss: tensor(0.1330)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.3889) KL loss: tensor(0.1398)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.3442) KL loss: tensor(0.1413)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.3961) KL loss: tensor(0.1584)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.3935) KL loss: tensor(0.1533)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.2685) KL loss: tensor(0.1380)
loss: NT_CE learning rate: 0.000532
training loss: tensor(0.2111) KL loss: tensor(0.1221)
Global Model Acc on global data: 0.45 length of data: 10000
-------------Round number:  27  -------------
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.2383) KL loss: tensor(0.1074)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.3567) KL loss: tensor(0.1211)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.3094) KL loss: tensor(0.1265)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.3593) KL loss: tensor(0.1363)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.2677) KL loss: tensor(0.1291)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.2623) KL loss: tensor(0.1037)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.1889) KL loss: tensor(0.1151)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.3718) KL loss: tensor(0.1544)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.4598) KL loss: tensor(0.1655)
loss: NT_CE learning rate: 0.000514
training loss: tensor(0.3214) KL loss: tensor(0.1310)
Global Model Acc on global data: 0.4149 length of data: 10000
-------------Round number:  28  -------------
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.2361) KL loss: tensor(0.1023)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.2162) KL loss: tensor(0.1091)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.3557) KL loss: tensor(0.1061)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.3273) KL loss: tensor(0.1350)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.4512) KL loss: tensor(0.1709)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.3793) KL loss: tensor(0.1304)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.2521) KL loss: tensor(0.1177)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.4954) KL loss: tensor(0.1851)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.3716) KL loss: tensor(0.1388)
loss: NT_CE learning rate: 0.000496
training loss: tensor(0.1868) KL loss: tensor(0.0910)
Global Model Acc on global data: 0.4689 length of data: 10000
-------------Round number:  29  -------------
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.4239) KL loss: tensor(0.1902)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.4147) KL loss: tensor(0.1610)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.3777) KL loss: tensor(0.1452)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.4137) KL loss: tensor(0.1481)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.4400) KL loss: tensor(0.1585)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.2156) KL loss: tensor(0.0970)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.2928) KL loss: tensor(0.1231)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.3209) KL loss: tensor(0.1257)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.6344) KL loss: tensor(0.2150)
loss: NT_CE learning rate: 0.0004780000000000001
training loss: tensor(0.2905) KL loss: tensor(0.1181)
Global Model Acc on global data: 0.4958 length of data: 10000
save a model
-------------Round number:  30  -------------
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3925) KL loss: tensor(0.1313)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.2014) KL loss: tensor(0.1103)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3184) KL loss: tensor(0.1258)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.2458) KL loss: tensor(0.1230)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3429) KL loss: tensor(0.1411)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.4651) KL loss: tensor(0.1938)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3729) KL loss: tensor(0.1396)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.4309) KL loss: tensor(0.1513)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.3234) KL loss: tensor(0.1396)
loss: NT_CE learning rate: 0.00046000000000000007
training loss: tensor(0.1651) KL loss: tensor(0.0920)
Global Model Acc on global data: 0.4864 length of data: 10000
-------------Round number:  31  -------------
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.3582) KL loss: tensor(0.1371)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.2609) KL loss: tensor(0.1269)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.3099) KL loss: tensor(0.1425)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.2838) KL loss: tensor(0.1353)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.3780) KL loss: tensor(0.1805)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.2654) KL loss: tensor(0.1055)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.2878) KL loss: tensor(0.1584)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.2931) KL loss: tensor(0.1315)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.4562) KL loss: tensor(0.2138)
loss: NT_CE learning rate: 0.00044200000000000006
training loss: tensor(0.2483) KL loss: tensor(0.1164)
Global Model Acc on global data: 0.53 length of data: 10000
save a model
-------------Round number:  32  -------------
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.3475) KL loss: tensor(0.1514)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.3870) KL loss: tensor(0.1606)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.3076) KL loss: tensor(0.1333)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.5540) KL loss: tensor(0.2120)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.3490) KL loss: tensor(0.1431)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.2023) KL loss: tensor(0.1134)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.2768) KL loss: tensor(0.1377)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.3322) KL loss: tensor(0.1651)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.2021) KL loss: tensor(0.1222)
loss: NT_CE learning rate: 0.00042400000000000006
training loss: tensor(0.5487) KL loss: tensor(0.2346)
Global Model Acc on global data: 0.4891 length of data: 10000
-------------Round number:  33  -------------
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.2671) KL loss: tensor(0.1248)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.3065) KL loss: tensor(0.1190)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.1880) KL loss: tensor(0.0857)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.2770) KL loss: tensor(0.1015)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.4015) KL loss: tensor(0.1926)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.1878) KL loss: tensor(0.0727)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.2786) KL loss: tensor(0.1134)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.3239) KL loss: tensor(0.1475)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.2793) KL loss: tensor(0.1204)
loss: NT_CE learning rate: 0.00040600000000000006
training loss: tensor(0.1621) KL loss: tensor(0.1003)
Global Model Acc on global data: 0.4727 length of data: 10000
-------------Round number:  34  -------------
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.1811) KL loss: tensor(0.0885)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.3596) KL loss: tensor(0.1475)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.2846) KL loss: tensor(0.1057)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.3854) KL loss: tensor(0.1313)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.4213) KL loss: tensor(0.1919)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.4201) KL loss: tensor(0.1252)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.1377) KL loss: tensor(0.0835)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.2701) KL loss: tensor(0.1127)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.2861) KL loss: tensor(0.1040)
loss: NT_CE learning rate: 0.000388
training loss: tensor(0.2831) KL loss: tensor(0.1124)
Global Model Acc on global data: 0.4886 length of data: 10000
-------------Round number:  35  -------------
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.2680) KL loss: tensor(0.1155)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.2078) KL loss: tensor(0.0885)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.4404) KL loss: tensor(0.1498)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.1362) KL loss: tensor(0.0907)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.1878) KL loss: tensor(0.1152)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.2012) KL loss: tensor(0.0967)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.1782) KL loss: tensor(0.0946)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.3686) KL loss: tensor(0.1286)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.4675) KL loss: tensor(0.1801)
loss: NT_CE learning rate: 0.00037
training loss: tensor(0.3176) KL loss: tensor(0.1197)
Global Model Acc on global data: 0.4145 length of data: 10000
-------------Round number:  36  -------------
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.3144) KL loss: tensor(0.0991)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.3161) KL loss: tensor(0.0926)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.2975) KL loss: tensor(0.1032)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.0792) KL loss: tensor(0.0660)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.4099) KL loss: tensor(0.1132)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.3384) KL loss: tensor(0.1175)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.3416) KL loss: tensor(0.1185)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.3922) KL loss: tensor(0.0891)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.2016) KL loss: tensor(0.0966)
loss: NT_CE learning rate: 0.0003520000000000001
training loss: tensor(0.3007) KL loss: tensor(0.1423)
Global Model Acc on global data: 0.4364 length of data: 10000
-------------Round number:  37  -------------
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.3157) KL loss: tensor(0.1141)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.2836) KL loss: tensor(0.1046)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.2634) KL loss: tensor(0.1023)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.3258) KL loss: tensor(0.0966)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.3258) KL loss: tensor(0.0943)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.1133) KL loss: tensor(0.0739)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.2904) KL loss: tensor(0.1317)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.2636) KL loss: tensor(0.0950)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.1609) KL loss: tensor(0.0730)
loss: NT_CE learning rate: 0.0003340000000000001
training loss: tensor(0.3333) KL loss: tensor(0.1040)
Global Model Acc on global data: 0.4939 length of data: 10000
-------------Round number:  38  -------------
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.1755) KL loss: tensor(0.0886)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.4053) KL loss: tensor(0.1736)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.2417) KL loss: tensor(0.1125)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.3788) KL loss: tensor(0.1166)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.4161) KL loss: tensor(0.1596)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.5780) KL loss: tensor(0.1783)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.3413) KL loss: tensor(0.1168)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.3222) KL loss: tensor(0.1096)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.4206) KL loss: tensor(0.1355)
loss: NT_CE learning rate: 0.0003160000000000001
training loss: tensor(0.4831) KL loss: tensor(0.1392)
Global Model Acc on global data: 0.5066 length of data: 10000
-------------Round number:  39  -------------
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.3652) KL loss: tensor(0.1456)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2426) KL loss: tensor(0.1079)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2814) KL loss: tensor(0.1121)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2046) KL loss: tensor(0.1089)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2526) KL loss: tensor(0.0802)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2489) KL loss: tensor(0.1067)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.1972) KL loss: tensor(0.0930)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.3312) KL loss: tensor(0.1182)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.2761) KL loss: tensor(0.1217)
loss: NT_CE learning rate: 0.00029800000000000003
training loss: tensor(0.4546) KL loss: tensor(0.1485)
Global Model Acc on global data: 0.5177 length of data: 10000
-------------Round number:  40  -------------
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.3959) KL loss: tensor(0.1357)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.3326) KL loss: tensor(0.1343)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.4041) KL loss: tensor(0.1455)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.2726) KL loss: tensor(0.1117)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.6366) KL loss: tensor(0.2028)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.6461) KL loss: tensor(0.3382)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.1379) KL loss: tensor(0.0750)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.2831) KL loss: tensor(0.1000)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.3535) KL loss: tensor(0.1334)
loss: NT_CE learning rate: 0.00028000000000000003
training loss: tensor(0.1696) KL loss: tensor(0.0851)
Global Model Acc on global data: 0.5188 length of data: 10000
-------------Round number:  41  -------------
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.3714) KL loss: tensor(0.1269)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.3556) KL loss: tensor(0.1206)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.2420) KL loss: tensor(0.1241)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.2246) KL loss: tensor(0.0950)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.4068) KL loss: tensor(0.1239)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.3958) KL loss: tensor(0.1718)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.3414) KL loss: tensor(0.1186)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.2253) KL loss: tensor(0.0787)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.1907) KL loss: tensor(0.0912)
loss: NT_CE learning rate: 0.000262
training loss: tensor(0.2647) KL loss: tensor(0.0961)
Global Model Acc on global data: 0.5306 length of data: 10000
save a model
-------------Round number:  42  -------------
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.2976) KL loss: tensor(0.1056)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.2352) KL loss: tensor(0.1088)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.1476) KL loss: tensor(0.0846)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.2219) KL loss: tensor(0.0878)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.3896) KL loss: tensor(0.1094)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.3079) KL loss: tensor(0.1193)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.3472) KL loss: tensor(0.1427)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.2199) KL loss: tensor(0.1026)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.1969) KL loss: tensor(0.0997)
loss: NT_CE learning rate: 0.0002440000000000001
training loss: tensor(0.1305) KL loss: tensor(0.0589)
Global Model Acc on global data: 0.5216 length of data: 10000
-------------Round number:  43  -------------
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.3662) KL loss: tensor(0.0900)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.2573) KL loss: tensor(0.0997)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.1864) KL loss: tensor(0.0820)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.2866) KL loss: tensor(0.1224)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.1643) KL loss: tensor(0.0784)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.0817) KL loss: tensor(0.0602)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.2805) KL loss: tensor(0.0987)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.2708) KL loss: tensor(0.0955)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.6516) KL loss: tensor(0.3454)
loss: NT_CE learning rate: 0.0002260000000000001
training loss: tensor(0.3356) KL loss: tensor(0.1144)
Global Model Acc on global data: 0.5295 length of data: 10000
-------------Round number:  44  -------------
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.3249) KL loss: tensor(0.1164)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.8449) KL loss: tensor(0.2628)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.1006) KL loss: tensor(0.0635)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.2324) KL loss: tensor(0.0908)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.2749) KL loss: tensor(0.0954)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.5682) KL loss: tensor(0.2210)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.3757) KL loss: tensor(0.1181)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.8352) KL loss: tensor(0.2807)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.2746) KL loss: tensor(0.0950)
loss: NT_CE learning rate: 0.00020800000000000007
training loss: tensor(0.4299) KL loss: tensor(0.1927)
Global Model Acc on global data: 0.5061 length of data: 10000
-------------Round number:  45  -------------
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.1770) KL loss: tensor(0.0546)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.4851) KL loss: tensor(0.1229)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.1691) KL loss: tensor(0.0851)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.2531) KL loss: tensor(0.0999)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.2032) KL loss: tensor(0.1150)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.3884) KL loss: tensor(0.1659)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.3288) KL loss: tensor(0.0958)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.4974) KL loss: tensor(0.1237)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.2016) KL loss: tensor(0.0843)
loss: NT_CE learning rate: 0.00019000000000000006
training loss: tensor(0.2025) KL loss: tensor(0.0837)
Global Model Acc on global data: 0.5323 length of data: 10000
save a model
-------------Round number:  46  -------------
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.3732) KL loss: tensor(0.0949)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.2015) KL loss: tensor(0.0910)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.1698) KL loss: tensor(0.1040)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.1302) KL loss: tensor(0.0701)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.3795) KL loss: tensor(0.0987)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.3526) KL loss: tensor(0.1103)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.2428) KL loss: tensor(0.0898)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.4296) KL loss: tensor(0.1354)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.3468) KL loss: tensor(0.1212)
loss: NT_CE learning rate: 0.00017200000000000003
training loss: tensor(0.2578) KL loss: tensor(0.0859)
Global Model Acc on global data: 0.5421 length of data: 10000
save a model
-------------Round number:  47  -------------
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.2871) KL loss: tensor(0.0907)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.3347) KL loss: tensor(0.1356)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.3216) KL loss: tensor(0.1076)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.3526) KL loss: tensor(0.1363)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.3187) KL loss: tensor(0.1164)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.3957) KL loss: tensor(0.1146)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.5247) KL loss: tensor(0.1356)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.1908) KL loss: tensor(0.0980)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.2406) KL loss: tensor(0.1092)
loss: NT_CE learning rate: 0.00015400000000000003
training loss: tensor(0.2807) KL loss: tensor(0.1167)
Global Model Acc on global data: 0.5427 length of data: 10000
save a model
-------------Round number:  48  -------------
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.2132) KL loss: tensor(0.0885)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.2640) KL loss: tensor(0.0834)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.3045) KL loss: tensor(0.0868)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.4006) KL loss: tensor(0.1120)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.5316) KL loss: tensor(0.3300)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.3249) KL loss: tensor(0.1159)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.1934) KL loss: tensor(0.0777)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.3523) KL loss: tensor(0.1091)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.2594) KL loss: tensor(0.0909)
loss: NT_CE learning rate: 0.00013600000000000013
training loss: tensor(0.2809) KL loss: tensor(0.1244)
Global Model Acc on global data: 0.5463 length of data: 10000
save a model
-------------Round number:  49  -------------
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2437) KL loss: tensor(0.0989)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.3395) KL loss: tensor(0.0880)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.4264) KL loss: tensor(0.1027)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2665) KL loss: tensor(0.0961)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.3314) KL loss: tensor(0.1288)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.1953) KL loss: tensor(0.0895)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2652) KL loss: tensor(0.0792)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2851) KL loss: tensor(0.1182)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.2335) KL loss: tensor(0.0849)
loss: NT_CE learning rate: 0.0001180000000000001
training loss: tensor(0.3968) KL loss: tensor(0.1052)
Global Model Acc on global data: 0.5558 length of data: 10000
save a model
-------------Round number:  50  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2382) KL loss: tensor(0.0927)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3391) KL loss: tensor(0.1119)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3531) KL loss: tensor(0.0941)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2044) KL loss: tensor(0.0872)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2872) KL loss: tensor(0.0938)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1705) KL loss: tensor(0.0770)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1767) KL loss: tensor(0.0774)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2259) KL loss: tensor(0.0815)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1562) KL loss: tensor(0.0687)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3880) KL loss: tensor(0.1293)
Global Model Acc on global data: 0.5527 length of data: 10000
-------------Round number:  51  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2889) KL loss: tensor(0.0853)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3433) KL loss: tensor(0.0941)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7831) KL loss: tensor(0.3456)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2473) KL loss: tensor(0.0957)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2902) KL loss: tensor(0.0955)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5279) KL loss: tensor(0.1487)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1732) KL loss: tensor(0.0834)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4181) KL loss: tensor(0.1138)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2918) KL loss: tensor(0.1200)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2495) KL loss: tensor(0.1036)
Global Model Acc on global data: 0.5593 length of data: 10000
save a model
-------------Round number:  52  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3092) KL loss: tensor(0.1136)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3839) KL loss: tensor(0.1178)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2463) KL loss: tensor(0.0938)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4465) KL loss: tensor(0.1267)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3110) KL loss: tensor(0.1100)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2792) KL loss: tensor(0.1363)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1405) KL loss: tensor(0.0902)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1257) KL loss: tensor(0.0723)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2236) KL loss: tensor(0.0936)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3412) KL loss: tensor(0.1038)
Global Model Acc on global data: 0.5535 length of data: 10000
-------------Round number:  53  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4011) KL loss: tensor(0.1296)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1874) KL loss: tensor(0.0798)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5111) KL loss: tensor(0.1395)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2290) KL loss: tensor(0.0939)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1832) KL loss: tensor(0.0787)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4725) KL loss: tensor(0.1887)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3409) KL loss: tensor(0.0884)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3778) KL loss: tensor(0.1201)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3777) KL loss: tensor(0.1170)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6094) KL loss: tensor(0.1763)
Global Model Acc on global data: 0.5599 length of data: 10000
save a model
-------------Round number:  54  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4938) KL loss: tensor(0.1791)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2596) KL loss: tensor(0.0915)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4264) KL loss: tensor(0.1401)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2709) KL loss: tensor(0.0917)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2709) KL loss: tensor(0.1356)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3579) KL loss: tensor(0.1259)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5936) KL loss: tensor(0.1622)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2660) KL loss: tensor(0.0960)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2998) KL loss: tensor(0.1069)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2697) KL loss: tensor(0.1318)
Global Model Acc on global data: 0.5516 length of data: 10000
-------------Round number:  55  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3538) KL loss: tensor(0.0828)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2915) KL loss: tensor(0.1069)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3131) KL loss: tensor(0.1135)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1807) KL loss: tensor(0.0811)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1960) KL loss: tensor(0.0770)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2569) KL loss: tensor(0.0891)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4799) KL loss: tensor(0.1382)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1699) KL loss: tensor(0.0866)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2077) KL loss: tensor(0.0780)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2024) KL loss: tensor(0.0833)
Global Model Acc on global data: 0.5492 length of data: 10000
-------------Round number:  56  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2453) KL loss: tensor(0.1121)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2765) KL loss: tensor(0.1311)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4482) KL loss: tensor(0.1370)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3818) KL loss: tensor(0.1000)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1602) KL loss: tensor(0.0877)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4708) KL loss: tensor(0.1776)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2042) KL loss: tensor(0.1073)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3610) KL loss: tensor(0.1146)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2527) KL loss: tensor(0.0994)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1540) KL loss: tensor(0.0643)
Global Model Acc on global data: 0.5445 length of data: 10000
-------------Round number:  57  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1366) KL loss: tensor(0.0737)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1946) KL loss: tensor(0.0751)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3178) KL loss: tensor(0.0916)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2436) KL loss: tensor(0.0920)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2890) KL loss: tensor(0.1050)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2539) KL loss: tensor(0.1111)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1914) KL loss: tensor(0.0787)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3845) KL loss: tensor(0.0808)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1821) KL loss: tensor(0.0803)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3140) KL loss: tensor(0.1012)
Global Model Acc on global data: 0.555 length of data: 10000
-------------Round number:  58  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3286) KL loss: tensor(0.1027)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2845) KL loss: tensor(0.1155)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2073) KL loss: tensor(0.0870)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2030) KL loss: tensor(0.0899)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2122) KL loss: tensor(0.0922)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2207) KL loss: tensor(0.0858)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2551) KL loss: tensor(0.1100)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1623) KL loss: tensor(0.0789)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2258) KL loss: tensor(0.0964)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4232) KL loss: tensor(0.1307)
Global Model Acc on global data: 0.5469 length of data: 10000
-------------Round number:  59  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3541) KL loss: tensor(0.1144)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2779) KL loss: tensor(0.1030)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1831) KL loss: tensor(0.0821)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3176) KL loss: tensor(0.0899)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3555) KL loss: tensor(0.1141)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3144) KL loss: tensor(0.1310)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5054) KL loss: tensor(0.1468)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4094) KL loss: tensor(0.1499)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2807) KL loss: tensor(0.0983)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3574) KL loss: tensor(0.1420)
Global Model Acc on global data: 0.5417 length of data: 10000
-------------Round number:  60  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4670) KL loss: tensor(0.1191)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4525) KL loss: tensor(0.1277)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2652) KL loss: tensor(0.1041)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1136) KL loss: tensor(0.0707)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2439) KL loss: tensor(0.0963)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2278) KL loss: tensor(0.0911)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3381) KL loss: tensor(0.1074)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3618) KL loss: tensor(0.0973)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2837) KL loss: tensor(0.1108)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3879) KL loss: tensor(0.1057)
Global Model Acc on global data: 0.5495 length of data: 10000
-------------Round number:  61  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1936) KL loss: tensor(0.0870)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3270) KL loss: tensor(0.1102)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1535) KL loss: tensor(0.0794)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3379) KL loss: tensor(0.1167)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2001) KL loss: tensor(0.0898)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3281) KL loss: tensor(0.0975)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4855) KL loss: tensor(0.1914)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1878) KL loss: tensor(0.0812)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2491) KL loss: tensor(0.1111)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2427) KL loss: tensor(0.1035)
Global Model Acc on global data: 0.546 length of data: 10000
-------------Round number:  62  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1750) KL loss: tensor(0.0876)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3423) KL loss: tensor(0.1100)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1630) KL loss: tensor(0.0800)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2007) KL loss: tensor(0.0962)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1203) KL loss: tensor(0.0654)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2715) KL loss: tensor(0.1325)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4384) KL loss: tensor(0.1784)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1485) KL loss: tensor(0.0846)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2889) KL loss: tensor(0.1037)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2037) KL loss: tensor(0.0994)
Global Model Acc on global data: 0.5587 length of data: 10000
-------------Round number:  63  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3945) KL loss: tensor(0.1294)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1994) KL loss: tensor(0.0962)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3159) KL loss: tensor(0.1411)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3031) KL loss: tensor(0.1039)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1818) KL loss: tensor(0.0956)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4223) KL loss: tensor(0.1001)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2270) KL loss: tensor(0.0858)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2574) KL loss: tensor(0.0972)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4792) KL loss: tensor(0.1455)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4721) KL loss: tensor(0.1858)
Global Model Acc on global data: 0.5608 length of data: 10000
save a model
-------------Round number:  64  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1311) KL loss: tensor(0.0718)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2631) KL loss: tensor(0.1341)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3003) KL loss: tensor(0.1415)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2840) KL loss: tensor(0.1229)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3161) KL loss: tensor(0.1195)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3418) KL loss: tensor(0.1232)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3394) KL loss: tensor(0.0926)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1675) KL loss: tensor(0.0821)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1993) KL loss: tensor(0.1010)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2187) KL loss: tensor(0.1039)
Global Model Acc on global data: 0.5554 length of data: 10000
-------------Round number:  65  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1927) KL loss: tensor(0.1225)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2595) KL loss: tensor(0.1285)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2802) KL loss: tensor(0.1070)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3011) KL loss: tensor(0.1110)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1534) KL loss: tensor(0.0886)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2743) KL loss: tensor(0.1267)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1254) KL loss: tensor(0.0608)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2058) KL loss: tensor(0.1016)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3104) KL loss: tensor(0.0982)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1719) KL loss: tensor(0.0910)
Global Model Acc on global data: 0.5533 length of data: 10000
-------------Round number:  66  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3077) KL loss: tensor(0.1192)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2393) KL loss: tensor(0.1079)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3753) KL loss: tensor(0.1164)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2858) KL loss: tensor(0.0966)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1091) KL loss: tensor(0.0639)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3000) KL loss: tensor(0.1015)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2818) KL loss: tensor(0.0974)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3497) KL loss: tensor(0.1392)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4005) KL loss: tensor(0.1083)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3281) KL loss: tensor(0.0759)
Global Model Acc on global data: 0.5606 length of data: 10000
-------------Round number:  67  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1010) KL loss: tensor(0.0602)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1980) KL loss: tensor(0.0932)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2717) KL loss: tensor(0.0988)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1255) KL loss: tensor(0.0733)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4218) KL loss: tensor(0.1408)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1588) KL loss: tensor(0.0735)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1921) KL loss: tensor(0.0810)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2030) KL loss: tensor(0.0889)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2881) KL loss: tensor(0.1170)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.8367) KL loss: tensor(0.3724)
Global Model Acc on global data: 0.5538 length of data: 10000
-------------Round number:  68  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1366) KL loss: tensor(0.0657)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2583) KL loss: tensor(0.0961)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4746) KL loss: tensor(0.1381)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2200) KL loss: tensor(0.0777)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4521) KL loss: tensor(0.1366)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2441) KL loss: tensor(0.0629)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1435) KL loss: tensor(0.0801)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1607) KL loss: tensor(0.0732)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3723) KL loss: tensor(0.1820)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1726) KL loss: tensor(0.0641)
Global Model Acc on global data: 0.5484 length of data: 10000
-------------Round number:  69  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3444) KL loss: tensor(0.1158)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1336) KL loss: tensor(0.0672)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2876) KL loss: tensor(0.1089)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1423) KL loss: tensor(0.0705)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1553) KL loss: tensor(0.0858)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2060) KL loss: tensor(0.0867)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7610) KL loss: tensor(0.2590)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1069) KL loss: tensor(0.0648)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2654) KL loss: tensor(0.0985)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3819) KL loss: tensor(0.1455)
Global Model Acc on global data: 0.5521 length of data: 10000
-------------Round number:  70  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1969) KL loss: tensor(0.1079)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2747) KL loss: tensor(0.0927)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3883) KL loss: tensor(0.1072)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4227) KL loss: tensor(0.2097)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2058) KL loss: tensor(0.0993)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1925) KL loss: tensor(0.0705)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1907) KL loss: tensor(0.1088)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2283) KL loss: tensor(0.0934)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2676) KL loss: tensor(0.1022)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2835) KL loss: tensor(0.1046)
Global Model Acc on global data: 0.5482 length of data: 10000
-------------Round number:  71  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3302) KL loss: tensor(0.1049)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1905) KL loss: tensor(0.0860)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2407) KL loss: tensor(0.1100)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1986) KL loss: tensor(0.0886)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3343) KL loss: tensor(0.1235)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3715) KL loss: tensor(0.0990)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2731) KL loss: tensor(0.1644)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4322) KL loss: tensor(0.1492)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1195) KL loss: tensor(0.0784)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1542) KL loss: tensor(0.0696)
Global Model Acc on global data: 0.5463 length of data: 10000
-------------Round number:  72  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1087) KL loss: tensor(0.0733)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2630) KL loss: tensor(0.0973)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2353) KL loss: tensor(0.0812)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3877) KL loss: tensor(0.1455)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1342) KL loss: tensor(0.0683)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1283) KL loss: tensor(0.0699)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1951) KL loss: tensor(0.0832)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0840) KL loss: tensor(0.0603)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2609) KL loss: tensor(0.0932)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2249) KL loss: tensor(0.1105)
Global Model Acc on global data: 0.5409 length of data: 10000
-------------Round number:  73  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2567) KL loss: tensor(0.1084)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1327) KL loss: tensor(0.0751)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2577) KL loss: tensor(0.1094)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2403) KL loss: tensor(0.0959)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2139) KL loss: tensor(0.0959)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3587) KL loss: tensor(0.1099)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3856) KL loss: tensor(0.0794)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0991) KL loss: tensor(0.0696)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1384) KL loss: tensor(0.0850)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2981) KL loss: tensor(0.1112)
Global Model Acc on global data: 0.5461 length of data: 10000
-------------Round number:  74  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2330) KL loss: tensor(0.0987)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2073) KL loss: tensor(0.0893)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2294) KL loss: tensor(0.1057)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1279) KL loss: tensor(0.0721)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2258) KL loss: tensor(0.0808)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2876) KL loss: tensor(0.0799)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2146) KL loss: tensor(0.0733)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1563) KL loss: tensor(0.0689)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2508) KL loss: tensor(0.0940)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2010) KL loss: tensor(0.0896)
Global Model Acc on global data: 0.5602 length of data: 10000
-------------Round number:  75  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3562) KL loss: tensor(0.1292)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1903) KL loss: tensor(0.0909)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1731) KL loss: tensor(0.0857)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2113) KL loss: tensor(0.0852)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2399) KL loss: tensor(0.1159)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6108) KL loss: tensor(0.2476)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2098) KL loss: tensor(0.1035)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1799) KL loss: tensor(0.1073)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1916) KL loss: tensor(0.1001)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3507) KL loss: tensor(0.0856)
Global Model Acc on global data: 0.5678 length of data: 10000
save a model
-------------Round number:  76  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4853) KL loss: tensor(0.1797)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1656) KL loss: tensor(0.1141)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4906) KL loss: tensor(0.2757)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2627) KL loss: tensor(0.1152)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1281) KL loss: tensor(0.0781)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1927) KL loss: tensor(0.0941)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3192) KL loss: tensor(0.1154)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3103) KL loss: tensor(0.1121)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1748) KL loss: tensor(0.0860)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3388) KL loss: tensor(0.1369)
Global Model Acc on global data: 0.5583 length of data: 10000
-------------Round number:  77  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2421) KL loss: tensor(0.0908)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1876) KL loss: tensor(0.0875)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2548) KL loss: tensor(0.0888)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5042) KL loss: tensor(0.1974)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3712) KL loss: tensor(0.1441)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1944) KL loss: tensor(0.0963)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1738) KL loss: tensor(0.1087)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0914) KL loss: tensor(0.0692)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1857) KL loss: tensor(0.0938)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2386) KL loss: tensor(0.1087)
Global Model Acc on global data: 0.551 length of data: 10000
-------------Round number:  78  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2463) KL loss: tensor(0.0984)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2587) KL loss: tensor(0.0950)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1683) KL loss: tensor(0.0888)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2049) KL loss: tensor(0.1019)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2052) KL loss: tensor(0.0898)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2980) KL loss: tensor(0.0998)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3013) KL loss: tensor(0.1474)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1478) KL loss: tensor(0.0784)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1823) KL loss: tensor(0.0768)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4440) KL loss: tensor(0.1774)
Global Model Acc on global data: 0.5561 length of data: 10000
-------------Round number:  79  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2754) KL loss: tensor(0.0791)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2368) KL loss: tensor(0.0940)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1722) KL loss: tensor(0.0846)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3162) KL loss: tensor(0.1731)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1973) KL loss: tensor(0.1080)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3457) KL loss: tensor(0.1863)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2409) KL loss: tensor(0.0924)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1688) KL loss: tensor(0.0862)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1725) KL loss: tensor(0.0782)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2050) KL loss: tensor(0.1011)
Global Model Acc on global data: 0.568 length of data: 10000
save a model
-------------Round number:  80  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1736) KL loss: tensor(0.1007)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5701) KL loss: tensor(0.2445)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1651) KL loss: tensor(0.0876)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1691) KL loss: tensor(0.0855)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4311) KL loss: tensor(0.1303)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1447) KL loss: tensor(0.0669)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2478) KL loss: tensor(0.1306)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1750) KL loss: tensor(0.0790)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1750) KL loss: tensor(0.0807)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3113) KL loss: tensor(0.1558)
Global Model Acc on global data: 0.561 length of data: 10000
-------------Round number:  81  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1755) KL loss: tensor(0.1171)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1072) KL loss: tensor(0.0674)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2352) KL loss: tensor(0.0894)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1561) KL loss: tensor(0.0721)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3462) KL loss: tensor(0.1351)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2822) KL loss: tensor(0.1089)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1328) KL loss: tensor(0.0646)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1736) KL loss: tensor(0.0854)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2094) KL loss: tensor(0.0973)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1987) KL loss: tensor(0.0799)
Global Model Acc on global data: 0.553 length of data: 10000
-------------Round number:  82  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3899) KL loss: tensor(0.1430)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1432) KL loss: tensor(0.0901)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1262) KL loss: tensor(0.0765)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3430) KL loss: tensor(0.1140)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2102) KL loss: tensor(0.0902)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2121) KL loss: tensor(0.0840)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7314) KL loss: tensor(0.2146)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2592) KL loss: tensor(0.1053)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2654) KL loss: tensor(0.0903)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1430) KL loss: tensor(0.0729)
Global Model Acc on global data: 0.5566 length of data: 10000
-------------Round number:  83  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1489) KL loss: tensor(0.0743)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4974) KL loss: tensor(0.2290)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1224) KL loss: tensor(0.0702)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1457) KL loss: tensor(0.0693)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2596) KL loss: tensor(0.1046)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2432) KL loss: tensor(0.0863)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1328) KL loss: tensor(0.0651)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1385) KL loss: tensor(0.0762)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2691) KL loss: tensor(0.0974)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3619) KL loss: tensor(0.1180)
Global Model Acc on global data: 0.5629 length of data: 10000
-------------Round number:  84  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1805) KL loss: tensor(0.0849)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1534) KL loss: tensor(0.0830)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1418) KL loss: tensor(0.0832)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3137) KL loss: tensor(0.1148)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1759) KL loss: tensor(0.0964)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7330) KL loss: tensor(0.3493)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2649) KL loss: tensor(0.1410)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2193) KL loss: tensor(0.1000)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1385) KL loss: tensor(0.0756)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2141) KL loss: tensor(0.0844)
Global Model Acc on global data: 0.5615 length of data: 10000
-------------Round number:  85  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1631) KL loss: tensor(0.0929)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2917) KL loss: tensor(0.0965)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1302) KL loss: tensor(0.0681)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2555) KL loss: tensor(0.0821)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2265) KL loss: tensor(0.0849)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2817) KL loss: tensor(0.1194)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1187) KL loss: tensor(0.0737)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1200) KL loss: tensor(0.0700)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1950) KL loss: tensor(0.0914)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5282) KL loss: tensor(0.1595)
Global Model Acc on global data: 0.5567 length of data: 10000
-------------Round number:  86  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1500) KL loss: tensor(0.0818)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3252) KL loss: tensor(0.1567)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2182) KL loss: tensor(0.0925)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2876) KL loss: tensor(0.0813)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2178) KL loss: tensor(0.1012)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1835) KL loss: tensor(0.0742)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3339) KL loss: tensor(0.1969)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3103) KL loss: tensor(0.0979)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1821) KL loss: tensor(0.0863)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1918) KL loss: tensor(0.0869)
Global Model Acc on global data: 0.5652 length of data: 10000
-------------Round number:  87  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3610) KL loss: tensor(0.1479)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1586) KL loss: tensor(0.1020)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2205) KL loss: tensor(0.1075)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2470) KL loss: tensor(0.1560)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2214) KL loss: tensor(0.0962)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1951) KL loss: tensor(0.0760)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.6554) KL loss: tensor(0.2841)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1656) KL loss: tensor(0.0664)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5246) KL loss: tensor(0.2323)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1660) KL loss: tensor(0.0860)
Global Model Acc on global data: 0.5557 length of data: 10000
-------------Round number:  88  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1194) KL loss: tensor(0.0705)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1217) KL loss: tensor(0.0742)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2387) KL loss: tensor(0.0867)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1157) KL loss: tensor(0.0686)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2653) KL loss: tensor(0.0989)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2731) KL loss: tensor(0.1364)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1762) KL loss: tensor(0.0853)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1298) KL loss: tensor(0.0736)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5722) KL loss: tensor(0.1937)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3414) KL loss: tensor(0.1320)
Global Model Acc on global data: 0.5587 length of data: 10000
-------------Round number:  89  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4383) KL loss: tensor(0.1780)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2725) KL loss: tensor(0.1139)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2415) KL loss: tensor(0.0828)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1308) KL loss: tensor(0.0730)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2331) KL loss: tensor(0.0950)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2238) KL loss: tensor(0.1374)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2685) KL loss: tensor(0.0955)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2756) KL loss: tensor(0.1785)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2945) KL loss: tensor(0.1241)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1673) KL loss: tensor(0.0739)
Global Model Acc on global data: 0.5656 length of data: 10000
-------------Round number:  90  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2219) KL loss: tensor(0.0771)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1820) KL loss: tensor(0.0927)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4772) KL loss: tensor(0.1741)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.0991) KL loss: tensor(0.0627)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3637) KL loss: tensor(0.1993)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1558) KL loss: tensor(0.0777)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1423) KL loss: tensor(0.0728)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2122) KL loss: tensor(0.0894)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2893) KL loss: tensor(0.1355)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1878) KL loss: tensor(0.0929)
Global Model Acc on global data: 0.5593 length of data: 10000
-------------Round number:  91  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1973) KL loss: tensor(0.0938)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2020) KL loss: tensor(0.0761)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1675) KL loss: tensor(0.0755)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1265) KL loss: tensor(0.0744)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1699) KL loss: tensor(0.0797)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.5636) KL loss: tensor(0.2217)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1612) KL loss: tensor(0.0805)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.7661) KL loss: tensor(0.3372)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1519) KL loss: tensor(0.0784)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3339) KL loss: tensor(0.1391)
Global Model Acc on global data: 0.5677 length of data: 10000
-------------Round number:  92  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1517) KL loss: tensor(0.0747)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3088) KL loss: tensor(0.1234)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2324) KL loss: tensor(0.1351)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2179) KL loss: tensor(0.0966)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3783) KL loss: tensor(0.1122)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3672) KL loss: tensor(0.1648)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2580) KL loss: tensor(0.1189)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1703) KL loss: tensor(0.0800)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.4333) KL loss: tensor(0.1106)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1101) KL loss: tensor(0.0672)
Global Model Acc on global data: 0.5686 length of data: 10000
save a model
-------------Round number:  93  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2052) KL loss: tensor(0.1004)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2083) KL loss: tensor(0.0901)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1641) KL loss: tensor(0.0947)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1177) KL loss: tensor(0.0603)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2241) KL loss: tensor(0.1001)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1656) KL loss: tensor(0.0786)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2817) KL loss: tensor(0.1108)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2746) KL loss: tensor(0.1324)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2815) KL loss: tensor(0.0993)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1940) KL loss: tensor(0.0939)
Global Model Acc on global data: 0.552 length of data: 10000
-------------Round number:  94  -------------
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3113) KL loss: tensor(0.1296)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1971) KL loss: tensor(0.0984)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1843) KL loss: tensor(0.0748)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2333) KL loss: tensor(0.0914)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1976) KL loss: tensor(0.1128)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.3286) KL loss: tensor(0.1176)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1881) KL loss: tensor(0.0972)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.2793) KL loss: tensor(0.1129)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1185) KL loss: tensor(0.0653)
loss: NT_CE learning rate: 0.0001
training loss: tensor(0.1229) KL loss: tensor(0.0684)
